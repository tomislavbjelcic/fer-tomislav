\documentclass[times, utf8, zavrsni]{fer}
\usepackage{booktabs}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{relsize}

\newenvironment{croatianalgorithm}[1][]
  {\begin{algorithm}[#1]
     \selectlanguage{croatian}%
     \floatname{algorithm}{Algoritam}%
     \renewcommand{\algorithmicrepeat}{\textbf{ponavljaj}}%
     \renewcommand{\algorithmicuntil}{\textbf{dok}}%
     \renewcommand{\algorithmicforall}{\textbf{za svaki}}%
     \renewcommand{\algorithmicendfor}{\textbf{kraj za}}
     \renewcommand{\algorithmicdo}{\,}
     \renewcommand{\algorithmicwhile}{\textbf{ponavljaj}}
     \renewcommand{\algorithmicendwhile}{\textbf{kraj ponavljaj}}
     \renewcommand{\algorithmicreturn}{\textbf{vrati}}
     \renewcommand{\algorithmicif}{\textbf{ako}}
     \renewcommand{\algorithmicendif}{\textbf{kraj ako}}
     % Set other language requirements
  }
  {\end{algorithm}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}

% TODO: Navedite broj rada.
\thesisnumber{121}

% TODO: Navedite naslov rada.
\title{Usporedba metoda grupiranja primjenom programskog jezika Python}

% TODO: Navedite vaše ime i prezime.
\author{Tomislav Bjelčić}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{}

\tableofcontents

\chapter{Uvod}

Grupiranje \engl{clustering} je postupak kojim se neki skup podataka razvrstava u skupine, odnosno grupe \engl{clusters}, u kojima su podaci međusobno slični. Grupiranje je jedno od metoda \textbf{nenadziranog strojnog učenja} \engl{unsupervised learning}, dakle ulazni podaci nisu označeni \engl{unlabeled}, odnosno nemaju neku ciljnu vrijednost koja bi naznačila kojoj grupi pripada neki podatak. Algoritmi grupiranja iz takvih podataka onda moraju sami prepoznati grupe podataka, odrediti za svaki ulazni podatak kojoj grupi bi pripadao i ukoliko je to moguće, odrediti \textbf{stršeće vrijednosti} \engl{outliers}.

Primjerice, neka su na raspolaganju podaci o visini i masi odraslih pasa iz nekog 
skloništa za životinje. U ovom jednostavnom primjeru ulazni podaci imaju dvije značajke, odnosno dimenzije: visina i masa. Općenito, ulazni podaci mogu imati proizvoljno mnogo značajka. Ulazni podaci su na slici \ref{fig:intro} prikazani kao točke na grafu gdje os apscisa predstavlja masu, a os ordinata predstavlja visinu pojedinog psa.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.53]{img/uvod.png}
    \caption{Podaci o psima u skloništu za životinje}
    \label{fig:intro}
\end{figure}

Podaci se prirodno grupiraju u tri grupe, koje odgovaraju različitim pasminama. Ulazni podaci nisu označeni, dakle jedino na raspolaganju imamo masu i visinu. Algoritmi grupiranja moraju prepoznati da se radi o tri grupe i slične točke pridruži istoj grupi. Općenito, poželjno je da algoritam grupiranja podatke grupira na onaj način koji odgovara prirodnom grupiranju. U navedenom primjeru prirodno grupiranje je vizualno jasno i ono odgovara pasminama, no to ne mora biti slučaj. Neće uvijek ni vizualnom metodom biti jednoznačno jasno kakvo je njihovo prirodno grupiranje. Ovdje bi mjera sličnosti mogla biti definirana koristeći udaljenost točaka (što su točke bliže, više su slične) s obzirom da se radi o brojevnim podacima, no općenito podaci mogu biti bilo kakve prirode, što će pojedini algoritmi grupiranja uzimati u obzir.

Cilj ovog završnog rada je opisati različite algoritme grupiranja, pokrenuti ih na odabranim skupovima podataka, različitim metodama ih vrednovati i na taj način usporediti. Uz to će biti objašnjeno zašto neki algoritmi daju bolje rezultate od drugih algoritama na određenim skupovima podataka.

Ostatak rada organiziran je na sljedeći način: u poglavlju \ref{clusteringgeneral} spominju se općeniti pojmovi vezani za algoritme grupiranja te su navedene osnovne podjele algoritama grupiranja. Poglavlje \ref{clusteringalgos} opisuje četiri različita popularna modela i algoritma grupiranja. Osim opisa pojmova vezanih za svaki algoritam, spominje se njihova učinkovitost i kako ispravno odabrati parametre tih algoritama. Metode vrednovanja algoritama grupiranja dane su u poglavlju \ref{evaluation} gdje su opisane često korištene mjere koje ocjenjuju rezultate grupiranja. U poglavlju \ref{chap:results} odabrano je nekoliko skupova podataka, pokrenuti su algoritmi grupiranja nad njima i rezultati su vrednovani mjerama opisanim u poglavlju \ref{evaluation}. Konačno, zaključak je dan u poglavlju \ref{chap:conclusion}.

\chapter{Općenito o algoritmima grupiranja}
\label{clusteringgeneral}
U kontekstu algoritama grupiranja\footnote{Isto tako i u kontekstu nenadziranog strojnog učenja.} ulazni podaci (primjeri) su skup od $N$ neoznačenih, višedimenzionalnih \textbf{točaka} (vektora) \[\mathcal{D} = \left\{ \mathbf{x}^{(i)} \right\}_{i=1}^{N} \]
gdje svaka točka $\mathbf{x} = \left(x_1, x_2, \dots, x_{n-1}, x_n\right)$ ima $n$ \textbf{značajki} \engl{features}, odnosno dimenzija. Prostor podataka iz koje dolaze pojedine značajke, odnosno točke, mogu biti razne. Primjerice, može biti riječ o realnim brojevima (uvodni primjer), znakovima (primjerice grupiranje neoznačenih novinskih članaka), kategorijskim podacima poput vrijednosti istina/laž, itd. Neki algoritmi grupiranja imaju osnovne pretpostavke o tome iz kojeg prostora podataka dolaze značajke, što znači da ne funkcioniraju za one podatke koji nemaju ispunjene takve pretpostavke.

\section{Udaljenost, sličnost i različitost točaka}
\label{pointsimilarity}
U uvodnom poglavlju smo grupiranje je opisano kao postupak kojim se skup podataka razvrstava u grupe na taj način gdje su točke u istoj grupi međusobno manje-više ‘‘slični’’. Potrebno je definirati kako se određuje, odnosno kako se mjeri takva ‘‘sličnost’’ između dvije točke.

Neka je $\mathcal{V}$ prostor iz kojeg dolaze ulazni podaci čiji je $\mathcal{D}$ podskup, te neka je $\mathbb{R}$ skup realnih brojeva. \textbf{Udaljenost} \engl{distance measure} je funkcija
\[d: \mathcal{V} \times \mathcal{V} \to \mathbb{R}\]
koja za svaki \(\mathbf{x}, \mathbf{y}, \mathbf{z} \in \mathcal{V}\) zadovoljava tzv.\ svojstva \textbf{metrike}:
\begin{enumerate}
    \item \(d \left(\mathbf{x}, \mathbf{y}\right) = 0 \quad
                \text{akko} \quad \mathbf{x} = \mathbf{y}\)
    \item \(d \left(\mathbf{x}, \mathbf{y}\right) = d \left(\mathbf{y}, \mathbf{x}\right)\) \qquad (simetričnost)
    \item \(d \left(\mathbf{x}, \mathbf{z}\right) \leq 
            d \left(\mathbf{x}, \mathbf{y}\right)
            + d \left(\mathbf{y}, \mathbf{z}\right)\) \qquad (nejednakost trokuta)
\end{enumerate}
Iz navedenih aksioma metrike može se pokazati da vrijedi \(d \left(\mathbf{x}, \mathbf{y}\right) \geq 0\)
za svaki \(\mathbf{x}, \mathbf{y} \in \mathcal{V}\).
Konceptualno, dvije točke koje se više ‘‘razlikuju’’ u svom prostoru imaju veću udaljenost.
Ponekad su podaci takvi da su sve značajke realni brojevi, to jest $\mathcal{V} \subseteq \mathbb{R}^{n}$, tada je \textbf{Euklidova udaljenost} prikladna\footnote{Euklidska udaljenost nije jedina mjera udaljenosti za takav prostor, ali je najkorištenija.} mjera udaljenosti koja zadovoljava svojstva metrike:
\begin{equation}
    d \left(\mathbf{x}, \mathbf{y}\right) = 
    \sqrt{\sum_{i=1}^{n} \left(x_i - y_i\right)^2}
    \label{euclidean_distance}
\end{equation}
Općenito, ako je \(\mathcal{V}\) normirani vektorski prostor sa definiranom normom \(\Vert \cdot \Vert : \mathcal{V} \to \mathbb{R}\) tada se udaljenost može definirati pomoću te norme:
\begin{equation}
    d \left(\mathbf{x}, \mathbf{y}\right) = \left\Vert \mathbf{y} - \mathbf{x} \right\Vert
    \label{norm_distance}
\end{equation}
U slučaju realnih značajki i ako se koristi Euklidova norma, dobije se upravo \ref{euclidean_distance}.\\
Vrlo često se u praksi pojavljuju značajke koje nisu realni brojevi niti dolaze iz nekog drugog normiranog prostora. Tada se koriste neke druge mjere udaljenosti specijalizirane za specifične prostore \(\mathcal{V}\). Primjerice, postoji
\begin{itemize}
    \item \textbf{Hammingova udaljenost}: binarni nizovi duljine $n$
    \item \textbf{Jaccardova udaljenost}: skupovi i multiskupovi
    \item \textbf{??? udaljenost} \engl{Edit distance}: znakovni nizovi
    %TODO: PREVESTI EDIT DISTANCE
\end{itemize}
\textbf{Sličnost} \engl{similarity measure} je vrsta mjere koja, poput mjere udaljenosti, brojčano iskazuje koliko se razlikuju dvije točke. Za razliku od mjere udaljenosti, mjera sličnosti ne mora zadovoljavati sva svojstva metrike poput nejednakosti trokuta. Jednako vrijedi i za \textbf{mjeru različitosti} \engl{dissimilariy measure}. Kako su dvije točke ‘‘sličnije’’ tako mjera sličnosti raste, a mjera različitosti pada. Detaljna definicija mjera sličnosti i različitosti neće biti navedena u ovom poglavlju iz razloga što se u algoritmima grupiranja (barem onim koji će biti opisani) koriste uglavnom mjere udaljenosti u svrhu kvantificiranja sličnosti ili različitosti točaka.

\section{Vrste grupiranja}
\label{vrstegrupiranja}
Ne postoji jedinstvena definicija grupe koja vrijedi za sve algoritme grupiranja. Svaki algoritam ima svoj model grupiranja u kojem je definirano što je to grupa, a sam algoritam pokušava točke grupirati na način koji to najviše odgovara za taj model. Različiti modeli grupiranja će biti detaljnije razmotreni u sljedećem poglavlju.\\
Postoje dvije generalne podjele algoritama grupiranja. S obzirom na koji način se oblikuju grupe, grupiranje može biti:
\begin{itemize}
    \item \textbf{Hijerarhijsko grupiranje}
    \item \textbf{Particijsko grupiranje}
\end{itemize}
Kod hijerarhijskog grupiranja svaka grupa, počevši od grupe koja predstavlja cijeli skup točaka $\mathcal{D}$, ima podgrupe koje se tako rekurzivno dijele sve dok svaka točka nije svoja grupa. Na taj način se gradi hijerarhija grupa (od tud i naziv hijerarhijskog grupiranja) koji se može prikazati \textbf{dendrogramom}. Kod hijerarhijskog grupiranja postoje dva pristupa:
\begin{itemize}
    \item \textbf{Aglomerativno grupiranje}: početno je svaka točka u svojoj grupi pa se spajaju u veće grupe;
    \item \textbf{Divizivno grupiranje}: početna grupa je $\mathcal{D}$ pa se ona dijeli u manje podgrupe.
\end{itemize}
Na slici \ref{fig:hier_clustering} je prikazan tijek nekog hijerarhijskog aglomerativnog grupiranja nekog ulaznog skupa točaka.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.95]{img/hier.png}
    \caption{Tijek nekog hijerarhijskog aglomerativnog grupiranja}
    \label{fig:hier_clustering}
\end{figure}
Na lijevoj strani crnom bojom su označene i numerirane točke ulaznog skupa podataka, a crvenim brojevima je označen redoslijed kojim je postupak grupiranja spajao grupe u veće grupe. Na desnoj strani je postupak i rezultat grafički prikazan u obliku dendrograma. Na donjoj liniji su oznake točaka, a značenje brojeva na lijevoj liniji će biti objašnjeno u poglavlju koje detaljnije opisuje hijerarhijsko grupiranje. Ako se dendrogram promatra kao stablo čiji su čvorovi (pod)grupe, a listovi točke iz $\mathcal{D}$, tada aglomerativno grupiranje gradi dendrogram od listova prema korijenu, a divizivno grupiranje od korijena prema listovima.\\
Za razliku od hijerarhijskog grupiranja, particijsko grupiranje nema hijerarhiju grupa i podgrupa. Rezultat grupiranja je neki fiksiran broj grupa bez neke unutarnje strukture osim prikladnih točaka u njima. Najpopularniji i najučinkovitiji algoritmi su upravo particijska grupiranja.\\
Primjer na slici \ref{fig:partvshier} ilustrira razliku particijskog i hijerarhijskog grupiranja.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{img/partvshier.jpg}
    \caption{Usporedba particijskog i hijerarhijskog grupiranja}
    \label{fig:partvshier}
\end{figure}

Prirodne grupe točaka ponekad nije moguće jednoznačno odrediti. Grupiranja tada mogu, bilo grupiranje particijsko ili hijerarhijsko, neke točke svrstati ili u isključivo\footnote{Neki algoritmi grupiranja uključuju mogućnost stršećih vrijednosti i tada te točke nisu svrstane u nijednu grupu.} jednu grupu ili u više grupa istovremeno. Prema tome se grupiranja dijele na:
\begin{itemize}
    \item \textbf{Čvrsto grupiranje} \engl{hard clustering}: jedna točka može pripadati isključivo jednoj grupi;
    \item \textbf{Meko grupiranje} \engl{soft clustering}: jedna točka može pripadati više grupa sa nekom mjerom pripadnosti svakoj od tih grupa. Primjerice, mjera pripadnosti točke nekoj grupi se može prikazati kao vjerojatnost da ta točka pripada toj grupi.
\end{itemize}
Na slici \ref{fig:hard_vs_soft_clustering} vizualno je prikazana razlika između čvrstog i mekog grupiranja nekog skupa točaka. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/hard_vs_soft_clustering.png}
    \caption{Primjer čvrstog i mekog grupiranja}
    \label{fig:hard_vs_soft_clustering}
\end{figure} Primjerice crvenu točku u ovom primjeru je meko grupiranje svrstalo u dvije grupe istovremeno, a čvrsto grupiranje u isključivo jednu.

\chapter{Algoritmi grupiranja}
\label{clusteringalgos}
U daljnjim potpoglavljima opisujemo podskup algoritama grupiranja implementiranih u Pythonovoj programskoj knjižnici \textbf{Scikit-learn}, unutar modula za grupiranje \texttt{sklearn.cluster}. Svako potpoglavlje sadrži najprije opis modela grupiranja kojeg taj algoritam koristi, a zatim opis samog algoritma.
\section{Algoritam K-sredina} \label{kmeans}
Algoritam \textbf{K-sredina} \engl{K-means clustering} je jednostavan, učinkovit i najpopularniji algoritam particijskog čvrstog grupiranja koji ulazni skup točaka $\mathcal{D}$ particionira u $K$ grupa. $K$ je broj grupa koje je potrebno proizvesti i on se zadaje unaprijed kao parametar algoritma. U poglavlju \ref{subsection:odabirK} su opisane neke metode određivanja optimalnog broja grupa $K$.

Algoritam K-sredina predstavlja svaku grupu sa jednom zamišljenom točkom koja označava središte te grupe: \textbf{centroidom}. Neka je $\mathcal{C} \subseteq \mathcal{D}$ grupa koja sadrži $M$ točaka ($M = \vert \mathcal{C} \vert$, $\vert \mathcal{C} \vert$ predstavlja kardinalitet skupa $\mathcal{C}$), odnosno 
$\mathcal{C} = \left\{ \mathbf{x}^{(i)}  \right\}_{i=1}^{M}$.
Centroid te grupe $\boldsymbol{\mu}$ se definira kao:
\[\boldsymbol{\mu} = \frac{1}{\vert \mathcal{C} \vert} \sum_{\mathbf{x} \in \mathcal{C}} \mathbf{x} = 
\frac{1}{M} \sum_{i=1}^{M} \mathbf{x}^{(i)}\]
Centroid se računa na isti način kao i aritmetička sredina, odakle dolazi naziv K-sredina. Radi se o operacijama zbrajanja točaka te u konačnici dijeljenja sa nekim brojem. Dakle da bi uopće bilo moguće računati centroide, nad točkama, odnosno ulaznim podacima, koje potječu iz prostora $\mathcal{V}$, moraju biti definirane operacije zbrajanja i množenja sa skalarom (u našem slučaju realnim brojem $\frac{1}{M}$). Ovo razmatranje dovodi do pretpostavke modela u kojem prostor podataka $\mathcal{V}$ mora biti vektorski prostor\footnote{Vektorski prostor je skup objekata, odnosno vektora, nad kojim su definirane operacije međusobnog zbrajanja i množenja sa skalarom, a te operacije zadovoljavaju 8 aksioma vektorskog prostora.}. Standardni algoritam K-sredina je tada primjenjiv samo ako se radi o vektorskom prostoru. Najčešće će u pitanju biti podskup realnog koordinatnog prostora, odnosno $\mathcal{V} \subseteq \mathbb{R}^n$. 
Postoji i varijanta algoritma koja radi nad podacima bilo kakve prirode i koristi mjeru sličnosti (koja je općenitija, odnosno manje ‘‘zahtjevna’’ od mjere udaljenosti): \textbf{algoritam K-medoida}, no taj algoritam neće biti opisan u ovom radu.\\
Kao što pojam sugerira, centroid neke grupe konceptualno predstavlja centar, odnosno središte te grupe. Jasno je da centroid grupe se uopće ne mora nalaziti u grupi.

Cilj algoritma K-sredina jest minimizirati \textbf{kriterijsku funkciju}. Neka je ulazni skup točaka $\mathcal{D}$ čvrsto grupiran u $K$ grupa:
\[\mathcal{D} = \bigcup_{k=1}^{K} \mathcal{C}_k, \qquad \forall \left(i \neq j\right) :  \mathcal{C}_i \cap \mathcal{C}_j = \emptyset\]
Kriterijska funkcija mjeri raspršenje točaka unutar grupa tako da zbraja kvadratna odstupanja od centroida.
Kako se radi o čvrstom grupiranju, grupe su međusobno disjunktne, odnosno ne može se dogoditi da neka točka iz $\mathcal{D}$ završi u više grupa. Neka je $\boldsymbol{\mu}_k$ centroid grupe $\mathcal{C}_k$. Kriterijska funkcija za dano grupiranje kod algoritma K-sredina se definira izrazom:
\[J = \sum_{k=1}^{K} \sum_{\mathbf{x} \in \mathcal{C}_k} 
d \left(\mathbf{x}, \boldsymbol{\mu}_k\right)^2\]
Ako podaci dolaze iz podskupa realnog koordinatnog prostora, onda možemo koristiti Euklidsku udaljenost i Euklidsku normu, odnosno
$d \left(\mathbf{x}, \boldsymbol{\mu}_k\right) = \left\Vert \mathbf{x} - \boldsymbol{\mu}_k \right\Vert$. Tada kriterijska funkcija glasi:
\[J = \sum_{k=1}^{K} \sum_{\mathbf{x} \in \mathcal{C}_k} 
\Vert \mathbf{x} - \boldsymbol{\mu}_k \Vert^2\]
Kriterijska funkcija zbraja kvadratna odstupanja točaka\footnote{Zbog toga se u engleskoj literaturi $J$ navodi pod imenom \emph{Within-cluster sum of squares}, ili skraćeno WCSS.} od centroida grupe u kojima se nalaze. Što su točke bliže svojim centroidima to će iznos kriterijske funkcije biti manji, i to predstavlja bolje grupiranje kod algoritma K-sredina. Algoritam K-sredina nastoji minimizirati iznos kriterijske funkcije, odnosno particionirati (grupirati) skup $\mathcal{D}$ na način koji će dati minimalan iznos kriterijske funkcije. Analitičkim postupcima se ne može pronaći minimum kriterijske funkcije, stoga se optimizacija provodi iterativno, a iterativni postupak optimizacije nudi algoritam K-sredina.

\subsection{Opis algoritma}
Najprije se inicijalizira početnih $K$ centroida, te se u svakoj iteraciji sve točke iz $\mathcal{D}$ pridružuju najbližem centroidu. Zatim se centroidi svake grupe ponovno računaju na temelju točaka iz te grupe (pridruženi starom centroidu koji im je bio najbliži). Postupak se ponavlja do konvergencije, to jest dok dvije uzastopne iteracije nisu ništa promijenile u smislu da sve točke zadržavaju svoje grupe i ponovno računanje svih centroida ih ne mijenja.
\begin{croatianalgorithm}[H]
\caption{Algoritam K-sredina}
\label{algo:k-means}
\begin{algorithmic}
\STATE{\textbf{Parametri}: broj grupa $K$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} čvrste disjunktne grupe $\mathcal{C}_k, \mathcal{D} = \bigcup_{k=1}^{K} \mathcal{C}_k$}
\STATE
\STATE{\textbf{inicijaliziraj} centroide $\boldsymbol{\mu}_k, \; k \in \left\{1, \dots, K\right\}$}
\REPEAT
\STATE{$\mathcal{C}_k \gets \emptyset, \; k \in \left\{1, \dots, K\right\}$}
\FORALL{$\mathbf{x}^{(i)} \in \mathcal{D}$}
\STATE{$k \gets \argmin_{j \in \left\{1, \dots, K\right\}} \left\Vert \mathbf{x}^{(i)} - \boldsymbol{\mu}_j \right\Vert$}
\STATE{$\mathcal{C}_k \gets \mathcal{C}_k \cup \left\{\mathbf{x}^{(i)}\right\}$ }
\ENDFOR
\FORALL{$k \in \left\{1, \dots, K\right\}$}
\STATE{$\boldsymbol{\mu}_k \gets \frac{1}{\vert \mathcal{C}_k \vert} \sum_{\mathbf{x} \in \mathcal{C}_k} \mathbf{x}$}
\ENDFOR
\UNTIL{$\boldsymbol{\mu}_k$ ne konvergiraju}
\end{algorithmic}
\end{croatianalgorithm}

Na slici \ref{fig:kmeans_demo} je na jednostavnom primjeru prikazan princip rada algoritma sa zadanim brojem grupa $K = 3$. Križići predstavljaju centroide, a točke iste boje su pridružene istoj grupi.
\begin{figure}[H]
    \centering
    \includegraphics{img/kmeans_demo.png}
    \caption{Demonstracijski primjer grupiranja algoritmom K-sredina}
    \label{fig:kmeans_demo}
\end{figure}
Potrebno je napomenuti da je algoritam kao početne centroide odabrao već postojeće točke, no to općenito ne mora biti slučaj. Metode odabira početnih centroida su raspravljene u poglavlju \ref{kminitial}.

\subsection{Svojstva i složenost algoritma}
\label{kmsvojstva}
Kao što je spomenuto na početku poglavlja \ref{kmeans}, cilj algoritma K-sredina jest minimizirati kriterijsku funkciju. Postavlja se pitanje, hoće li algoritam u tome uvijek i uspjeti? Odgovor je: neće. Algoritam će pronaći lokalni optimum, ali ne garantira da će to biti i globalni optimum. Velik utjecaj na konačni rezultat ima upravo prvi korak algoritma: inicijalizacija početnih $K$ centroida. Kako bi se osigurao što bolji rezultat, odnosno što manji iznos kriterijske funkcije, jako je važno na pametan način odabrati početne centroide. Jednako tako je važno odabrati ispravan broj grupa: parametar $K$.

Vremenska složenost algoritma K-sredina je $\mathcal{O} \left(TnKN\right)$, gdje je $T$ broj iteracija algoritma do konvergencije, $n$ broj značajki (dimenzija) točaka, a $N$ broj točaka. Korak algoritma u kojem se točke pridružuju najbližem centroidu je složenosti $\mathcal{O} \left(nKN\right)$ jer je potrebno za svaku od $N$ točaka ispitati koji od $K$ trenutnih centroida je najbliži, dakle mora ispitati udaljenost od svakog centroida, a računanje udaljenosti je složenosti $\mathcal{O} \left(n\right)$. Korak algoritma u kojem se računaju centroidi je složenosti $\mathcal{O} \left(nN\right)$ jer iako se iterira po grupama, efektivno se iterira po svim ulaznim točkama i obavljaju se operacije zbrajanja i dijeljenja sa nekim brojem (koje su također složenosti $\mathcal{O} \left(n\right)$).\\
Takva složenost je prihvatljiva i algoritam uz dobar odabir početnih centroida i dobar odabir parametra $K$ proizvodi dobre rezultate. Zbog povoljne složenosti, algoritam je učinkovit i za velike količine podatka (veliki $N$), a broj grupa i značajka ionako su najčešće\footnote{Kod velikog broja značajki tradicionalni algoritmi grupiranja, ali i postupci obrade podataka općenito, nisu učinkoviti. Tada se radi o problemu prokletstva visoke dimenzionalnosti \engl{curse of dimensionality}} puno manji od $N$.

\subsection{Odabir početnih $K$ centroida}
\label{kminitial}
Način na koji se početni centroidi odabiru kod algoritma K-sredina nije jasno naznačen u algoritmu, a u poglavlju \ref{kmsvojstva} je spomenuto da je to jako važno kako bi algoritam što više smanjio iznos kriterijske funkcije.

Postoji niz strategija za odabir početnih $K$ centroida. Prvo mogućnost jest kao početne centroide postaviti $K$ slučajno izabranih točaka iz $\mathcal{D}$. Slučajan odabir nije baš najbolja ideja, jer rezultati neće biti dobri ako neki od početnih $K$ centroida se nalaze unutar istih prirodnih grupa.\\
Početni centroidi se mogu izabrati na način da samo prvi centroid bude slučajno odabrana točka iz $\mathcal{D}$, a daljnji centroidi se odabiru isto iz $\mathcal{D}$ (ali među onima koji nisu još odabrani kao centroidi) na način da su što udaljeniji od već odabranih centroida. Kao sljedeći uzastopni centroid odabire se točka koja maksimizira udaljenost do najbližeg, već odabranog centroida. Takva strategija odabira opisana je u nastavku. 
\begin{croatianalgorithm}[H]
\begin{algorithmic}
\STATE{\textbf{Parametri}: broj grupa $K$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} skup inicijalnih $K$ centroida: 
$\mathcal{I} = \left\{ \boldsymbol{\mu}_k \right\}_{k=1}^{K}$}
\STATE
\STATE{$\mathcal{I} \gets \emptyset$}
\STATE{$\widehat{\mathcal{D}} \gets \mathcal{D}$}
\STATE{\textbf{slučajno odaberi} točku $\mathbf{x} \in \widehat{\mathcal{D}}$}
\STATE{$\mathcal{I} \gets \mathcal{I} \cup \left\{ \mathbf{x} \right\}$}
\STATE{$\widehat{\mathcal{D}} \gets \widehat{\mathcal{D}} \setminus
        \left\{ \mathbf{x} \right\}$}
\WHILE{dokle god $\left\vert \mathcal{I} \right\vert < K$}
\STATE{$\boldsymbol{\mu} \gets \argmax_{\textbf{x} \in \widehat{\mathcal{D}}} 
        \left\Vert \mathbf{x} - \argmin_{\boldsymbol{\mu}' \in \mathcal{I}}
                \Vert \mathbf{x} - \boldsymbol{\mu}' \Vert
        \right\Vert$}
\STATE{$\mathcal{I} \gets \mathcal{I} \cup \left\{ \boldsymbol{\mu} \right\}$}
\STATE{$\widehat{\mathcal{D}} \gets \widehat{\mathcal{D}} \setminus
        \left\{ \boldsymbol{\mu} \right\}$}
\ENDWHILE
\RETURN{$\mathcal{I}$}
\end{algorithmic}
\end{croatianalgorithm}
Ova strategija odabira početnih centroida je u pravilu dobra, no postoji jedna mana: odabirat će se stršeće vrijednosti, a kako su one relativno udaljene od prirodnih grupa, to bi moglo uzrokovati loše grupiranje. Postoji alternativa ovakvoj strategiji koja do neke mjere rješava problem odabira stršećih vrijednosti.

Umjesto odabira točke koja maksimizira udaljenost do najbližeg centroida, druga mogućnost je svakoj točki pridružiti vjerojatnost odabira koja je proporcionalna kvadratu udaljenosti do najbližeg centroida. Takva strategija odabira je poznata kao \textbf{K-means++}.
Pretpostavimo da je u nekom trenutku već odabrano $k$ centroida i one se nalaze u skupu $\mathcal{I}$. Vjerojatnost odabira neke točke $\mathbf{x} \in \widehat{\mathcal{D}}$ kao sljedeći centroid glasi:
\[P \left( \boldsymbol{\mu}_{k+1} = \mathbf{x} \middle| \widehat{\mathcal{D}}, \mathcal{I}\right)
= \frac{\min_{\boldsymbol{\mu}' \in \mathcal{I}} \Vert \mathbf{x} - \boldsymbol{\mu}' \Vert^2}{\sum_{\mathbf{x}' \in \widehat{\mathcal{D}}} 
\min_{\boldsymbol{\mu}' \in \mathcal{I}} \Vert \mathbf{x}' - \boldsymbol{\mu}' \Vert^2}\]
Nakon što se izračunaju vjerojatnosti za svaku od neodabranih točaka iz $\widehat{\mathcal{D}}$, onda se točka slučajno odabire prema navedenoj razdiobi vjerojatnosti. Stršeće vrijednosti će i dalje imati najveću vjerojatnost odabira, ali stršećih vrijednosti nema mnogo, pa je ipak vjerojatniji odabir neki od prirodno prosječnih primjera.\\
K-means++ u praksi daje bolje rezultate od varijante K-sredina u kojem se svih $K$ početnih centroida odabire slučajno. Iznosi kriterijskih funkcija ispadaju manji i algoritam prije konvergira, odnosno potreban je manji broj iteracija do stacionarnog stanja.

\subsection{Odabir broja grupa $K$}
\label{subsection:odabirK}
Kako bi se algoritam K-sredina mogao pokrenuti, potrebno je kao parametar $K$ specificirati koliko grupa treba proizvesti. Idealno se postavlja $K$ jednak broju prirodnih grupa, no to najčešće unaprijed nije poznato. Također postoji mogućnost da optimalan broj grupa $K$ nije jednoznačno određen jer postoji više načina prirodnog grupiranja podataka.

Ovo poglavlje opisuje često korištenu metodu za određivanje optimalnog broja grupa kod algoritma K-sredina: \textbf{metodu koljena} \engl{elbow method}. Još jedna metoda bit će opisana u poglavlju \ref{subsec:silhouette} nakon što se spomenu potrebni pojmovi.
\subsubsection{Metoda koljena}
Metoda koljena je grafička metoda koja prati ovisnost kriterijske funkcije $J$ (nakon što završi algoritam K-sredina) o broju grupa $K$. Jasno je da porast $K$ smanjuje iznos $J$ jer se grupe ‘‘smanjuju’’ povećanjem broja grupa u smislu da im se smanjuje raspršenost od centroida. U krajnjem slučaju, ukoliko se odabere broj grupa $K = N$, tada će svaka ulazna točka biti vlastita grupa i onda vrijedi $J = 0$.

Neka je na raspolaganju neki skup podataka sa dvjema realnim značajkama. Grafički prikaz tih podataka u dvodimenzijskom koordinatnom sustavu prikazan je na slici \ref{fig:elbowdata}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/elbowfig.png}
    \caption{Primjer skupa dvodimenzijskih podataka}
    \label{fig:elbowdata}
\end{figure} Cilj je metodom koljena odrediti optimalan broj grupa za algoritam K-sredina. Neka su mogući kandidati $K \in \left\{2, \dots, 8\right\}$. Za svaku od tih vrijednosti pokreće se algoritam K-sredina (uz strategiju K-means++ i Euklidovom udaljenosti) nad ovim skupom podataka i računa konačan iznos kriterijske funkcije $J$. Rezultati su prikazani grafom na slici \ref{fig:elbowgraph}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/elbowgraph.png}
    \caption{Ovisnost kriterijske funkcije o broju grupa}
    \label{fig:elbowgraph}
\end{figure}
Porast broja grupa do 4 bilježi strmi pad iznosa kriterijske funkcije, a daljnim porastom se ne dobiva značajan pad iznosa kriterijske funkcije. Graf kod broja grupa 4 ima oblik koljena, otkud dolazi naziv metode koljena. Navedeno razmatranje je jaka naznaka da se radi o optimalnom broju grupa $K = 4$. Broj grupa manji od toga naglo povećava iznos kriterijske funkcije, a broj grupa veći od toga ne rezultira značajnim smanjenjima kriterijske funkcije. Zaista, vizualnom metodom je jasno da se u navedenom primjeru radi o 4 prirodne grupe.


\section{Model Gaussove mješavine}
\label{gmm}
\textbf{Model Gaussove mješavine} \engl{Gaussian mixture model, GMM} je najčešće korišten algoritam mekog probabilističkog grupiranja koji kao cilj ima svakoj točki iz $\mathcal{D}$ pridružiti vjerojatnosti da pripada različitim grupama: tzv.\ \textbf{odgovornost}. Model Gaussove mješavine pretpostavlja da su podaci ‘‘nastali’’ iz kombinacije (mješavine) $K$ različitih vjerojatnosnih distribucija, odnosno gustoća. Svaka od $K$ grupa je predstavljena svojom vjerojatnosnom gustoćom, a u slučaju Gaussovih mješavina, radi se o multivarijatnim $n$-dimenzijskim normalnim (Gaussovim) razdiobama. To znači da je svaka grupa predstavljena svojom višedimenzijskom normalnom razdiobom: svojom lokacijom, kovarijacijskom matricom i vjerojatnosnom gustoćom.

\subsection{Multivarijatna normalna razdioba}
Slučajni $n$-dimenzijski stupčani vektor $\mathbf{X} = \left(X_1, \dots, X_n\right)^{\mathrm{T}}$ ima multivarijatnu normalnu razdiobu, odnosno
\[\mathbf{X} \sim \mathcal{N} \left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right)\]
sa \textbf{lokacijom}
$\boldsymbol{\mu} = \left(\operatorname{E}[X_1], \dots, \operatorname{E}[X_n]\right)^{\mathrm{T}}$ i \textbf{kovarijacijskom matricom} $\boldsymbol{\Sigma}$ kada svaka komponenta $X_i$ ima univarijatnu normalnu razdiobu. Kovarijacijska matrica je simetrična, pozitivno semidefinitna matrica reda $n \times n$ čiji elementi predstavljaju kovarijancu između komponenata, odnosno
\[\boldsymbol{\Sigma}_{i,j} = \operatorname{cov}
\left[X_i, X_j\right] 
= \operatorname{E}\left[ 
            \left(X_i - \operatorname{E}[X_i]\right)
            \left(X_j - \operatorname{E}[X_j]\right)
\right]
\]
Funkcija gustoće takvog slučajnog vektora tada glasi:
\begin{equation}
\label{gaussdensity}
p \left(\mathbf{x} \middle| \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)
= \frac{1}{\sqrt{\left(2 \pi\right)^n \operatorname{det} \left(\boldsymbol{\Sigma}\right)}}
\exp{\left(
    -\frac{1}{2}
        \left(\mathbf{x} - \boldsymbol{\mu}\right)^{\mathrm{T}}
        \boldsymbol{\Sigma}^{-1}
        \left(\mathbf{x} - \boldsymbol{\mu}\right)
\right)}
\end{equation}
Izraz
$\left(\mathbf{x} - \boldsymbol{\mu}\right)^{\mathrm{T}}
        \boldsymbol{\Sigma}^{-1}
        \left(\mathbf{x} - \boldsymbol{\mu}\right)
$ je kvadrat takozvane Mahalanobisove udaljenosti između vektora $\mathbf{x}$ i lokacije $\boldsymbol{\mu}$. To je višedimenzijska generaliziracija ideje udaljenosti točke od srednje vrijednosti u broju standardnih devijacija kod jednodimenzijskih razdioba.
\subsection{Model miješane gustoće}
Model miješane gustoće, pa tako i model Gaussove mješavine, je \textbf{generativan model} podataka koji svaki primjer generira, odnosno uzorkuje, iz miješane gustoće: linearne kombinacije $K$ gustoća svake grupe, odnosno $K$ komponenata. Svaki podatak u tom modelu se uzorkuje na sljedeći način: prvo se prema kategoričkoj distribuciji grupa, gdje se grupe sada prikazuju apriorno nepoznatim oznakama $y$ (jer se radi o nenadziranom strojnom učenju), odabere grupa $k$. Neka je $P \left(y = k\right)$ vjerojatnost da je odabrana grupa $k$. Nakon što se odabere grupa, primjer $\mathbf{x}$ se onda uzorkuje prema vjerojatnosnoj gustoći te grupe koju označavamo sa $p \left(\mathbf{x} \middle| y = k\right)$, odnosno ako se označe parametri distribucije (gustoće) grupe $k$ sa $\boldsymbol{\theta}_k$, onda se ta vjerojatnost označava sa $p \left(\mathbf{x} \middle| \boldsymbol{\theta}_k\right)$. Apriorna gustoća $p \left(\mathbf{x}\right)$ iz koje su uzorkovani ulazni primjeri iskaže se formulom potpune vjerojatnosti:
\begin{equation}
\label{mixdensity}
p \left(\mathbf{x}\right)
=
\sum_{k=1}^{K} P \left(y = k\right) p \left(\mathbf{x} \middle| y = k\right)
=
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x} \middle| \boldsymbol{\theta}_k\right)
\end{equation}
Uvedena je oznaka oznaku $\pi_k = P \left(y = k\right)$, i ta vjerojatnost se naziva \textbf{koeficijent mješavine} \engl{mixture weight}. Jasno je da mora vrijediti
\[\sum_{k=1}^{K} \pi_k = 1\]
U slučaju modela Gaussove mješavine, gustoća grupe $p \left(\mathbf{x} \middle| \boldsymbol{\theta}_k\right)$ se računa prema \ref{gaussdensity}, gdje su parametri grupe
$\boldsymbol{\theta}_k = \left\{
    \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k
\right\}$.\\
Cilj mekog grupiranja kod ovakvog modela jest odrediti vjerojatnost da primjer $\mathbf{x}$ pripada nekoj grupi $k$, što se označava kao $P \left(y = k \middle| \mathbf{x}\right)$ i naziva se odgovornost. Za svaki podatak $\mathbf{x}^{(i)}$ iz skupa od $N$ ulaznih primjera $\mathcal{D} = \left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N}$ računamo odgovornost: vjerojatnost da je $\mathbf{x}^{(i)}$ uzorkovan iz grupe $k$. Ta vjerojatnost se može izraziti Bayesovom formulom:
\begin{equation}
\label{odgovornost}
P \left(y = k \middle| \mathbf{x}^{(i)}\right)
=
\frac
{P \left(y = k\right) p \left(\mathbf{x}^{(i)} \middle| y = k\right)}
{p \left(\mathbf{x}^{(i)}\right)}
=
\frac
{\pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)}
{\sum_{j=1}^{K} \pi_j \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_j\right)}
\end{equation}
Kako bi se mogla računati ovakva vjerojatnost, potrebno je poznavati parametre modela miješane gustoće. To znači da je za svaku od $K$ komponenata potrebno poznavati koeficijent mješavine i parametre gustoće svake komponente. U slučaju Gaussove mješavine, parametri svake komponente su lokacije i kovarijacijske matrice svake komponente. Neka su parametri modela predstavljeni oznakom
\[\boldsymbol{\theta} 
= \left\{ \pi_k, \boldsymbol{\theta}_k \right\}_{k=1}^{K}
= \left\{ \pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \right\}_{k=1}^{K}
\]
Sljedeće poglavlje se bavi pitanjem na koji način i kojim kriterijem se određuju parametri modela $\boldsymbol{\theta}$.

\subsection{Metoda najveće izglednosti}
Parametri modela $\boldsymbol{\theta}$ se procjenjuju \textbf{metodom najveće izglednosti} \engl{maximum likelihood estimation}. Potrebno je definirati funkciju izglednosti parametara $\boldsymbol{\theta}$ na uzorku $\mathcal{D}$. Pretpostavimo da su primjeri iz uzorka nezavisni, tada se funkcija izglednosti može definirati kao umnožak gustoća svakog primjera iz uzorka:
\[L \left(\boldsymbol{\theta} ; \mathcal{D} \right)
= \prod_{i=1}^{N} p \left( \mathbf{x}^{(i)} \right)
= \prod_{i=1}^{N} 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)
\]
Iz definicije i iz \ref{mixdensity} je jasno da funkcija izglednosti ovisi o parametrima $\boldsymbol{\theta}$.

Cilj metode najveće najveće izglednosti jest procijeniti parametre $\boldsymbol{\theta}$, koji će uz uzorak $\mathcal{D}$ maksimizirati funkciju izglednosti. Drugim riječima, potrebno je pronaći parametre koje od svih mogućih parametara (tzv.\ prostor parametara $\boldsymbol{\Theta}$) maksimiziraju vjerojatnost pojavljivanja podataka iz uzorka $\mathcal{D}$. Formalno, traži se
\[\widehat{\boldsymbol{\theta}}_{\text{mle}} =
\argmax_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
L \left(\boldsymbol{\theta} ; \mathcal{D} \right)
\]
Vrlo često u praksi se ne maksimizira funkcija izglednosti, nego \textbf{log-izglednosti}:
\begin{equation}
\label{loglikelihood}
\ell \left(\boldsymbol{\theta} ; \mathcal{D} \right) 
= \ln {L \left(\boldsymbol{\theta} ; \mathcal{D} \right)}
= \ln{\prod_{i=1}^{N} 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)}
= \sum_{i=1}^{N} \ln{ 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)
}
\end{equation}
Razlog tomu je taj što se primjerice u ovom slučaju radi sa produktima, a lakše je raditi sa sumama, a logaritmiranje produkte pretvara u sume. Zbog toga što je prirodni logaritam monotono rastuća funkcija, kad god se postiže maksimum funkcije izglednosti, tada i log-izglednost postiže svoj maksimum:
\[\widehat{\boldsymbol{\theta}}_{\text{mle}} =
\argmax_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
L \left(\boldsymbol{\theta} ; \mathcal{D} \right)
= \argmax_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
\ell \left(\boldsymbol{\theta} ; \mathcal{D} \right)
= \argmax_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
\sum_{i=1}^{N} \ln{ 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)
}
\]
Ovakav optimizacijski problem nema rješejne u zatvorenoj formi, stoga su potrebne iterativne optimizacijske metode koje će pokušati maksimizirati log-izglednost. Jedan od takvih metoda je \textbf{algoritam maksimizacije očekivanja}.

\subsection{Algoritam maksimizacije očekivanja}
Algoritam maksimizacije očekivanja \engl{expectation-maximization algorithm, EM} se osniva na proširenju generativnog modela opisanog u \ref{gmm} sa tzv.\ \textbf{skrivenim (latentnim) varijablama} koje modeliraju vrijednosti koje se ne opažaju u podacima. Takvo proširenje modela u konačnici omogućuje pronalazak iterativnog postupka koji postupno povećava log-izglednost. Proširenje modela i matematičke formulacije iza njega neće biti navedeni, nego će samo biti iskazan konačni algoritam.

EM algoritam ima vrlo sličnu strukturu kao i algoritam K-sredina. Najprije se na neki način inicijaliziraju parametri modela Gaussove mješavine $\boldsymbol{\theta} = 
\left\{\pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right\}_{k=1}^{K}$ kao što je algoritam K-sredina inicijalizirao početnih $K$ centroida. Broj komponenata Gaussove mješavine $K$ je, kao i kod algoritma K-sredina, parametar algoritma (razlikovati od parametara modela) i on u smislu algoritma mora biti unaprijed poznat. Nakon što su inicijalizirani parametri modela, iterativni postupak počinje. Najprije se za sve ulazne primjere računa odgovornost za svaku komponentu, odnosno grupu, prema formuli \ref{odgovornost}. Taj korak kod EM algoritma se naziva \textbf{E-korak}. U usporedbi sa algoritmom K-sredina, ovo je analogno pridruživanju točaka najbližim centroidima, što je bilo ‘‘čvrsto’’ pridruživanje, a sada se radi o probabilističkom mekom pridruživanju. Nakon što se izračunaju odgovornosti, parametri modela se iznova računaju na temelju novih izračunatih odgovornosti, što je analogno ponovnom računanju centroida kod algoritma K-sredina. Taj korak se naziva \textbf{M-korak}. Koraci E i M se ponavljaju do konvergencije parametara ili log-izglednosti. Kao oznaku odgovornosti korištena je oznaka
\[h_{k}^{(i)} = P \left(y = k \middle| \mathbf{x}^{(i)}\right)\]

\begin{croatianalgorithm}[H]
\label{em}
\caption{Algoritam maksimizacije očekivanja nad GMM}
\begin{algorithmic}

\STATE{\textbf{Parametri:} broj grupa, odnosno komponenata $K$}
\STATE{\textbf{Ulaz:} skup ulaznih primjera 
$\mathcal{D} = \left\{ \mathbf{x}^{(i)} \right\}_{i=1}^{N}$}
\STATE{\textbf{Izlaz:} parametri modela
$\boldsymbol{\theta} = \left\{ 
    \pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k
\right\}_{k=1}^{K}$}
\STATE{}

\STATE{\textbf{inicijaliziraj} parametre modela 
$\boldsymbol{\theta} = \left\{ 
    \pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k
\right\}_{k=1}^{K}$}

\REPEAT
\STATE{\textbf{E-korak:}}
\FORALL{ $\mathbf{x}^{(i)} \in \mathcal{D}$ i 
    $k \in \left\{1, \dots, K\right\}$ }
    \STATE{$h_{k}^{(i)} \gets 
    \frac
{\pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)}
{\sum_{j=1}^{K} \pi_j \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j\right)}
    $}
\ENDFOR
\STATE{}

\STATE{\textbf{M-korak:}}
\FORALL{ $k \in \left\{1, \dots, K\right\}$ }
    \STATE{$\boldsymbol{\mu}_k
    \gets \frac{\sum_{i=1}^{N} h_{k}^{(i)} \mathbf{x}^{(i)}}
    {\sum_{i=1}^{N} h_{k}^{(i)}}
    $}
    \STATE{$\boldsymbol{\Sigma}_k \gets
    \frac
    {\sum_{i=1}^{N} h_{k}^{(i)}
        \left( \mathbf{x}^{(i)} - \boldsymbol{\mu}_k \right)
        \left( \mathbf{x}^{(i)} - \boldsymbol{\mu}_k \right)^{\mathrm{T}}}
    {\sum_{i=1}^{N} h_{k}^{(i)}}$}
    \STATE{$\pi_k \gets 
    \frac{1}{N} \sum_{i=1}^{N} h_{k}^{(i)}$}
\ENDFOR
\STATE{}
\STATE{$\ell \left(\boldsymbol{\theta} ; \mathcal{D} \right) 
\gets \sum_{i=1}^{N} \ln{ 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)
}$}
\UNTIL{ne konvergiraju parametri $\boldsymbol{\theta}$ ili log-izglednost $\ell \left(\boldsymbol{\theta} ; \mathcal{D} \right)$}
\RETURN{$\boldsymbol{\theta}$}

\end{algorithmic}
\end{croatianalgorithm}
Iz ‘‘natreniranih’’ parametara modela se lako računaju odgovornosti iz formule \ref{odgovornost}, što je rezultat mekog grupiranja:
\[
P \left(y = k \middle| \mathbf{x}^{(i)}\right) =
h_{k}^{(i)} =
\frac
{\pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)}
{\sum_{j=1}^{K} \pi_j \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j\right)}
\]
Na slici \ref{fig:gmmem} je prikazan tijek rada algoritma nad dvodimenzijskim primjerima sa zadanim brojem komponenata $K = 2$. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/gmmem.jpg}
    \caption{Tijek rada algoritma maksimizacije očekivanja nad modelom Gaussovih mješavina}
    \label{fig:gmmem}
\end{figure}
Nijanse boja točaka označuju u kojoj mjeri točka pripada pojedinim grupama. Nakon svake iteracije se lokacije (zamišljene točke sa crnim obrubom) i kovarijacijske matrice (vizualizirane elipsama, sve točke na elipsi imaju istu Mahalanobisovu udaljenost od lokacije) mijenjaju dok se ne postigne stacionarno stanje: konvergencija log-izglednosti ili parametara modela. 

\subsection{Svojstva i složenost algoritma}
Algoritam maksimizacije očekivanja nastoji maksimizirati log-izglednost opisanu formulom \ref{loglikelihood}, ali u tome neće uvijek uspjeti. Pronaći će lokalni maksimum log-izglednosti, ali to ne mora biti globalni maksimum. Rezultat uvelike ovisi o kvalitetnoj inicijalizaciji parametara $\boldsymbol{\theta}$. Slučajna inicijalizacija je moguća, ali najčešće se parametri inicijaliziraju tako da se najprije nad skupom primjera provede algoritam K-sredina koji će kao rezultat dati $K$ čvrstih grupa. Lokacije $\boldsymbol{\mu}_k$ svake komponente se inicijaliziraju centroidima dobivenih grupa, a koeficijenti mješavine $\pi_k$ se računaju kao udjeli broja primjera u grupi $k$ u odnosu na ukupan broj primjera $N$. Kovarijacijske matrice $\boldsymbol{\Sigma}_k$ se mogu inicijalizirati procjenama kovarijanci iz uzorka: primjera iz grupe $k$.

Što se tiče vremenske složenosti, algoritam je učinkovit jer je linearan sa brojem primjera i brojem komponenata. Računanje Gaussove gustoće, uz prethodno izračunatu determinantu i inverz kovarijacijske matrice\footnote{Takozvana matrica preciznosti ili samo preciznost.}, je $\mathcal{O} \left(n^2\right)$. Prema tome, E-korak je vremenske složenosti $\mathcal{O} \left(N K n^2\right)$ i prostorne složenosti $\mathcal{O} \left(N K\right)$. Što se tiče M-koraka, za svaku komponentu, računanje nove lokacije je $\mathcal{O} \left(N n\right)$, računanje nove kovarijacijske matrice je $\mathcal{O} \left(N n^2\right)$, a računanje novog koeficijenta mješavine je $\mathcal{O} \left(N\right)$. U opisu algoritma nije eksplicitno navedeno, ali potrebno je i računati inverz i determinantu nove kovarijacijske matrice jer su potrebni za kasnije računanje gustoće, što je u općenitom slučaju $\mathcal{O} \left(n^3\right)$. Dakle, M-korak je složenosti $\mathcal{O} \left(K \left(N n^2 + n^3\right)\right)$. Računanje log-izglednosti je vremenske složenosti $\mathcal{O} \left(N K n^2\right)$. Najzahtjevniji od svih tih koraka je M-korak, stoga je uz $T$ iteracija vremenska složenost $\mathcal{O} \left(T K \left(N n^2 + n^3\right)\right)$.

Složenost se može poboljšati ako se uvedu ograničenja na oblik kovarijacijske matrice, što pojednostavljuje model Gaussove mješavine i operacije poput matričnog množenja, invertiranja i računanje determinante postaju jednostavnije. Tako primjerice kovarijacijske matrice mogu biti
\begin{enumerate}
    \item \textbf{Izotropne}: svaka komponenta ima kovarijacijsku matricu oblika $\boldsymbol{\Sigma}_k = \sigma_{k}^2 \, \mathbf{I}$, gdje je $\mathbf{I}$ jedinična matrica, a $\sigma_{k}^2$ dijeljena (između značajki) varijanca svake komponente. Značajke svake komponente su tada nekorelirane, imaju jednaku varijancu i grupe su sferičnog oblika.
    \item \textbf{Dijeljene}: svaka komponenta ima istu kovarijacijsku matricu.
    \item \textbf{Dijagonalne}: značajke komponente su nekorelirane.
\end{enumerate}
Algoritam je učinkovit i na velikim skupovima primjera zbog toga što je složenost u svakom slučaju linearan sa brojem primjera.


\section{Hijerarhijsko aglomerativno grupiranje}
U poglavlju \ref{vrstegrupiranja} je navedeno na koji način hijerarhijsko grupiranje gradi grupe, te su spomenuta dva pristupa hijerarhijskog grupiranja: aglomerativno i divizivno. Ovo poglavlje se se bavi standardnim algoritmom \textbf{hijerarhijskog aglomerativnog grupiranja} \engl{hierarchical agglomerative clustering, HAC}. Također je u poglavlju \ref{vrstegrupiranja} dan primjer jednog hijerarhijskog aglomerativnog grupiranja na slici \ref{fig:hier_clustering}. U tom primjeru je prikazano kako su se grupe međusobno spajale. Odluka koje dvije grupe spojiti nije slučajna, stoga prije formalnog opisa algoritma potrebno je prvo definirati pojmove vezane za spajanje grupa.

\subsection{Spajanje grupa}
Odluka koje dvije grupe spojiti se osniva na \textbf{kriteriju spajanja} \engl{linkage criterion}. Kriterij spajanja mjeri koliko su dvije grupe slične, odnosno različite. On se osniva na nekoj od mjera sličnosti između dviju točaka, bila to udaljenost, sličnost ili različitost, i one su opisane u poglavlju \ref{vrstegrupiranja}. Najčešće se u svrhu definiranja kriterija spajanja koriste mjere udaljenosti, no mogu se koristiti i mjere sličnosti ili različitosti.

Kriterij spajanja u ovom radu neće biti detaljno opisani kao mjera udaljenosti, samo na površnoj razini. Neka su su $\mathcal{C}_i$, $\mathcal{C}_j \subset \mathcal{D}$ neprazni i disjunktni podskupovi skupa ulaznih točaka $\mathcal{D}$. Kriterij povezanosti označimo sa $D \left(\mathcal{C}_i, \mathcal{C}_j\right)$ (bitno je ovu oznaku razlikovati od oznake udaljenosti između dvije točke, npr.\ $d \left(\mathbf{x}, \mathbf{y}\right)$) i on mjeri koliko su grupe $\mathcal{C}_i$ i $\mathcal{C}_j$ udaljene. Postoji mnogo kriterija spajanja, u ovom poglavlju se navode one koje su implementirane u programskoj knjižnici Scikit-learn.

\textbf{Jednostruka povezanost} \engl{single-linkage clustering} udaljenost između grupa definira kao minimalnu udaljenost između točaka u tim grupama, gdje je jedna točka iz jedne, a druga točka iz druge grupe:
\[D_{\text{min}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \min_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Potpuna povezanost} \engl{complete-linkage clustering} udaljenost između grupa definira slično kao i jednostruka povezanost, ali ovaj put se gleda maksimalna udaljenost:
\[D_{\text{max}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \max_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Prosječna povezanost} \engl{average-linkage clustering} udaljenost između grupa definira kao prosječnu udaljenost svih parova grupa, opet gdje je jedna točka iz jedna druga točka iz druge grupe:
\[D_{\text{avg}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \frac{1}{\vert \mathcal{C}_i \vert \cdot \vert \mathcal{C}_j \vert} 
            \sum_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Wardova metoda}, također poznata kao Wardova metoda minimalne varijance, prati zbroj kvadratnih pogrešaka (između točaka i centroida) u grupama prije i nakon spajanja. U osnovi Wardova metoda pretpostavlja da točke dolaze iz realnog koordinatnog prostora, no postoje generalizacije na bilo kakve podatke koje koriste mjeru sličnosti. Prema tome, kao mjera pogreške koristi se Euklidova udaljenost između točke i centroida, odnosno koristi se Euklidova norma. Neka je $\operatorname{ESS} \left(\mathcal{C}\right)$ zbroj kvadratnih pogreški \engl{error sum of squares} neke neprazne grupe $\mathcal{C}$ čiji je centroid $\boldsymbol{\mu}$:
\[\operatorname{ESS} \left(\mathcal{C}\right) = \sum_{\mathbf{x} \in \mathcal{C}} 
\Vert \mathbf{x} - \boldsymbol{\mu} \Vert^2\]
Wardova metoda udaljenost između grupa definira kao razliku sume kvadratnih pogrešaka nakon i prije spajanja grupa:
\[D_{\text{Ward}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
= \operatorname{ESS} \left(\mathcal{C}_i \cup \mathcal{C}_j\right) - \left(\operatorname{ESS} \left(\mathcal{C}_i\right) + \operatorname{ESS} \left(\mathcal{C}_j\right)\right)
\]
Gornja formula se može raspisivanjem svesti na jednostavniju formulu:
\[D_{\text{Ward}} \left( \mathcal{C}_i, \mathcal{C}_j \right)
= \frac{\vert \mathcal{C}_i \vert \cdot \vert \mathcal{C}_j \vert}
{\vert \mathcal{C}_i \vert + \vert \mathcal{C}_j \vert}
\left\Vert \boldsymbol{\mu}_i - \boldsymbol{\mu}_j \right\Vert^2
\]
gdje su $\boldsymbol{\mu}_i$ i $\boldsymbol{\mu}_j$ redom centroidi grupa $\mathcal{C}_i$ i $\mathcal{C}_j$.

\subsection{Opis algoritma}
Hijerarhijskog aglomerativno grupiranje najprije stvara $N$ grupa, odnosno na početku tretira svaku ulaznu točku iz $\mathcal{D}$ kao zasebnu grupu. Prilikom svake iteracije donosi odluku koje dvije grupe spojiti na temelju kriterija spajanja: spojiti one dvije grupe koje su najbliže prema kriteriju spajanja koji je unaprijed odabran kao parametar algoritma. Nakon spajanja grupa, broj ukupnih grupa se smanjuje za 1 i postupak se ponavlja.\\
Naravno, algoritam može ponavljati postupak dok ne ostane samo jedna grupa, ali to najčešće nije od koristi. Umjesto toga, postupak se može zaustaviti\footnote{Tada se na neki način radi o particijskom grupiranje, jer kada se zaustavi postupak, onda se ‘‘presiječe’’ horizontalno dendrogram i prihvate grupe kakve jesu na toj razini, bez hijerarhijske strukture.} kada se ispuni neki kriterij zaustavljanja, a to može biti jedno od sljedećih opcija:
\begin{itemize}
    \item dostignut je željeni broj grupa;
    \item najmanja udaljenost između dvije grupe je premašila neku granicu.
\end{itemize}
Odluka kada će postupak završiti je također parametar algoritma. Ukoliko se želi zaustaviti postupak nakon dostignutih $K$ grupa, broj $K$ je potrebno predati kao parametar, a ako se ne želi premašiti najmanja udaljenost $\epsilon$ tijekom spajanja, tada je potrebno specificirati broj $\epsilon$.\\
\begin{croatianalgorithm}[H]
\caption{Hijerarhijsko aglomerativno grupiranje}
\label{algo:hac}
\begin{algorithmic}
\STATE{\textbf{Parametri}: kriterij spajanja $D \left(\mathcal{C}_i, \mathcal{C}_j\right)$, uvjet zaustavljanja: broj grupa $K$ ili najveća dopuštena minimalna udaljenost između grupa $\epsilon$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} grupiranje: skup međusobno disjunktnih grupa
$\Gamma = \left\{ \mathcal{C}_i\right\}_{i=1}^{M}$ koji sačinjavaju skup točaka
$ \mathcal{D} = \bigcup_{i=1}^{M} \mathcal{C}_i $}, gdje broj grupa $M$ je ili nepoznat ili $M = K$ ovisno o uvjetu zaustavljanja

\STATE

\STATE{$\Gamma \gets \emptyset$}
\FORALL{$i \in \left\{1, \dots, N\right\}$}
    \STATE{$\mathcal{C}_i \gets \left\{ \mathbf{x}^{(i)} \right\}$}
    \STATE{$\Gamma \gets \Gamma \cup \left\{ \mathcal{C}_i \right\}$}
\ENDFOR
\WHILE{}
\STATE{\textbf{prekini} ako je zadan $K$ i vrijedi $\vert \Gamma \vert \leq K$}
\STATE{$\left(\mathcal{C}_i, \mathcal{C}_j\right) \gets 
        \argmin_{\left(\mathcal{C}_a, \mathcal{C}_b\right) \in \Gamma \times \Gamma}
        D \left(\mathcal{C}_a, \mathcal{C}_b\right)
$}
\STATE{\textbf{prekini} ako je zadan $\epsilon$ i vrijedi $D \left(\mathcal{C}_i, \mathcal{C}_j\right) > \epsilon$}
\STATE{$\mathcal{C}_i \gets \mathcal{C}_i \cup \mathcal{C}_j$}
\STATE{$\Gamma \gets \Gamma \setminus \left\{\mathcal{C}_j\right\}$}
\ENDWHILE
\RETURN{$\Gamma$}
\end{algorithmic}
\end{croatianalgorithm}

\subsection{Primjer grupiranja}
Neka su ulazni podaci točke sa dvjema realnim značajkama prikazani na slici \ref{fig:hierdata}. Radi lakšeg snalaženja, sve točke su označene brojevima.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{img/hierdata.png}
    \caption{Ulazni podaci}
    \label{fig:hierdata}
\end{figure}
Neka kriterij spajanja bude prosječna povezanost i neka algoritam spoji sve podgrupe, odnosno postavimo broj grupa $K = 1$. Nad tim podacima pokrenut je algoritam hijerarhijskog aglomerativnog grupiranja. Rezultat grupiranja prikazan je dendrogramom na slici \ref{fig:dendrogram}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.85]{img/dendro.png}
    \caption{Dendrogram hijerarhijskog aglomerativnog grupiranja}
    \label{fig:dendrogram}
\end{figure}
Na donjoj liniji se nalaze oznake svake točke, a na lijevoj liniji se nalazi mjera koja iskazuje koja je bila prosječna udaljenost (jer koristimo prosječnu povezanost kao kriterij spajanja) između grupa u trenutku spajanja. Ukoliko se želi particijski grupirati primjere, to se odlučuje prema vlastitoj volji. Primjerice, ukoliko se ne žele spajati grupe čija prosječna udaljenost prelazi $\epsilon = 3.5$, tada se dendrogram ‘‘presiječe’’ horizontalno na toj razini i kao rezultat dobiju 3 grupe jer bi se dendrogram presjekao na 3 mjesta. Na sličan način se mogu i po volji grupirati točke u proizvoljan broj grupa $K$: dendrogram se presiječe na razini na kojoj se nalazi $K$ grupa. Na slici \ref{fig:hiereps} je prikazan rezultat grupiranja ako se specificira $\epsilon = 3.5$.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{img/hiereps.png}
    \caption{Hijerarhijsko aglomerativno grupiranje uz $\epsilon = 3.5$}
    \label{fig:hiereps}
\end{figure}

\subsection{Svojstva i složenost algoritma}
Opisani algoritam daje dobre rezultate, no najveći nedostatak je vremenska i prostorna složenost. Pretpostavimo da se algoritam zaustavlja nakon što je preostalo $K$ grupa. To znači da algoritam u glavnoj petlji radi $N-K$ koraka. Korak u kojem se odlučuje koje dvije grupe spojiti mora proći kroz sve kombinacije grupa, što je vremenske složenosti $\mathcal{O} \left(N^2\right)$. Prema tome, vremenska složenost algoritma je $\mathcal{O} \left(N^2 \left(N-K\right)\right)$, a kako je u praksi $N$ puno veći od $K$, onda je vremenska složenost $\mathcal{O} \left(N^3\right)$. Iz tog razloga algoritam nije dobar izbor za velike skupove podataka. Informacija o udaljenosti između točaka se jako često koristi tijekom izvršenja algoritma, stoga implementacije u pravilu najprije konstruiraju tzv.\ \textbf{matricu udaljenosti} \engl{distance matrix}. Matrica udaljenosti je simetrična matrica dimenzija $N \times N$ i sadrži izračunatu udaljenost između svih parova točaka. Računanje udaljenosti je tada vremenski brzo, ali problem je u prostornoj složenosti: $\mathcal{O} \left(N^2\right)$ (jer je u najmanju ruku potrebno pohraniti $\binom{N}{2}$ brojeva), i to za velike skupove podataka već predstavlja veliki problem. Umjesto matrice udaljenosti, analogno postoji matrica sličnosti i matrica udaljenosti.

Standardni ‘‘naivni’’ algoritam je vremenske složenosti $\mathcal{O} \left(N^3\right)$, no postoje bolje implementacije složenosti $\mathcal{O} \left(N^2 \log N\right)$. Specijalno, ako se koriste jednostruka ili potpuna povezanost, postoje algoritmi složenosti $\mathcal{O} \left(N^2\right)$ poznati kao SLINK i CLINK.

\section{DBSCAN}
DBSCAN.

\chapter{Postupci vrednovanja algoritama grupiranja}
\label{evaluation}
Prethodno poglavlje se bavilo problemom grupiranja: na koji način podijeliti ulazni skup točaka u grupe sa međusobno sličnim točkama. Nakon što se to napravi na neki način, postavlja se pitanje na koji način ocijeniti rezultat grupiranja. Taj zadatak može biti jednako težak kao i sam problem grupiranja ukoliko nije dostupna neka vanjska informacija o tome kako bi podaci trebali stvarno biti grupirani. Naravno, kada bi na raspolaganju bile takve informacije, ne bi bilo potrebe za grupiranjem. Prema tome, vrednovanja grupiranja se dijele na \textbf{unutarnje} \engl{internal evaluation} i \textbf{vanjsko} \engl{external evaluation} vrednovanje. Kod vanjskog vrednovanja, osim ulaznih podataka dostupne su i njihove oznake, dok to nije slučaj kod unutarnjeg vrednovanja.

\section{Unutarnje vrednovanje}
Unutarnje vrednovanje nastoji nekom grupiranju dodijeliti ocjenu samo na temelju ulaznih podataka i oznaka koje je generiralo grupiranje. Kriteriji vrednovanja najčećše daju veće ocjene grupiranjima koja stvaraju grupe u kojima su točke međusobno slične, a različite u odnosu na točke iz ostalih grupa. Nedostatak takvih vrednovanja jest taj da će ono dati bolje rezultate algoritmima koji upravo nastoje povećati ocjenu tog specifičnog vrednovanja, a da takav algoritam nije u stvarnosti obavio grupiranje na optimalan način.

\subsection{Davies-Bouldin indeks}
Davies-Bouldin indeks promatra sve parove grupa i bilježi koliko su grupe udaljene, kao i koliko su grupe same po sebi raspršene. Neka je dano grupiranje $\Gamma = \left\{\mathcal{C}_k\right\}_{k=1}^{K}$. Neka je $S_i$ prosječna kvadratna udaljenost točaka iz grupe $\mathcal{C}_i$ od centroida $\boldsymbol{\mu}_i$ te grupe:
\[S_i = \frac{1}{\left\vert \mathcal{C}_i \right\vert}
\sum_{\mathbf{x} \in \mathcal{C}_i} \Vert \mathbf{x} - \boldsymbol{\mu}_i  \Vert^2
\]
$S_i$ je mjera raspršenja unutar grupe $\mathcal{C}_i$. Naravno, kako bi se mogao prema ovakvoj definiciji računati $S_i$, mora se moći izračunati centroid, a to je moguće jedino ako podaci dolaze iz nekog vektorskog prostora. Ranije smo spomenuli da se najčešće radi o realnom koordinatnom prostoru pa se koristi Euklidova udaljenost i Euklidova norma. Neka je $M_{ij}$ udaljenost centroida grupa $\mathcal{C}_i$ i $\mathcal{C}_j$:
\[M_{ij} = \Vert \boldsymbol{\mu}_i - \boldsymbol{\mu}_j \Vert\]
$M_{ij}$ mjeri koliko dobro su razdvojene grupe u smislu koliko su im udaljeni centroidi. Neka je $R_{ij}$ definiran kao
\[R_{ij} = \frac{S_i + S_j}{M_{ij}}\]
Ono mjeri koliko dobro su grupirane međusobno grupe $\mathcal{C}_i$ i $\mathcal{C}_j$: veća razdvojenost i manje raspršenje unutar grupa će dati manji iznos $R_{ij}$, što znači bolje grupiranje između te dvije grupe. $R_{ij}$ se može shvatiti kao neka mjera sličnosti između dvije grupe. Označimo sa $R_i$ kao najveći iznos $R_{ij}$ za grupu $\mathcal{C}_i$ i sve ostale grupe:
\[R_i = \max_{\substack{1 \leq j \leq K \\ j \neq i}} R_{ij}\]
Davies-Bouldin indeks $DBI$ se tada definira kao
\[DBI = \frac{1}{K} \sum_{k=1}^{K} R_k\]

\subsection{Vrijednost siluete}
\label{subsec:silhouette}
Vrijednost siluete \engl{silhouette score} mjeri koliko je svaka točka iz $\mathcal{D}$ slična svojoj grupi u odnosu na ostale grupe, dakle svakoj točki se pridružuje vrijednost siluete. Metoda koja se zasniva na ovoj mjeri se naziva \textbf{metoda siluete} i ta metoda se spominje u poglavlju. Za svaku točku $\mathbf{x}^{(i)}$ iz ulaznog skupa točaka $\mathcal{D} = \left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N}$ neka $\mathcal{C} \left(i\right)$ označava grupu kojoj pripada ta točka. Također je na raspolaganju grupiranje koje je generirao promatrani algoritam, odnosno skup od $K$ grupa $\Gamma = \left\{\mathcal{C}_k\right\}_{k=1}^{K}$.
Prije nego što se definira vrijednost siluete $s (i)$,  neka $a (i)$ označava prosječnu udaljenost točke $\mathbf{x}^{(i)}$ do ostalih točaka u svojoj grupi:
\[a (i) = \frac{1}{\left\vert \mathcal{C} \left(i\right) \right\vert - 1} \sum_{\substack{\mathbf{y} \in \mathcal{C} \left(i\right) \\ \mathbf{y} \neq \mathbf{x}^{(i)}}} d \left(\mathbf{x}^{(i)}, \mathbf{y}\right)
\]
Neka $b (i)$ označava prosječnu udaljenost točke $\mathbf{x}^{(i)}$ do točaka najbliže grupe (najbližu u smislu prosječne udaljenosti):
\[b (i) = \min_{\substack{\mathcal{C} \in \Gamma \\ \mathcal{C} \neq \mathcal{C} \left(i\right)}} \frac{1}{\vert \mathcal{C} \vert} \sum_{\mathbf{y} \in \mathcal{C}}
d \left(\mathbf{x}^{(i)}, \mathbf{y}\right)\]
Vrijednost siluete se tada definira kao
\[s (i) = \frac{b (i) - a (i)}{\operatorname{max} \left\{a (i), b (i)\right\}}\]
a u slučaju da je točka $\mathbf{x}^{(i)}$ sama u svojoj grupi, odnosno $\left\vert \mathcal{C} \left(i\right) \right\vert = 1$, tada je $s (i) = 0$ jer vrijednost $a (i)$ nema smisla. Uzimajući u obzir međusobne odnose $a (i)$ i $b (i)$, definicija vrijednosti siluete se može izraziti na sljedeći način:
\[
s (i) = \begin{cases}
 0 & \text{ako} \; \left\vert \mathcal{C} \left(i\right) \right\vert = 1 \\
 1 - \frac{a (i)}{b (i)} & \text{ako} \; a (i) < b (i) \\  
 0 & \text{ako} \; a (i) = b (i) \\  
 \frac{b (i)}{a (i)} - 1 & \text{ako} \; a (i) > b(i)  
 \end{cases}
\]
Jasno je vidljivo iz ovakve definicije vrijednosti siluete da ona može samo poprimiti vrijednosti između -1 i 1, odnosno $-1 \leq s (i) \leq 1$. Kako $a (i)$ mjeri koliko je točka $\mathbf{x}^{(i)}$ blizu ostalim točkama iz svoje grupe, manja vrijedost $a(i)$ znači veću sličnost $\mathbf{x}^{(i)}$ sa ostalim točkama iz svoje grupe. S druge strane, $b(i)$ mjeri koliko je $\mathbf{x}^{(i)}$ blizu točkama iz najbliže susjedne grupe, stoga veća vrijednost $b (i)$ znači veću ‘‘razdvojenost’’, odnosno veću različitost od ostalih grupa. Prema tome, kada je $a(i)$ puno manji od $b(i)$, vrijednost siluete $s (i)$ će biti blizu 1, što označava dobro grupiranje konkretno za promatranu točku. Analogno, kada je $b (i)$ puno veći od $a (i)$, to znači da je točka bliža nekoj drugoj grupi nego vlastitoj. Tada će $s(i)$ biti blizu -1 i to označava loše grupiranje. Vrijednost siluete blizu 0 znači da se $a(i)$ i $b(i)$ malo razlikuju, što znači da je točka $\mathbf{x}^{(i)}$ na granici dvije grupe.
%TODO: NASTAVITI METODU SILUETE%

\section{Vanjsko vrednovanje}
Vanjsko vrednovanje, uz ulazne podatke i grupe koje je promatrani algoritam generirao, na raspolaganju ima i oznake svake točke koje otkrivaju referentno grupiranje. Te oznake nisu poznate postupcima nenadziranog strojnog učenja, pa tako i algoritmima grupiranja. Do njih je ponekad moguće doći primjerice ljudskom ocjenom ulaznog skupa primjera, ili su mogli biti poznati ranije, no odluka je bila ne specificirati oznake metodama nenadziranog strojnog učenja (pa tako i grupiranja).


\subsection{Randov indeks}
Randov indeks mjeri preciznost grupiranja tako da promatra sve parove ulaznih primjera i uspoređuje dva grupiranja istovremeno. Jedno grupiranje je ono koje je generirao algoritam grupiranja, a drugo grupiranje (referentno grupiranje) je prema prethodno poznatim oznakama: dva primjera su u istoj grupi ako imaju istu oznaku. Za svaki mogući par primjera se promatra jesu li završili u istoj grupi, i jesu li oni u istoj grupi kod referentnog grupiranja. Parovi ulaznih primjera se tada dijele na:
\begin{enumerate}
    \item \textbf{Istinito pozitivne} \engl{true positive}: primjeri se nalaze u istoj grupi u oba grupiranja. Neka je $TP$ broj takvih parova.
    \item \textbf{Istinito negativne} \engl{true negative}: primjeri se nalaze u različitim grupama u oba grupiranja. Neka je $TN$ broj takvih parova.
    \item \textbf{Lažno pozitivne} \engl{false positive}: primjeri se nalaze u istoj grupi u dobivenom grupiranju, a u referentnom grupiranju se nalaze u različitim grupama. Neka je $FP$ broj takvih parova.
    \item \textbf{Lažno negativne} \engl{false negative}: primjeri se nalaze u različitim grupama u dobivenom grupiranju, a u referentnom grupiranju se nalaze u istoj grupi. Neka je $FN$ broj takvih parova.
\end{enumerate}

Parovi primjera su dobro grupirani ako situacija odgovara referentnom grupiranju, dakle promatraju se primjeri koji su istinito pozitivni i istinito negativni.
Randov indeks $R$ se tada definira kao:
\[R = \frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{\binom{N}{2}}\]
Jasno je da su u nazivniku pokriveni svi mogući parovi ulaznih primjera, dakle broj takvih parova mora biti $\binom{N}{2}$.
Grupiranje koje je identično referentnom grupiranju ima $R = 1$.

Primjerice, neka su ulazni primjeri skup od 5 prirodnih brojeva od 1 do 5. Neka su ulazni podaci grupirani na sljedeći način:
\[\left\{ \left\{1, 2, 4\right\}, \left\{3, 5\right\} \right\}\]
U programskim implementacijama obično se grupiranje specificira nizom oznakama. Za ovaj primjer bi onda u pitanju bio niz $\left\{0, 0, 1, 0, 1\right\}$ Kod oznaka je samo bitno da primjeri iste grupe imaju istu oznaku, kakva je oznaka nije bitno. Oznake su mogle biti primjerice  $\left\{7, 7, 4, 7, 4\right\}$. Neka je referentno grupiranje
\[\left\{\left\{1,2\right\}, \left\{3,4\right\}, \left\{5\right\}\right\}\]
odnosno u terminima oznaka bi se specificiralo nešto poput $\left\{0, 0, 1, 1, 2\right\}$.
Za ovaj primjer je $TP = 1$ jer samo par 12 se nalazi u istim grupama u oba grupiranja, a $TN = 5$ i radi se o parovima 13, 15, 23, 25, 45. Prema tome $R = \frac{1 + 5}{\binom{5}{2}} = \frac{6}{10} = 0.6$.

\chapter{Programsko ostvarenje i rezultati}
\label{chap:results}
Programsko ostvarenje i rezultati.

\chapter{Zaključak}
\label{chap:conclusion}
Zaključak.

\bibliography{literatura}
\bibliographystyle{fer}

\begin{sazetak}
Sažetak na hrvatskom jeziku.

\kljucnerijeci{Ključne riječi, odvojene zarezima.}
\end{sazetak}
%‘‘slični’’
% TODO: Navedite naslov na engleskom jeziku.
\engtitle{Title}
\begin{abstract}
Abstract.

\keywords{Keywords.}
\end{abstract}

\end{document}
