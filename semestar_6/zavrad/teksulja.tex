\documentclass[times, utf8, zavrsni]{fer}
\usepackage{booktabs}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}

\newenvironment{croatianalgorithm}[1][]
  {\begin{algorithm}[#1]
     \selectlanguage{croatian}%
     \floatname{algorithm}{Algoritam}%
     \renewcommand{\algorithmicrepeat}{\textbf{ponavljaj}}%
     \renewcommand{\algorithmicuntil}{\textbf{dok}}%
     \renewcommand{\algorithmicforall}{\textbf{za svaki}}%
     \renewcommand{\algorithmicendfor}{\textbf{kraj za}}
     \renewcommand{\algorithmicdo}{\,}
     % Set other language requirements
  }
  {\end{algorithm}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}

% TODO: Navedite broj rada.
\thesisnumber{000}

% TODO: Navedite naslov rada.
\title{Usporedba metoda grupiranja primjenom programskog jezika Python}

% TODO: Navedite vaše ime i prezime.
\author{Tomislav Bjelčić}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{}

\tableofcontents

\chapter{Uvod}

\textbf{Grupiranje} \engl{clustering} je postupak kojim se neki skup podataka razvrstava u skupine, odnosno \textbf{grupe} \engl{clusters}, u kojima su podaci međusobno slični. Grupiranje je jedno od metoda \textbf{nenadziranog strojnog učenja} \engl{unsupervised learning}, dakle ulazni podaci nisu označeni, odnosno nemaju neku ciljnu vrijednost koja bi naznačila kojoj grupi pripada neki podatak. Algoritmi grupiranja iz takvih podataka onda moraju sami prepoznati grupe podataka i odrediti za svaki ulazni podatak kojoj grupi bi pripadao.

Uzmimo neki jednostavan primjer. Neka na raspolaganju imamo podatke o visini i masi odraslih pasa iz nekog 
skloništa za životinje. U ovom jednostavnom primjeru ulazni podaci imaju dvije značajke, odnosno dimenzije: visina i masa. Općenito, ulazni podaci mogu imati proizvoljno mnogo značajka. Prikažimo ulazne podatke kao 
točke na grafu gdje os apscisa predstavlja masu, a os ordinata predstavlja visinu pojedinog psa.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.52]{img/uvod.png}
    \caption{Podaci o psima u skloništu za životinje}
\end{figure}

Promatranjem točaka na grafu možemo uočiti da se podaci prirodno grupiraju u tri grupe, koje odgovaraju različitim pasminama. Sjetimo se, ulazni podaci nisu označeni, dakle jedino na raspolaganju imamo masu i visinu. Htjeli bismo da odabran algoritam grupiranja prepozna da se radi o tri grupe i slične točke pridruži istoj grupi. Općenito, poželjno je da algoritam grupiranja podatke grupira na onaj način koji odgovara prirodnom grupiranju. U našem primjeru je prirodno grupiranje vizualno očito i ono odgovara pasminama, no to ne mora biti slučaj. Neće uvijek ni vizualnom metodom biti jednoznačno jasno kakvo je njihovo prirodno grupiranje. Ovdje bismo mjeru sličnosti mogli definirati koristeći udaljenost točaka (što su točke bliže, više su slične) s obzirom da se radi o brojevnim podacima, no općenito podaci mogu biti bilo kakve prirode, što će pojedini algoritmi grupiranja uzimati u obzir.

\chapter{Općenito o algoritmima grupiranja}
Definirajmo što su ulazni podaci. U kontekstu algoritama grupiranja\footnote{Isto tako i u kontekstu nenadziranog strojnog učenja.} ulazni podaci su skup od $N$ neoznačenih, višedimenzionalnih \textbf{točaka} (vektora) \[\mathcal{D} = \left\{ \mathbf{x}^{(i)} \right\}_{i=1}^{N} \]
gdje svaka točka $\mathbf{x} = \left(x_1, x_2, \dots, x_{n-1}, x_n\right)$ ima $n$ \textbf{značajki} \engl{features}, odnosno dimenzija. Prostor podataka iz koje dolaze pojedine značajke, odnosno točke, mogu biti razne. Primjerice, može biti riječ o realnim brojevima (uvodni primjer), znakovima (primjerice grupiranje neoznačenih novinskih članaka), kategorijskim podacima poput vrijednosti istina/laž, itd. Neki algoritmi grupiranja imaju osnovne pretpostavke o tome iz kojeg prostora podataka dolaze značajke, što znači da ne funkcioniraju za one podatke koji nemaju ispunjene takve pretpostavke.

\section{Udaljenost, sličnost i različitost točaka}
U uvodnom poglavlju smo grupiranje opisali kao postupak kojim se skup podataka razvrstava u grupe na taj način gdje su točke u istoj grupi međusobno manje-više ‘‘slični’’. Potrebno je definirati kako se određuje, odnosno kako se mjeri takva ‘‘sličnost’’ između dvije točke. Definirajmo najprije \textbf{mjeru udaljenosti} \engl{distance measure}. Neka je $\mathcal{V}$ prostor iz kojeg dolaze ulazni podaci čiji je $\mathcal{D}$ podskup, te neka je $\mathbb{R}$ skup realnih brojeva. Udaljenost je funkcija
\[d: \mathcal{V} \times \mathcal{V} \to \mathbb{R}\]
koja za svaki \(\mathbf{x}, \mathbf{y}, \mathbf{z} \in \mathcal{V}\) zadovoljava tzv.\ svojstva \textbf{metrike}:
\begin{enumerate}
    \item \(d \left(\mathbf{x}, \mathbf{y}\right) = 0 \quad
                \text{akko} \quad \mathbf{x} = \mathbf{y}\)
    \item \(d \left(\mathbf{x}, \mathbf{y}\right) = d \left(\mathbf{y}, \mathbf{x}\right)\) \qquad (simetričnost)
    \item \(d \left(\mathbf{x}, \mathbf{z}\right) \leq 
            d \left(\mathbf{x}, \mathbf{y}\right)
            + d \left(\mathbf{y}, \mathbf{z}\right)\) \qquad (nejednakost trokuta)
\end{enumerate}
Iz navedenih aksioma metrike može se pokazati da vrijedi \(d \left(\mathbf{x}, \mathbf{y}\right) \geq 0\)
za svaki \(\mathbf{x}, \mathbf{y} \in \mathcal{V}\).
Konceptualno, dvije točke koje se više ‘‘razlikuju’’ u svom prostoru imaju veću udaljenost.
Ponekad su podaci takvi da su sve značajke realni brojevi, to jest \(\mathcal{V} \subseteq \mathbb{R}^n\), tada je \textbf{Euklidska udaljenost} prikladna\footnote{Euklidska udaljenost nije jedina mjera udaljenosti za takav prostor, ali je najkorištenija.} mjera udaljenosti koja zadovoljava svojstva metrike:
\begin{equation}
    d \left(\mathbf{x}, \mathbf{y}\right) = 
    \sqrt{\sum_{i=1}^{n} \left(x_i - y_i\right)^2}
    \label{euclidean_distance}
\end{equation}
Općenito, ako je \(\mathcal{V}\) normirani vektorski prostor sa definiranom normom \(\Vert \cdot \Vert : \mathcal{V} \to \mathbb{R}\) tada se udaljenost može definirati pomoću te norme:
\begin{equation}
    d \left(\mathbf{x}, \mathbf{y}\right) = \Vert \mathbf{y} - \mathbf{x} \Vert
    \label{norm_distance}
\end{equation}
U slučaju realnih značajki i ako se koristi Euklidova norma, dobijemo upravo \ref{euclidean_distance}.\\
Vrlo često u praksi imamo značajke koje nisu realni brojevi niti dolaze iz nekog drugog normiranog prostora. Tada se koriste neke druge mjere udaljenosti specijalizirane za specifične prostore \(\mathcal{V}\). Primjerice, postoji
\begin{itemize}
    \item \textbf{Hammingova udaljenost}: binarni nizovi duljine $n$
    \item \textbf{Jaccardova udaljenost}: skupovi i multiskupovi
    \item \textbf{??? udaljenost} \engl{Edit distance}: znakovni nizovi %TODO: prevesti Edit distance
\end{itemize}
\textbf{Sličnost} \engl{similarity measure} je vrsta mjere koja, poput mjere udaljenosti, brojčano iskazuje koliko se razlikuju dvije točke. Za razliku od mjere udaljenosti, mjera sličnosti ne mora zadovoljavati sva svojstva metrike poput nejednakosti trokuta. Jednako vrijedi i za \textbf{mjeru različitosti} \engl{dissimilariy measure}. Kako su dvije točke ‘‘sličnije’’ tako mjera sličnosti raste, a mjera različitosti pada. Detaljnu definiciju mjera sličnosti i različitosti nećemo navoditi iz razloga što se u algoritmima grupiranja (barem onih koje ćemo opisati) koriste uglavnom mjere udaljenosti u svrhu kvantificiranja sličnosti ili različitosti točaka.

\section{Vrste grupiranja}
Ne postoji jedinstvena definicija grupe koja vrijedi za sve algoritme grupiranja. Svaki algoritam ima svoj model grupiranja u kojem je definirano što je to grupa, a sam algoritam pokušava točke grupirati na način koji to najviše odgovara za taj model. Različite modele grupiranja ćemo detaljnije razmotriti u sljedećem poglavlju, a sada navedimo dvije generalne podijele.\\
S obzirom na koji način se oblikuju grupe, grupiranje može biti:
\begin{itemize}
    \item \textbf{Hijerarhijsko grupiranje}
    \item \textbf{Particijsko grupiranje}
\end{itemize}
Kod hijerarhijskog grupiranja svaka grupa, počevši od grupe koja predstavlja cijeli skup točaka $\mathcal{D}$, ima podgrupe koje se tako rekurzivno dijele sve dok svaka točka nije svoja grupa. Na taj način se gradi hijerarhija grupa (od tud i naziv hijerarhijskog grupiranja) koji se može prikazati \textbf{dendrogramom}. Kod hijerarhijskog grupiranja postoje dva pristupa:
\begin{itemize}
    \item \textbf{Aglomerativno grupiranje}: početno je svaka točka u svojoj grupi pa se spajaju u veće grupe;
    \item \textbf{Divizivno grupiranje}: početna grupa je $\mathcal{D}$ pa se ona dijeli u manje podgrupe.
\end{itemize}
Na slici \ref{fig:hier_clustering} je prikazan tijek nekog hijerarhijskog aglomerativnog grupiranja nekog ulaznog skupa točaka.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.95]{img/hier.png}
    \caption{Tijek nekog hijerarhijskog aglomerativnog grupiranja}
    \label{fig:hier_clustering}
\end{figure}
Na lijevoj strani crnom bojom su označene i numerirane točke ulaznog skupa podataka, a crvenim brojevima je označen redosljed kojim je postupak grupiranja spajao grupe u veće grupe. Na desnoj strani je postupak i rezultat grafički prikazan u obliku dendrograma. Na donjoj liniji su oznake točaka, a brojeve na lijevoj liniji zasad zanemarimo. Ako dendrogram shvatimo kao stablo čiji su čvorovi (pod)grupe, a listovi točke iz $\mathcal{D}$, tada aglomerativno grupiranje gradi dendrogram od listova prema korijenu, a divizivno grupiranje od korijena prema listovima.\\
Za razliku od hijerarhijskog grupiranja, particijsko grupiranje nema hijerarhiju grupa i podgrupa. Rezultat grupiranja je neki fiksiran broj grupa bez neke unutarnje strukture osim prikladnih točaka u njima. Najpopularniji i najučinkovitiji algoritmi su upravo particijska grupiranja.

Prirodne grupe točaka ponekad nije moguće jednoznačno odrediti. Grupiranja tada mogu, bilo grupiranje particijsko ili hijerarhijsko, neke točke svrstati ili u isključivo jednu grupu ili u više grupa istovremeno. Prema tome razlikujemo:
\begin{itemize}
    \item \textbf{Čvrsto grupiranje} \engl{hard clustering}: jedna točka može pripadati isključivo jednoj grupi;
    \item \textbf{Meko grupiranje} \engl{soft clustering}: jedna točka može pripadati više grupa sa nekom mjerom pripadnošću svakoj od tih grupa. Primjerice, možemo vjerojatnošću da neka točka pripada nekoj grupi modelirati sigurnost ili nesigurnost njegovoj stvarnoj pripadnosti.
\end{itemize}
Na slici \ref{fig:hard_vs_soft_clustering} vizualno je prikazana razlika između čvrstog i mekog grupiranja nekog skupa točaka. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/hard_vs_soft_clustering.png}
    \caption{Primjer čvrstog i mekog grupiranja}
    \label{fig:hard_vs_soft_clustering}
\end{figure} Uočimo kako je primjerice crvenu točku meko grupiranje svrstalo u dvije grupe istovremeno, a čvrsto grupiranje u isključivo jednu.

\chapter{Algoritmi grupiranja}
U daljnjim potpoglavljima opisat ćemo podskup algoritama grupiranja implementiranih u Pythonovoj programskoj knjižnici \textbf{Scikit-learn}, unutar modula za grupiranje \texttt{sklearn.cluster}. Svako potpoglavlje sadrži najprije opis modela grupiranja kojeg taj algoritam koristi, a zatim opis samog algoritma. Krenimo sa najpopularnijim algoritmom grupiranja.
\section{Algoritam K-sredina} \label{kmeans}
Algoritam \textbf{K-sredina} \engl{K-means clustering} je jednostavan i učinkovit algoritam particijskog čvrstog grupiranja koji ulazni skup točaka $\mathcal{D}$ particionira u $K$ grupa. $K$ je broj grupa koje je potrebno proizvesti i on se zadaje unaprijed kao parametar algoritma. Nešto kasnije ćemo reći nešto i o metodama kako odrediti broj grupa $K$.

Algoritam K-sredina predstavlja svaku grupu sa jednom točkom koja označava središte te grupe: \textbf{centroidom}. Neka je $\mathcal{C}$ grupa koja sadrži $M$ točaka ($M = \vert \mathcal{C} \vert$, $\vert \mathcal{C} \vert$ predstavlja kardinalitet skupa $\mathcal{C}$), odnosno 
$\mathcal{C} = \left\{ \mathbf{x}^{(i)}  \right\}_{i=1}^{M}$.
Centroid te grupe $\boldsymbol{\mu}$ se definira kao:
\[\boldsymbol{\mu} = \frac{1}{\vert \mathcal{C} \vert} \sum_{\mathbf{x} \in \mathcal{C}} \mathbf{x} = 
\frac{1}{M} \sum_{i=1}^{M} \mathbf{x}^{(i)}\]
Uočimo kako se centroid računa na isti način kako bismo računali aritmetičku sredinu, odakle dolazi naziv K-sredina. Također uočimo da se radi o zbrajanju točaka te u konačnici dijeljenju sa nekim brojem. Dakle da bi uopće bilo moguće računati centroide, nad točkama, odnosno ulaznim podacima, koje potječu iz prostora $\mathcal{V}$, moraju biti definirane operacije zbrajanja i množenja sa skalarom (u našem slučaju realnim brojem $\frac{1}{M}$). Tako dolazimo do prve pretpostavke modela: prostor podataka $\mathcal{V}$ mora biti vektorski prostor\footnote{Vektorski prostor je skup objekata, odnosno vektora, nad kojim su definirane operacije međusobnog zbrajanja i množenja sa skalarom, a te operacije zadovoljavaju 8 aksioma vektorskog prostora.}. Najčešće će u pitanju biti podskup realnog koordinatnog prostora, odnosno $\mathcal{V} \subseteq \mathbb{R}^n$. 
Postoji i varijanta algoritma koja radi nad podacima bilo kakve prirode i koristi mjeru sličnosti (koja je općenitija, odnosno manje ‘‘zahtjevna’’ od mjere udaljenosti): \textbf{algoritam K-medoida}, no nećemo se time baviti.\\
Kao što pojam sugerira, centroid neke grupe konceptualno predstavlja centar, odnosno središte te grupe. Jasno je da centroid grupe se uopće ne mora nalaziti u grupi.

Cilj algoritma K-sredina jest minimizirati \textbf{kriterijsku funkciju}. Neka je ulazni skup točaka $\mathcal{D}$ čvrsto grupiran u $K$ grupa:
\[\mathcal{D} = \bigcup_{k=1}^{K} \mathcal{C}_k, \qquad \forall i \neq j :  \mathcal{C}_i \cap \mathcal{C}_j = \emptyset\]
Kako se radi o čvrstom grupiranju, grupe su međusobno disjunktne, odnosno ne može se dogoditi da neka točka iz $\mathcal{D}$ završi u više grupa. Neka je $\boldsymbol{\mu}_k$ centroid grupe $\mathcal{C}_k$. Kriterijska funkcija za dano grupiranje kod algoritma K-sredina se definira izrazom:
\[J = \sum_{k=1}^{K} \sum_{\mathbf{x} \in \mathcal{C}_k} 
d \left(\mathbf{x}, \boldsymbol{\mu}_k\right)^2\]
Ako podaci dolaze iz podskupa realnog koordinatnog prostora, onda možemo koristiti Euklidsku udaljenost i Euklidsku normu, odnosno
$d \left(\mathbf{x}, \boldsymbol{\mu}_k\right) = \Vert \mathbf{x} - \boldsymbol{\mu}_k \Vert$. Tada kriterijska funkcija glasi:
\[J = \sum_{k=1}^{K} \sum_{\mathbf{x} \in \mathcal{C}_k} 
\Vert \mathbf{x} - \boldsymbol{\mu}_k \Vert^2\]
Kriterijska funkcija zbraja kvadratna odstupanja točaka od centroida grupe u kojima se nalaze. Što su točke bliže svojim centroidima to će iznos kriterijske funkcije biti manji, i to predstavlja bolje grupiranje kod algoritma K-sredina. Algoritam K-sredina nastoji minimizirati iznos kriterijske funkcije, odnosno particionirati (grupirati) skup $\mathcal{D}$ na način koji će dati minimalan iznos kriterijske funkcije. Analitičkim postupcima se ne može pronaći minimum kriterijske funkcije, stoga se optimizacija provodi iterativno, a iterativni postupak optimizacije nudi algoritam K-sredina.

\subsection{Opis algoritma}
Najprije se inicijalizira $K$ centroida, te se u svakoj iteraciji sve točke pridružuju najbližem centroidu. Zatim se centroidi svake grupe ponovno računaju na temelju točaka iz te grupe (pridruženi starom centroidu koji im je bio najbliži). Postupak se ponavlja do konvergencije, to jest dok dvije uzastopne iteracije nisu ništa promijenile u smislu da sve točke zadržavaju svoje grupe i ponovno računanje svih centroida ih ne mijenja. Formalno opišimo algoritam:
\begin{croatianalgorithm}[H]
\caption{Algoritam K-sredina}
\label{algo:k-means}
\begin{algorithmic}
\STATE{\textbf{Parametri}: broj grupa $K$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} čvrste grupe $\mathcal{C}_k, \mathcal{D} = \bigcup_{k=1}^{K} \mathcal{C}_k$}
\STATE
\STATE{\textbf{inicijaliziraj} centroide $\boldsymbol{\mu}_k, \; k \in \left\{1, \dots, K\right\}$}
\REPEAT
\STATE{$\mathcal{C}_k \gets \emptyset, \; k \in \left\{1, \dots, K\right\}$}
\FORALL{$\mathbf{x}^{(i)} \in \mathcal{D}$}
\STATE{$k \gets \argmin_{j \in \left\{1, \dots, K\right\}} \Vert \mathbf{x}^{(i)} - \boldsymbol{\mu}_j \Vert$}
\STATE{$\mathcal{C}_k \gets \mathcal{C}_k \cup \left\{\mathbf{x}^{(i)}\right\}$ }
\ENDFOR
\FORALL{$k \in \left\{1, \dots, K\right\}$}
\STATE{$\boldsymbol{\mu}_k \gets \frac{1}{\vert \mathcal{C}_k \vert} \sum_{\mathbf{x} \in \mathcal{C}_k} \mathbf{x}$}
\ENDFOR
\UNTIL{$\boldsymbol{\mu}_k$ ne konvergiraju}
\end{algorithmic}
\end{croatianalgorithm}

Na slici \ref{fig:kmeans_demo} je na jednostavnom primjeru prikazan princip rada algoritma sa zadanim brojem grupa $K = 3$. Križići predstavljaju centroide, a točke iste boje su pridružene istoj grupi.
\begin{figure}[H]
    \centering
    \includegraphics{img/kmeans_demo.png}
    \caption{Demonstracijski primjer grupiranja algoritmom K-sredina}
    \label{fig:kmeans_demo}
\end{figure}
Potrebno je napomenuti da je algoritam kao inicijalne centroide odabrao već postojeće točke, no to općenito ne mora biti slučaj. Metode odabira inicijalnih centroida ćemo raspraviti u poglavlju %TODO: UBACITI REF POGLAVLJA%

\subsection{Svojstva i složenost algoritma}
Kao što smo spomenuli na početku poglavlja \ref{kmeans}, cilj algoritma K-sredina jest minimizirati kriterijsku funkciju. Postavlja se pitanje, hoće li algoritam u tome uvijek i uspjeti? Odgovor je: neće. Algoritam će pronaći lokalni optimum, ali ne garantira da će to biti i globalni optimum. Velik utjecaj na konačni rezultat ima upravo prvi korak algoritma: inicijalizacija početnih $K$ centroida. Kako bi se osigurao što bolji rezultat, odnosno što manji iznos kriterijske funkcije, jako je važno na pametan način odabrati početne centroide. Jednako tako je važno odabrati ispravan broj grupa: parametar $K$.

Složenost algoritma K-sredina je $\mathcal{O} \left(TnKN\right)$, gdje je $T$ broj iteracija algoritma do konvergencije, $n$ broj značajki (dimenzija) točaka, a $N$ broj točaka. Korak algoritma u kojem pridružujemo točke najbližem centroidu je složenosti $\mathcal{O} \left(nKN\right)$ jer je potrebno za svaku od $N$ točaka ispitati koji od $K$ trenutnih centroida je najbliži, dakle mora ispitati udaljenost od svakog centroida, a računanje udaljenosti je složenosti $\mathcal{O} \left(n\right)$. Korak algoritma u kojem računamo centroide je složenosti $\mathcal{O} \left(nN\right)$ jer iako iteriramo po grupama, efektivno iteriramo po svim ulaznim točkama te radimo operacije zbrajanja i dijeljenja sa nekim brojem (koje je također složenosti $\mathcal{O} \left(n\right)$).\\
Takva složenost je prihvatljiva i algoritam uz dobar odabir početnih centroida i dobar odabir parametra $K$ proizvodi dobre rezultate. Zbog povoljne složenosti, algoritam je dobar i za jako velike podatke (veliki $N$), a broj grupa i značajka ionako su najčešće\footnote{Kod velikog broja značajki tradicionalni algoritmi grupiranja, ali i postupci obrade podataka općenito, nisu učinkoviti. Tada govorimo o problemu prokletstva visoke dimenzionalnosti \engl{curse of dimensionality}} puno manji od $N$.

\subsection{Odabir početnih $K$ centroida}
Odabir početnih $K$ centroida.

\section{Model Gaussove mješavine}
GMM i EM.

\section{DBSCAN}
DBSCAN.

\section{Hijerarhijsko aglomerativno grupiranje}
HAC.

\chapter{Postupci vrednovanja algoritama grupiranja}
Postupci vrednovanja algoritama grupiranja.

\chapter{Programsko ostvarenje i rezultati}
Programsko ostvarenje i rezultati.

\chapter{Zaključak}
Zaključak.

\bibliography{literatura}
\bibliographystyle{fer}

\begin{sazetak}
Sažetak na hrvatskom jeziku.

\kljucnerijeci{Ključne riječi, odvojene zarezima.}
\end{sazetak}

% TODO: Navedite naslov na engleskom jeziku.
\engtitle{Title}
\begin{abstract}
Abstract.

\keywords{Keywords.}
\end{abstract}

\end{document}
