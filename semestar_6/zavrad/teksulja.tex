\documentclass[times, utf8, zavrsni]{fer}
\usepackage{booktabs}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{relsize}

\newenvironment{croatianalgorithm}[1][]
  {\begin{algorithm}[#1]
     \selectlanguage{croatian}%
     \floatname{algorithm}{Algoritam}%
     \renewcommand{\algorithmicrepeat}{\textbf{ponavljaj}}%
     \renewcommand{\algorithmicuntil}{\textbf{dok}}%
     \renewcommand{\algorithmicforall}{\textbf{za svaki}}%
     \renewcommand{\algorithmicendfor}{\textbf{kraj za}}
     \renewcommand{\algorithmicdo}{\,}
     \renewcommand{\algorithmicwhile}{\textbf{ponavljaj}}
     \renewcommand{\algorithmicendwhile}{\textbf{kraj ponavljaj}}
     \renewcommand{\algorithmicreturn}{\textbf{vrati}}
     \renewcommand{\algorithmicif}{\textbf{ako}}
     \renewcommand{\algorithmicthen}{\textbf{onda}}
     \renewcommand{\algorithmicendif}{\textbf{kraj ako}}
     % Set other language requirements
  }
  {\end{algorithm}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}

% TODO: Navedite broj rada.
\thesisnumber{121}

% TODO: Navedite naslov rada.
\title{Usporedba metoda grupiranja primjenom programskog jezika Python}

% TODO: Navedite vaše ime i prezime.
\author{Tomislav Bjelčić}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{}

\tableofcontents

\chapter{Uvod}

Grupiranje \engl{clustering} je postupak kojim se neki skup podataka razvrstava u skupine, odnosno grupe \engl{clusters}, u kojima su podaci međusobno slični. Grupiranje je jedno od metoda \textbf{nenadziranog strojnog učenja} \engl{unsupervised learning}, dakle ulazni podaci nisu označeni \engl{unlabeled}, odnosno nemaju neku ciljnu vrijednost koja bi naznačila kojoj grupi pripada neki podatak. Algoritmi grupiranja iz takvih podataka onda moraju sami prepoznati grupe podataka, odrediti za svaki ulazni podatak kojoj grupi bi pripadao i ukoliko je to moguće, odrediti \textbf{stršeće vrijednosti} \engl{outliers}.

Primjerice, neka su na raspolaganju podaci o visini i masi odraslih pasa iz nekog 
skloništa za životinje. U ovom jednostavnom primjeru ulazni podaci imaju dvije značajke, odnosno dimenzije: visina i masa. Općenito, ulazni podaci mogu imati proizvoljno mnogo značajka. Ulazni podaci su na slici \ref{fig:intro} prikazani kao točke na grafu gdje os apscisa predstavlja masu, a os ordinata predstavlja visinu pojedinog psa.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.53]{img/uvod.png}
    \caption{Podaci o psima u skloništu za životinje}
    \label{fig:intro}
\end{figure}

Podaci se prirodno grupiraju u tri grupe, koje odgovaraju različitim pasminama. Ulazni podaci nisu označeni, dakle jedino na raspolaganju imamo masu i visinu. Algoritmi grupiranja moraju prepoznati da se radi o tri grupe i slične točke pridruži istoj grupi. Općenito, poželjno je da algoritam grupiranja podatke grupira na onaj način koji odgovara prirodnom grupiranju. U navedenom primjeru prirodno grupiranje je vizualno jasno i ono odgovara pasminama, no to ne mora biti slučaj. Neće uvijek ni vizualnom metodom biti jednoznačno jasno kakvo je njihovo prirodno grupiranje. Ovdje bi mjera sličnosti mogla biti definirana koristeći udaljenost točaka (što su točke bliže, više su slične) s obzirom da se radi o brojevnim podacima, no općenito podaci mogu biti bilo kakve prirode, što će pojedini algoritmi grupiranja uzimati u obzir.

Cilj ovog završnog rada je opisati različite algoritme grupiranja, pokrenuti ih na odabranim skupovima podataka, različitim metodama ih vrednovati i na taj način usporediti. Uz to će biti objašnjeno zašto neki algoritmi daju bolje rezultate od drugih algoritama na određenim skupovima podataka.

Ostatak rada organiziran je na sljedeći način: u poglavlju \ref{clusteringgeneral} spominju se općeniti pojmovi vezani za algoritme grupiranja te su navedene osnovne podjele algoritama grupiranja. Poglavlje \ref{clusteringalgos} opisuje četiri različita popularna modela i algoritma grupiranja. Osim opisa pojmova vezanih za svaki algoritam, spominje se njihova učinkovitost i kako ispravno odabrati parametre tih algoritama. Metode vrednovanja algoritama grupiranja dane su u poglavlju \ref{evaluation} gdje su opisane često korištene mjere koje ocjenjuju rezultate grupiranja. U poglavlju \ref{chap:results} odabrano je nekoliko skupova podataka, pokrenuti su algoritmi grupiranja nad njima i rezultati su vrednovani mjerama opisanim u poglavlju \ref{evaluation}. Konačno, zaključak je dan u poglavlju \ref{chap:conclusion}.

\chapter{Općenito o algoritmima grupiranja}
\label{clusteringgeneral}
U kontekstu algoritama grupiranja\footnote{Isto tako i u kontekstu nenadziranog strojnog učenja.} ulazni podaci (primjeri) su skup od $N$ neoznačenih, višedimenzionalnih \textbf{točaka} (vektora) \[\mathcal{D} = \left\{ \mathbf{x}^{(i)} \right\}_{i=1}^{N} \]
gdje svaka točka $\mathbf{x} = \left(x_1, x_2, \dots, x_{n-1}, x_n\right)$ ima $n$ \textbf{značajki} \engl{features}, odnosno dimenzija. Prostor podataka iz koje dolaze pojedine značajke, odnosno točke, mogu biti razne. Primjerice, može biti riječ o realnim brojevima (uvodni primjer), znakovima (primjerice grupiranje neoznačenih novinskih članaka), kategorijskim podacima poput vrijednosti istina/laž, itd. Neki algoritmi grupiranja imaju osnovne pretpostavke o tome iz kojeg prostora podataka dolaze značajke, što znači da ne funkcioniraju za one podatke koji nemaju ispunjene takve pretpostavke.

\section{Udaljenost, sličnost i različitost točaka}
\label{pointsimilarity}
U uvodnom poglavlju smo grupiranje je opisano kao postupak kojim se skup podataka razvrstava u grupe na taj način gdje su točke u istoj grupi međusobno manje-više ‘‘slični’’. Potrebno je definirati kako se određuje, odnosno kako se mjeri takva ‘‘sličnost’’ između dvije točke.

Neka je $\mathcal{V}$ prostor iz kojeg dolaze ulazni podaci čiji je $\mathcal{D}$ podskup, te neka je $\mathbb{R}$ skup realnih brojeva. \textbf{Udaljenost} \engl{distance measure} je funkcija
\[d: \mathcal{V} \times \mathcal{V} \to \mathbb{R}\]
koja za svaki \(\mathbf{x}, \mathbf{y}, \mathbf{z} \in \mathcal{V}\) zadovoljava tzv.\ svojstva \textbf{metrike}:
\begin{enumerate}
    \item \(d \left(\mathbf{x}, \mathbf{y}\right) = 0 \quad
                \text{akko} \quad \mathbf{x} = \mathbf{y}\)
    \item \(d \left(\mathbf{x}, \mathbf{y}\right) = d \left(\mathbf{y}, \mathbf{x}\right)\) \qquad (simetričnost)
    \item \(d \left(\mathbf{x}, \mathbf{z}\right) \leq 
            d \left(\mathbf{x}, \mathbf{y}\right)
            + d \left(\mathbf{y}, \mathbf{z}\right)\) \qquad (nejednakost trokuta)
\end{enumerate}
Iz navedenih aksioma metrike može se pokazati da vrijedi \(d \left(\mathbf{x}, \mathbf{y}\right) \geq 0\)
za svaki \(\mathbf{x}, \mathbf{y} \in \mathcal{V}\).
Konceptualno, dvije točke koje se više ‘‘razlikuju’’ u svom prostoru imaju veću udaljenost.
Ponekad su podaci takvi da su sve značajke realni brojevi, to jest $\mathcal{V} \subseteq \mathbb{R}^{n}$, tada je \textbf{Euklidova udaljenost} prikladna\footnote{Euklidska udaljenost nije jedina mjera udaljenosti za takav prostor, ali je najkorištenija.} mjera udaljenosti koja zadovoljava svojstva metrike:
\begin{equation}
    d \left(\mathbf{x}, \mathbf{y}\right) = 
    \sqrt{\sum_{i=1}^{n} \left(x_i - y_i\right)^2}
    \label{euclidean_distance}
\end{equation}
Općenito, ako je \(\mathcal{V}\) normirani vektorski prostor sa definiranom normom \(\Vert \cdot \Vert : \mathcal{V} \to \mathbb{R}\) tada se udaljenost može definirati pomoću te norme:
\begin{equation}
    d \left(\mathbf{x}, \mathbf{y}\right) = \left\Vert \mathbf{y} - \mathbf{x} \right\Vert
    \label{norm_distance}
\end{equation}
U slučaju realnih značajki i ako se koristi Euklidova norma, dobije se upravo \ref{euclidean_distance}.\\
Vrlo često se u praksi pojavljuju značajke koje nisu realni brojevi niti dolaze iz nekog drugog normiranog prostora. Tada se koriste neke druge mjere udaljenosti specijalizirane za specifične prostore \(\mathcal{V}\). Primjerice, postoji
\begin{itemize}
    \item \textbf{Hammingova udaljenost}: binarni nizovi duljine $n$
    \item \textbf{Jaccardova udaljenost}: skupovi i multiskupovi
    \item \textbf{Udaljenost uređivanja} \engl{Edit distance}: znakovni nizovi
    %TODO: PREVESTI EDIT DISTANCE
\end{itemize}
\textbf{Sličnost} \engl{similarity measure} je vrsta mjere koja, poput mjere udaljenosti, brojčano iskazuje koliko se razlikuju dvije točke. Za razliku od mjere udaljenosti, mjera sličnosti ne mora zadovoljavati sva svojstva metrike poput nejednakosti trokuta. Jednako vrijedi i za \textbf{mjeru različitosti} \engl{dissimilariy measure}. Kako su dvije točke ‘‘sličnije’’ tako mjera sličnosti raste, a mjera različitosti pada. Detaljna definicija mjera sličnosti i različitosti neće biti navedena u ovom poglavlju iz razloga što se u algoritmima grupiranja (barem onim koji će biti opisani) koriste uglavnom mjere udaljenosti u svrhu kvantificiranja sličnosti ili različitosti točaka.

\section{Vrste grupiranja}
\label{vrstegrupiranja}
Ne postoji jedinstvena definicija grupe koja vrijedi za sve algoritme grupiranja. Svaki algoritam ima svoj model grupiranja u kojem je definirano što je to grupa, a sam algoritam pokušava točke grupirati na način koji to najviše odgovara za taj model. Različiti modeli grupiranja će biti detaljnije razmotreni u sljedećem poglavlju.\\
Postoje dvije generalne podjele algoritama grupiranja. S obzirom na koji način se oblikuju grupe, grupiranje može biti:
\begin{itemize}
    \item \textbf{Hijerarhijsko grupiranje}
    \item \textbf{Particijsko grupiranje}
\end{itemize}
Kod hijerarhijskog grupiranja svaka grupa, počevši od grupe koja predstavlja cijeli skup točaka $\mathcal{D}$, ima podgrupe koje se tako rekurzivno dijele sve dok svaka točka nije svoja grupa. Na taj način se gradi hijerarhija grupa (od tud i naziv hijerarhijskog grupiranja) koji se može prikazati \textbf{dendrogramom}. Kod hijerarhijskog grupiranja postoje dva pristupa:
\begin{itemize}
    \item \textbf{Aglomerativno grupiranje}: početno je svaka točka u svojoj grupi pa se spajaju u veće grupe;
    \item \textbf{Divizivno grupiranje}: početna grupa je $\mathcal{D}$ pa se ona dijeli u manje podgrupe.
\end{itemize}
Na slici \ref{fig:hier_clustering} je prikazan tijek nekog hijerarhijskog aglomerativnog grupiranja nekog ulaznog skupa točaka.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.95]{img/hier.png}
    \caption{Tijek nekog hijerarhijskog aglomerativnog grupiranja \citep{rajpal2018hier}}
    \label{fig:hier_clustering}
\end{figure}
Na lijevoj strani crnom bojom su označene i numerirane točke ulaznog skupa podataka, a crvenim brojevima je označen redoslijed kojim je postupak grupiranja spajao grupe u veće grupe. Na desnoj strani je postupak i rezultat grafički prikazan u obliku dendrograma. Na donjoj liniji su oznake točaka, a značenje brojeva na lijevoj liniji će biti objašnjeno u poglavlju koje detaljnije opisuje hijerarhijsko grupiranje. Ako se dendrogram promatra kao stablo čiji su čvorovi (pod)grupe, a listovi točke iz $\mathcal{D}$, tada aglomerativno grupiranje gradi dendrogram od listova prema korijenu, a divizivno grupiranje od korijena prema listovima.\\
Za razliku od hijerarhijskog grupiranja, particijsko grupiranje nema hijerarhiju grupa i podgrupa. Rezultat grupiranja je neki fiksiran broj grupa bez neke unutarnje strukture osim prikladnih točaka u njima. Najpopularniji i najučinkovitiji algoritmi su upravo particijska grupiranja.\\
Primjer na slici \ref{fig:partvshier} ilustrira razliku particijskog i hijerarhijskog grupiranja.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{img/partvshier.jpg}
    \caption{Usporedba particijskog i hijerarhijskog grupiranja \citep{viktormysko}}
    \label{fig:partvshier}
\end{figure}

Prirodne grupe točaka ponekad nije moguće jednoznačno odrediti. Grupiranja tada mogu, bilo grupiranje particijsko ili hijerarhijsko, neke točke svrstati ili u isključivo\footnote{Neki algoritmi grupiranja uključuju mogućnost stršećih vrijednosti i tada te točke nisu svrstane u nijednu grupu.} jednu grupu ili u više grupa istovremeno. Prema tome se grupiranja dijele na:
\begin{itemize}
    \item \textbf{Čvrsto grupiranje} \engl{hard clustering}: jedna točka može pripadati isključivo jednoj grupi;
    \item \textbf{Meko grupiranje} \engl{soft clustering}: jedna točka može pripadati više grupa sa nekom mjerom pripadnosti svakoj od tih grupa. Primjerice, mjera pripadnosti točke nekoj grupi se može prikazati kao vjerojatnost da ta točka pripada toj grupi.
\end{itemize}
Na slici \ref{fig:hard_vs_soft_clustering} vizualno je prikazana razlika između čvrstog i mekog grupiranja nekog skupa točaka. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/hard_vs_soft_clustering.png}
    \caption{Primjer čvrstog i mekog grupiranja}
    \label{fig:hard_vs_soft_clustering}
\end{figure} Primjerice crvenu točku u ovom primjeru je meko grupiranje svrstalo u dvije grupe istovremeno, a čvrsto grupiranje u isključivo jednu.

\chapter{Algoritmi grupiranja}
\label{clusteringalgos}
U daljnjim potpoglavljima opisan je podskup algoritama grupiranja implementiranih u Pythonovoj programskoj knjižnici \textbf{Scikit-learn}, unutar modula za grupiranje \texttt{sklearn.cluster}. Svako potpoglavlje sadrži najprije opis modela grupiranja kojeg taj algoritam koristi, a zatim opis samog algoritma te svojstva i složenost. Za neke algoritme i modele je dodatno opisano na koji način odrediti optimalne parametre algoritma.


\section{Algoritam K-sredina} \label{kmeans}
Algoritam \textbf{K-sredina} \engl{K-means clustering} je jednostavan, učinkovit i najpopularniji algoritam particijskog čvrstog grupiranja koji ulazni skup točaka $\mathcal{D}$ particionira u $K$ grupa. $K$ je broj grupa koje je potrebno proizvesti i on se zadaje unaprijed kao parametar algoritma. U poglavlju \ref{subsection:odabirK} su opisane neke metode određivanja optimalnog broja grupa $K$.

Algoritam K-sredina predstavlja svaku grupu sa jednom zamišljenom točkom koja označava središte te grupe: \textbf{centroidom}. Neka je $\mathcal{C} \subseteq \mathcal{D}$ grupa koja sadrži $M$ točaka ($M = \vert \mathcal{C} \vert$, $\vert \mathcal{C} \vert$ predstavlja kardinalitet skupa $\mathcal{C}$), odnosno 
$\mathcal{C} = \left\{ \mathbf{x}^{(i)}  \right\}_{i=1}^{M}$.
Centroid te grupe $\boldsymbol{\mu}$ se definira kao:
\[\boldsymbol{\mu} = \frac{1}{\vert \mathcal{C} \vert} \sum_{\mathbf{x} \in \mathcal{C}} \mathbf{x} = 
\frac{1}{M} \sum_{i=1}^{M} \mathbf{x}^{(i)}\]
Centroid se računa na isti način kao i aritmetička sredina, odakle dolazi naziv K-sredina. Radi se o operacijama zbrajanja točaka te u konačnici dijeljenja sa nekim brojem. Dakle da bi uopće bilo moguće računati centroide, nad točkama, odnosno ulaznim podacima, koje potječu iz prostora $\mathcal{V}$, moraju biti definirane operacije zbrajanja i množenja sa skalarom (u našem slučaju realnim brojem $\frac{1}{M}$). Ovo razmatranje dovodi do pretpostavke modela u kojem prostor podataka $\mathcal{V}$ mora biti vektorski prostor\footnote{Vektorski prostor je skup objekata, odnosno vektora, nad kojim su definirane operacije međusobnog zbrajanja i množenja sa skalarom, a te operacije zadovoljavaju 8 aksioma vektorskog prostora.}. Standardni algoritam K-sredina je tada primjenjiv samo ako se radi o vektorskom prostoru. Najčešće je u pitanju podskup realnog koordinatnog prostora, odnosno $\mathcal{V} \subseteq \mathbb{R}^n$. 
Postoji i varijanta algoritma koja radi nad podacima bilo kakve prirode i koristi mjeru sličnosti (koja je općenitija, odnosno manje ‘‘zahtjevna’’ od mjere udaljenosti): \textbf{algoritam K-medoida}, no taj algoritam nije opisan u ovom radu.\\
Kao što pojam sugerira, centroid neke grupe konceptualno predstavlja centar, odnosno središte te grupe. Jasno je da centroid grupe se uopće ne mora nalaziti u grupi.

Cilj algoritma K-sredina jest minimizirati \textbf{kriterijsku funkciju}. Neka je ulazni skup točaka $\mathcal{D}$ čvrsto grupiran u $K$ grupa:
\[\mathcal{D} = \bigcup_{k=1}^{K} \mathcal{C}_k, \qquad \forall \left(i \neq j\right) :  \mathcal{C}_i \cap \mathcal{C}_j = \emptyset\]
Kriterijska funkcija mjeri raspršenje točaka unutar grupa tako da zbraja kvadratna odstupanja od centroida.
Kako se radi o čvrstom grupiranju, grupe su međusobno disjunktne, odnosno ne može se dogoditi da neka točka iz $\mathcal{D}$ završi u više grupa. Neka je $\boldsymbol{\mu}_k$ centroid grupe $\mathcal{C}_k$. Kriterijska funkcija za dano grupiranje kod algoritma K-sredina se definira izrazom:
\[J = \sum_{k=1}^{K} \sum_{\mathbf{x} \in \mathcal{C}_k} 
d \left(\mathbf{x}, \boldsymbol{\mu}_k\right)^2\]
Ako podaci dolaze iz podskupa realnog koordinatnog prostora, onda možemo koristiti Euklidsku udaljenost i Euklidsku normu, odnosno
$d \left(\mathbf{x}, \boldsymbol{\mu}_k\right) = \left\Vert \mathbf{x} - \boldsymbol{\mu}_k \right\Vert$. Tada kriterijska funkcija glasi:
\[J = \sum_{k=1}^{K} \sum_{\mathbf{x} \in \mathcal{C}_k} 
\Vert \mathbf{x} - \boldsymbol{\mu}_k \Vert^2\]
Kriterijska funkcija zbraja kvadratna odstupanja točaka\footnote{Zbog toga se u engleskoj literaturi $J$ navodi pod imenom \emph{Within-cluster sum of squares}, ili skraćeno WCSS.} od centroida grupe u kojima se nalaze. Što su točke bliže svojim centroidima to će iznos kriterijske funkcije biti manji, i to predstavlja bolje grupiranje kod algoritma K-sredina. Algoritam K-sredina nastoji minimizirati iznos kriterijske funkcije, odnosno particionirati (grupirati) skup $\mathcal{D}$ na način koji će dati minimalan iznos kriterijske funkcije. Analitičkim postupcima se ne može pronaći minimum kriterijske funkcije, stoga se optimizacija provodi iterativno, a iterativni postupak optimizacije nudi algoritam K-sredina.

\subsection{Opis algoritma}
Najprije se inicijalizira početnih $K$ centroida, te se u svakoj iteraciji sve točke iz $\mathcal{D}$ pridružuju najbližem centroidu. Zatim se centroidi svake grupe ponovno računaju na temelju točaka iz te grupe (pridruženi starom centroidu koji im je bio najbliži). Postupak se ponavlja do konvergencije, to jest dok dvije uzastopne iteracije nisu ništa promijenile u smislu da sve točke zadržavaju svoje grupe i ponovno računanje svih centroida ih ne mijenja.
\begin{croatianalgorithm}[H]
\caption{Algoritam K-sredina}
\label{algo:k-means}
\begin{algorithmic}
\STATE{\textbf{Parametri}: broj grupa $K$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} čvrste disjunktne grupe $\mathcal{C}_k, \mathcal{D} = \bigcup_{k=1}^{K} \mathcal{C}_k$}
\STATE
\STATE{\textbf{inicijaliziraj} centroide $\boldsymbol{\mu}_k, \; k \in \left\{1, \dots, K\right\}$}
\REPEAT
\STATE{$\mathcal{C}_k \gets \emptyset, \; k \in \left\{1, \dots, K\right\}$}
\FORALL{$\mathbf{x}^{(i)} \in \mathcal{D}$}
\STATE{$k \gets \argmin_{j \in \left\{1, \dots, K\right\}} \left\Vert \mathbf{x}^{(i)} - \boldsymbol{\mu}_j \right\Vert$}
\STATE{$\mathcal{C}_k \gets \mathcal{C}_k \cup \left\{\mathbf{x}^{(i)}\right\}$ }
\ENDFOR
\FORALL{$k \in \left\{1, \dots, K\right\}$}
\STATE{$\boldsymbol{\mu}_k \gets \frac{1}{\vert \mathcal{C}_k \vert} \sum_{\mathbf{x} \in \mathcal{C}_k} \mathbf{x}$}
\ENDFOR
\UNTIL{$\boldsymbol{\mu}_k$ ne konvergiraju}
\end{algorithmic}
\end{croatianalgorithm}

Na slici \ref{fig:kmeans_demo} je na jednostavnom primjeru prikazan princip rada algoritma sa zadanim brojem grupa $K = 3$. Križići predstavljaju centroide, a točke iste boje su pridružene istoj grupi.
\begin{figure}[H]
    \centering
    \includegraphics{img/kmeans_demo.png}
    \caption{Demonstracijski primjer grupiranja algoritmom K-sredina}
    \label{fig:kmeans_demo}
\end{figure}
Potrebno je napomenuti da je algoritam kao početne centroide odabrao već postojeće točke, no to općenito ne mora biti slučaj. Metode odabira početnih centroida su raspravljene u poglavlju \ref{kminitial}.

\subsection{Svojstva i složenost algoritma}
\label{kmsvojstva}
Kao što je spomenuto na početku poglavlja \ref{kmeans}, cilj algoritma K-sredina jest minimizirati kriterijsku funkciju. Postavlja se pitanje, hoće li algoritam u tome uvijek i uspjeti? Odgovor je: neće. Algoritam će pronaći lokalni optimum, ali ne garantira da će to biti i globalni optimum. Velik utjecaj na konačni rezultat ima upravo prvi korak algoritma: inicijalizacija početnih $K$ centroida. Kako bi se osigurao što bolji rezultat, odnosno što manji iznos kriterijske funkcije, jako je važno na pametan način odabrati početne centroide. Jednako tako je važno odabrati ispravan broj grupa: parametar $K$. Dodatno, algoritam se može pokrenuti više puta sa različitim izborima početnih centroida i tada se prati koji prolaz algoritma i koje grupiranje je dalo najmanji iznos kriterijske funkcije. Time se pomaže spriječiti problem ‘‘zaglavljivanja’’ u lokalnom optimumu.

Vremenska složenost algoritma K-sredina je $\mathcal{O} \left(TnKN\right)$, gdje je $T$ broj iteracija algoritma do konvergencije, $n$ broj značajki (dimenzija) točaka, a $N$ broj točaka. Korak algoritma u kojem se točke pridružuju najbližem centroidu je složenosti $\mathcal{O} \left(nKN\right)$ jer je potrebno za svaku od $N$ točaka ispitati koji od $K$ trenutnih centroida je najbliži, dakle mora ispitati udaljenost od svakog centroida, a računanje udaljenosti je složenosti $\mathcal{O} \left(n\right)$. Korak algoritma u kojem se računaju centroidi je složenosti $\mathcal{O} \left(nN\right)$ jer iako se iterira po grupama, efektivno se iterira po svim ulaznim točkama i obavljaju se operacije zbrajanja i dijeljenja sa nekim brojem (koje su također složenosti $\mathcal{O} \left(n\right)$).\\
Takva složenost je prihvatljiva i algoritam uz dobar odabir početnih centroida i dobar odabir parametra $K$ proizvodi dobre rezultate. Zbog povoljne složenosti, algoritam je učinkovit i za velike količine podatka (veliki $N$), a broj grupa i značajka ionako su najčešće\footnote{Kod velikog broja značajki tradicionalni algoritmi grupiranja, ali i postupci obrade podataka općenito, nisu učinkoviti. Tada se radi o problemu prokletstva visoke dimenzionalnosti \engl{curse of dimensionality}} puno manji od $N$.

Što se tiče kvalitete grupiranja, postoje slučajevi u kojima algoritam neće ispravno razdvojiti grupe čak i uz odabir optimalnog $K$ i kvalitetan odabir početnih centroida. Implicitna pretpostavka algoritma, zbog prirode računanja centroida i pridruživanja točaka najbližem centroidu, jest konveksan oblik grupe. Ukratko, skup u nekom prostoru je konveksan ako je segment između bilo koje dvije točke iz skupa također u skupu, u cjelosti. Na slici \ref{fig:convexset} su prikazana dva skupa, plavi je konveksan, a zeleni nije jer postoje dvije točke između kojih segment nije u cjelosti u skupu.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/convex_nonconvex.png}
    \caption{Konveksan i nekonvenksan skup}
    \label{fig:convexset}
\end{figure}
Algoritam K-sredina neće dobro razdvojiti grupe koje nisu konveksnog oblika. Razlog tomu je mogućnost da centroid takve grupe se nalazi izvan oblika grupe, pa ako se blizu tog centroida pojavljuje neka druga grupa, algoritam K-sredina ih neće dobro razdvojiti.\\
Postoji mogućnost da grupiranje neće biti kvalitetno čak i ako su sve prirodne grupe konveksne. Takvi slučajevi su primjerice bliske grupe nejednako velikih oblika, ili grupe koje su ‘‘nakošenog’’ oblika. Demonstracija takvih neoptimalnih grupiranja dana je u poglavlju \ref{chap:results}.

\subsection{Odabir početnih $K$ centroida}
\label{kminitial}
Način na koji se početni centroidi odabiru kod algoritma K-sredina nije jasno naznačen u algoritmu, a u poglavlju \ref{kmsvojstva} je spomenuto da je to jako važno kako bi algoritam što više smanjio iznos kriterijske funkcije.

Postoji niz strategija za odabir početnih $K$ centroida. Prvo mogućnost jest kao početne centroide postaviti $K$ slučajno izabranih točaka iz $\mathcal{D}$. Slučajan odabir nije baš najbolja ideja, jer rezultati neće biti dobri ako neki od početnih $K$ centroida se nalaze unutar istih prirodnih grupa.\\
Početni centroidi se mogu izabrati na način da samo prvi centroid bude slučajno odabrana točka iz $\mathcal{D}$, a daljnji centroidi se odabiru isto iz $\mathcal{D}$ (ali među onima koji nisu još odabrani kao centroidi) na način da su što udaljeniji od već odabranih centroida. Kao sljedeći uzastopni centroid odabire se točka koja maksimizira udaljenost do najbližeg, već odabranog centroida. Takva strategija odabira opisana je u nastavku. 
\begin{croatianalgorithm}[H]
\begin{algorithmic}
\STATE{\textbf{Parametri}: broj grupa $K$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} skup inicijalnih $K$ centroida: 
$\mathcal{I} = \left\{ \boldsymbol{\mu}_k \right\}_{k=1}^{K}$}
\STATE
\STATE{$\mathcal{I} \gets \emptyset$}
\STATE{$\widehat{\mathcal{D}} \gets \mathcal{D}$}
\STATE{\textbf{slučajno odaberi} točku $\mathbf{x} \in \widehat{\mathcal{D}}$}
\STATE{$\mathcal{I} \gets \mathcal{I} \cup \left\{ \mathbf{x} \right\}$}
\STATE{$\widehat{\mathcal{D}} \gets \widehat{\mathcal{D}} \setminus
        \left\{ \mathbf{x} \right\}$}
\WHILE{dokle god $\left\vert \mathcal{I} \right\vert < K$}
\STATE{$\boldsymbol{\mu} \gets \argmax_{\textbf{x} \in \widehat{\mathcal{D}}} 
        \left\Vert \mathbf{x} - \argmin_{\boldsymbol{\mu}' \in \mathcal{I}}
                \Vert \mathbf{x} - \boldsymbol{\mu}' \Vert
        \right\Vert$}
\STATE{$\mathcal{I} \gets \mathcal{I} \cup \left\{ \boldsymbol{\mu} \right\}$}
\STATE{$\widehat{\mathcal{D}} \gets \widehat{\mathcal{D}} \setminus
        \left\{ \boldsymbol{\mu} \right\}$}
\ENDWHILE
\RETURN{$\mathcal{I}$}
\end{algorithmic}
\end{croatianalgorithm}
Ova strategija odabira početnih centroida je u pravilu dobra, no postoji jedna mana: odabirat će se stršeće vrijednosti, a kako su one relativno udaljene od prirodnih grupa, to bi moglo uzrokovati loše grupiranje. Postoji alternativa ovakvoj strategiji koja do neke mjere rješava problem odabira stršećih vrijednosti.

Umjesto odabira točke koja maksimizira udaljenost do najbližeg centroida, druga mogućnost je svakoj točki pridružiti vjerojatnost odabira koja je proporcionalna kvadratu udaljenosti do najbližeg centroida. Takva strategija odabira je poznata kao \textbf{K-means++}.
Pretpostavimo da je u nekom trenutku već odabrano $k$ centroida i one se nalaze u skupu $\mathcal{I}$. Vjerojatnost odabira neke točke $\mathbf{x} \in \widehat{\mathcal{D}}$ kao sljedeći centroid glasi:
\[P \left( \boldsymbol{\mu}_{k+1} = \mathbf{x} \middle| \widehat{\mathcal{D}}, \mathcal{I}\right)
= \frac{\min_{\boldsymbol{\mu}' \in \mathcal{I}} \Vert \mathbf{x} - \boldsymbol{\mu}' \Vert^2}{\sum_{\mathbf{x}' \in \widehat{\mathcal{D}}} 
\min_{\boldsymbol{\mu}' \in \mathcal{I}} \Vert \mathbf{x}' - \boldsymbol{\mu}' \Vert^2}\]
Nakon što se izračunaju vjerojatnosti za svaku od neodabranih točaka iz $\widehat{\mathcal{D}}$, onda se točka slučajno odabire prema navedenoj razdiobi vjerojatnosti. Stršeće vrijednosti će i dalje imati najveću vjerojatnost odabira, ali stršećih vrijednosti nema mnogo, pa je ipak vjerojatniji odabir neki od prirodno prosječnih primjera.\\
K-means++ u praksi daje bolje rezultate od varijante K-sredina u kojem se svih $K$ početnih centroida odabire slučajno. Iznosi kriterijskih funkcija ispadaju manji i algoritam prije konvergira, odnosno potreban je manji broj iteracija do stacionarnog stanja.

\subsection{Odabir broja grupa $K$}
\label{subsection:odabirK}
Kako bi se algoritam K-sredina mogao pokrenuti, potrebno je kao parametar $K$ specificirati koliko grupa treba proizvesti. Idealno se postavlja $K$ jednak broju prirodnih grupa, no to najčešće unaprijed nije poznato. Također postoji mogućnost da optimalan broj grupa $K$ nije jednoznačno određen jer postoji više načina prirodnog grupiranja podataka.

Ovo poglavlje opisuje često korištenu metodu za određivanje optimalnog broja grupa kod algoritma K-sredina: \textbf{metodu koljena} \engl{elbow method}. Još jedna metoda bit će opisana u poglavlju \ref{subsec:silhouette} nakon što se spomenu potrebni pojmovi.
\subsubsection{Metoda koljena}
Metoda koljena je grafička metoda koja prati ovisnost kriterijske funkcije $J$ (nakon što završi algoritam K-sredina) o broju grupa $K$. Jasno je da porast $K$ smanjuje iznos $J$ jer se grupe ‘‘smanjuju’’ povećanjem broja grupa u smislu da im se smanjuje raspršenost od centroida. U krajnjem slučaju, ukoliko se odabere broj grupa $K = N$, tada će svaka ulazna točka biti vlastita grupa i onda vrijedi $J = 0$.

Neka je na raspolaganju neki skup podataka sa dvjema realnim značajkama. Grafički prikaz tih podataka u dvodimenzijskom koordinatnom sustavu prikazan je na slici \ref{fig:elbowdata}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/elbowfig.png}
    \caption{Primjer skupa dvodimenzijskih podataka}
    \label{fig:elbowdata}
\end{figure} Cilj je metodom koljena odrediti optimalan broj grupa za algoritam K-sredina. Neka su mogući kandidati $K \in \left\{2, \dots, 8\right\}$. Za svaku od tih vrijednosti pokreće se algoritam K-sredina (uz strategiju K-means++ i Euklidovom udaljenosti) nad ovim skupom podataka i računa konačan iznos kriterijske funkcije $J$. Rezultati su prikazani grafom na slici \ref{fig:elbowgraph}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/elbowgraph.png}
    \caption{Ovisnost kriterijske funkcije o broju grupa}
    \label{fig:elbowgraph}
\end{figure}
Porast broja grupa do 4 bilježi strmi pad iznosa kriterijske funkcije, a daljnim porastom se ne dobiva značajan pad iznosa kriterijske funkcije. Graf kod broja grupa 4 ima oblik koljena, otkud dolazi naziv metode koljena. Navedeno razmatranje je jaka naznaka da se radi o optimalnom broju grupa $K = 4$. Broj grupa manji od toga naglo povećava iznos kriterijske funkcije, a broj grupa veći od toga ne rezultira značajnim smanjenjima kriterijske funkcije. Zaista, vizualnim promatranjem na slici \ref{fig:elbowdata} je jasno da se u navedenom primjeru radi o 4 prirodne grupe.


\section{Model Gaussove mješavine}
\label{gmm}
\textbf{Model Gaussove mješavine} \engl{Gaussian mixture model, GMM} je najčešće korišten algoritam mekog probabilističkog grupiranja koji kao cilj ima svakoj točki iz $\mathcal{D}$ pridružiti vjerojatnosti da pripada različitim grupama: tzv.\ \textbf{odgovornost}. Model Gaussove mješavine pretpostavlja da su podaci ‘‘nastali’’ iz kombinacije (mješavine) $K$ različitih vjerojatnosnih distribucija, odnosno gustoća. Svaka od $K$ grupa je predstavljena svojom vjerojatnosnom gustoćom, a u slučaju Gaussovih mješavina, radi se o multivarijatnim $n$-dimenzijskim normalnim (Gaussovim) razdiobama. To znači da je svaka grupa predstavljena svojom višedimenzijskom normalnom razdiobom: svojom lokacijom, kovarijacijskom matricom i vjerojatnosnom gustoćom.

\subsection{Multivarijatna normalna razdioba}
Slučajni $n$-dimenzijski stupčani vektor $\mathbf{X} = \left(X_1, \dots, X_n\right)^{\mathrm{T}}$ ima multivarijatnu normalnu razdiobu, odnosno
\[\mathbf{X} \sim \mathcal{N} \left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right)\]
sa \textbf{lokacijom}
$\boldsymbol{\mu} = \left(\operatorname{E}[X_1], \dots, \operatorname{E}[X_n]\right)^{\mathrm{T}}$ i \textbf{kovarijacijskom matricom} $\boldsymbol{\Sigma}$ kada svaka komponenta $X_i$ ima univarijatnu normalnu razdiobu. Kovarijacijska matrica je simetrična, pozitivno semidefinitna matrica reda $n \times n$ čiji elementi predstavljaju kovarijancu između komponenata, odnosno
\[\boldsymbol{\Sigma}_{i,j} = \operatorname{cov}
\left[X_i, X_j\right] 
= \operatorname{E}\left[ 
            \left(X_i - \operatorname{E}[X_i]\right)
            \left(X_j - \operatorname{E}[X_j]\right)
\right]
\]
Funkcija gustoće takvog slučajnog vektora tada glasi:
\begin{equation}
\label{gaussdensity}
p \left(\mathbf{x} \middle| \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)
= \frac{1}{\sqrt{\left(2 \pi\right)^n \operatorname{det} \left(\boldsymbol{\Sigma}\right)}}
\exp{\left(
    -\frac{1}{2}
        \left(\mathbf{x} - \boldsymbol{\mu}\right)^{\mathrm{T}}
        \boldsymbol{\Sigma}^{-1}
        \left(\mathbf{x} - \boldsymbol{\mu}\right)
\right)}
\end{equation}
Izraz
$\left(\mathbf{x} - \boldsymbol{\mu}\right)^{\mathrm{T}}
        \boldsymbol{\Sigma}^{-1}
        \left(\mathbf{x} - \boldsymbol{\mu}\right)
$ je kvadrat takozvane Mahalanobisove udaljenosti između vektora $\mathbf{x}$ i lokacije $\boldsymbol{\mu}$. To je višedimenzijska generaliziracija ideje udaljenosti točke od srednje vrijednosti u broju standardnih devijacija kod jednodimenzijskih razdioba.
\subsection{Model miješane gustoće}
Model miješane gustoće, pa tako i model Gaussove mješavine, je \textbf{generativan model} podataka koji svaki primjer generira, odnosno uzorkuje, iz miješane gustoće: linearne kombinacije $K$ gustoća svake grupe, odnosno $K$ komponenata. Svaki podatak u tom modelu se uzorkuje na sljedeći način: prvo se prema kategoričkoj distribuciji grupa, gdje se grupe sada prikazuju apriorno nepoznatim oznakama $y$ (jer se radi o nenadziranom strojnom učenju), odabere grupa $k$. Neka je $P \left(y = k\right)$ vjerojatnost da je odabrana grupa $k$. Nakon što se odabere grupa, primjer $\mathbf{x}$ se onda uzorkuje prema vjerojatnosnoj gustoći te grupe koju označavamo sa $p \left(\mathbf{x} \middle| y = k\right)$, odnosno ako se označe parametri distribucije (gustoće) grupe $k$ sa $\boldsymbol{\theta}_k$, onda se ta vjerojatnost označava sa $p \left(\mathbf{x} \middle| \boldsymbol{\theta}_k\right)$. Apriorna gustoća $p \left(\mathbf{x}\right)$ iz koje su uzorkovani ulazni primjeri iskaže se formulom potpune vjerojatnosti:
\begin{equation}
\label{mixdensity}
p \left(\mathbf{x}\right)
=
\sum_{k=1}^{K} P \left(y = k\right) p \left(\mathbf{x} \middle| y = k\right)
=
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x} \middle| \boldsymbol{\theta}_k\right)
\end{equation}
Uvedena je oznaka oznaku $\pi_k = P \left(y = k\right)$, i ta vjerojatnost se naziva \textbf{koeficijent mješavine} \engl{mixture weight}. Jasno je da mora vrijediti
\begin{equation}
\label{eq:summixture}
    \sum_{k=1}^{K} \pi_k = 1
\end{equation}
U slučaju modela Gaussove mješavine, gustoća grupe $p \left(\mathbf{x} \middle| \boldsymbol{\theta}_k\right)$ se računa prema \ref{gaussdensity}, gdje su parametri grupe
$\boldsymbol{\theta}_k = \left\{
    \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k
\right\}$.\\
Cilj mekog grupiranja kod ovakvog modela jest odrediti vjerojatnost da primjer $\mathbf{x}$ pripada nekoj grupi $k$, što se označava kao $P \left(y = k \middle| \mathbf{x}\right)$ i naziva se odgovornost. Za svaki podatak $\mathbf{x}^{(i)}$ iz skupa od $N$ ulaznih primjera $\mathcal{D} = \left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N}$ računamo odgovornost: vjerojatnost da je $\mathbf{x}^{(i)}$ uzorkovan iz grupe $k$. Ta vjerojatnost se može izraziti Bayesovom formulom:
\begin{equation}
\label{odgovornost}
P \left(y = k \middle| \mathbf{x}^{(i)}\right)
=
\frac
{P \left(y = k\right) p \left(\mathbf{x}^{(i)} \middle| y = k\right)}
{p \left(\mathbf{x}^{(i)}\right)}
=
\frac
{\pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)}
{\sum_{j=1}^{K} \pi_j \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_j\right)}
\end{equation}
Kako bi se mogla računati ovakva vjerojatnost, potrebno je poznavati parametre modela miješane gustoće. To znači da je za svaku od $K$ komponenata potrebno poznavati koeficijent mješavine i parametre gustoće svake komponente. U slučaju Gaussove mješavine, parametri svake komponente su lokacije i kovarijacijske matrice svake komponente. Neka su parametri modela predstavljeni oznakom
\[\boldsymbol{\theta} 
= \left\{ \pi_k, \boldsymbol{\theta}_k \right\}_{k=1}^{K}
= \left\{ \pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \right\}_{k=1}^{K}
\]
Sljedeće poglavlje se bavi pitanjem na koji način i kojim kriterijem se određuju parametri modela $\boldsymbol{\theta}$.

\subsection{Metoda najveće izglednosti}
Parametri modela $\boldsymbol{\theta}$ se procjenjuju \textbf{metodom najveće izglednosti} \engl{maximum likelihood estimation}. Potrebno je definirati funkciju izglednosti parametara $\boldsymbol{\theta}$ na uzorku $\mathcal{D}$. Pretpostavimo da su primjeri iz uzorka nezavisni, tada se funkcija izglednosti može definirati kao umnožak gustoća svakog primjera iz uzorka:
\[L \left(\boldsymbol{\theta} ; \mathcal{D} \right)
= \prod_{i=1}^{N} p \left( \mathbf{x}^{(i)} \right)
= \prod_{i=1}^{N} 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)
\]
Iz definicije i iz \ref{mixdensity} je jasno da funkcija izglednosti ovisi o parametrima $\boldsymbol{\theta}$.

Cilj metode najveće najveće izglednosti jest procijeniti parametre $\boldsymbol{\theta}$, koji će uz uzorak $\mathcal{D}$ maksimizirati funkciju izglednosti. Drugim riječima, potrebno je pronaći parametre koje od svih mogućih parametara (tzv.\ prostor parametara $\boldsymbol{\Theta}$) maksimiziraju vjerojatnost pojavljivanja podataka iz uzorka $\mathcal{D}$. Formalno, traži se
\[\widehat{\boldsymbol{\theta}}_{\text{mle}} =
\argmax_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
L \left(\boldsymbol{\theta} ; \mathcal{D} \right)
\]
Vrlo često u praksi se ne maksimizira funkcija izglednosti, nego \textbf{log-izglednosti}:
\begin{equation}
\label{loglikelihood}
\ell \left(\boldsymbol{\theta} ; \mathcal{D} \right) 
= \ln {L \left(\boldsymbol{\theta} ; \mathcal{D} \right)}
= \ln{\prod_{i=1}^{N} 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)}
= \sum_{i=1}^{N} \ln{ 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)
}
\end{equation}
Razlog tomu je taj što se primjerice u ovom slučaju radi sa produktima, a lakše je raditi sa sumama, a logaritmiranje produkte pretvara u sume. Zbog toga što je prirodni logaritam monotono rastuća funkcija, kad god se postiže maksimum funkcije izglednosti, tada i log-izglednost postiže svoj maksimum:
\[\widehat{\boldsymbol{\theta}}_{\text{mle}} =
\argmax_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
L \left(\boldsymbol{\theta} ; \mathcal{D} \right)
= \argmax_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
\ell \left(\boldsymbol{\theta} ; \mathcal{D} \right)
= \argmax_{\boldsymbol{\theta} \in \boldsymbol{\Theta}}
\sum_{i=1}^{N} \ln{ 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\theta}_k\right)
}
\]
Ovakav optimizacijski problem nema rješejne u zatvorenoj formi, stoga su potrebne iterativne optimizacijske metode koje će pokušati maksimizirati log-izglednost. Jedan od takvih metoda je \textbf{algoritam maksimizacije očekivanja}.

\subsection{Algoritam maksimizacije očekivanja}
Algoritam maksimizacije očekivanja \engl{expectation-maximization algorithm, EM} se osniva na proširenju generativnog modela opisanog u \ref{gmm} sa tzv.\ \textbf{skrivenim (latentnim) varijablama} koje modeliraju vrijednosti koje se ne opažaju u podacima. Takvo proširenje modela u konačnici omogućuje pronalazak iterativnog postupka koji postupno povećava log-izglednost. Proširenje modela i matematičke formulacije iza njega neće biti navedeni, nego će samo biti iskazan konačni algoritam.

EM algoritam ima vrlo sličnu strukturu kao i algoritam K-sredina. Najprije se na neki način inicijaliziraju parametri modela Gaussove mješavine $\boldsymbol{\theta} = 
\left\{\pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right\}_{k=1}^{K}$ kao što je algoritam K-sredina inicijalizirao početnih $K$ centroida. Broj komponenata Gaussove mješavine $K$ je, kao i kod algoritma K-sredina, parametar algoritma (razlikovati od parametara modela) i on u smislu algoritma mora biti unaprijed poznat. Nakon što su inicijalizirani parametri modela, iterativni postupak počinje. Najprije se za sve ulazne primjere računa odgovornost za svaku komponentu, odnosno grupu, prema formuli \ref{odgovornost}. Taj korak kod EM algoritma se naziva \textbf{E-korak}. U usporedbi sa algoritmom K-sredina, ovo je analogno pridruživanju točaka najbližim centroidima, što je bilo ‘‘čvrsto’’ pridruživanje, a sada se radi o probabilističkom mekom pridruživanju. Nakon što se izračunaju odgovornosti, parametri modela se iznova računaju na temelju novih izračunatih odgovornosti, što je analogno ponovnom računanju centroida kod algoritma K-sredina. Taj korak se naziva \textbf{M-korak}. Koraci E i M se ponavljaju do konvergencije parametara ili log-izglednosti. Kao oznaku odgovornosti korištena je oznaka
\[h_{k}^{(i)} = P \left(y = k \middle| \mathbf{x}^{(i)}\right)\]

\begin{croatianalgorithm}[H]
\label{em}
\caption{Algoritam maksimizacije očekivanja nad GMM}
\begin{algorithmic}

\STATE{\textbf{Parametri:} broj grupa, odnosno komponenata $K$}
\STATE{\textbf{Ulaz:} skup ulaznih primjera 
$\mathcal{D} = \left\{ \mathbf{x}^{(i)} \right\}_{i=1}^{N}$}
\STATE{\textbf{Izlaz:} parametri modela
$\boldsymbol{\theta} = \left\{ 
    \pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k
\right\}_{k=1}^{K}$}
\STATE{}

\STATE{\textbf{inicijaliziraj} parametre modela 
$\boldsymbol{\theta} = \left\{ 
    \pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k
\right\}_{k=1}^{K}$}

\REPEAT
\STATE{\textbf{E-korak:}}
\FORALL{ $\mathbf{x}^{(i)} \in \mathcal{D}$ i 
    $k \in \left\{1, \dots, K\right\}$ }
    \STATE{$h_{k}^{(i)} \gets 
    \frac
{\pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)}
{\sum_{j=1}^{K} \pi_j \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j\right)}
    $}
\ENDFOR
\STATE{}

\STATE{\textbf{M-korak:}}
\FORALL{ $k \in \left\{1, \dots, K\right\}$ }
    \STATE{$\boldsymbol{\mu}_k
    \gets \frac{\sum_{i=1}^{N} h_{k}^{(i)} \mathbf{x}^{(i)}}
    {\sum_{i=1}^{N} h_{k}^{(i)}}
    $}
    \STATE{$\boldsymbol{\Sigma}_k \gets
    \frac
    {\sum_{i=1}^{N} h_{k}^{(i)}
        \left( \mathbf{x}^{(i)} - \boldsymbol{\mu}_k \right)
        \left( \mathbf{x}^{(i)} - \boldsymbol{\mu}_k \right)^{\mathrm{T}}}
    {\sum_{i=1}^{N} h_{k}^{(i)}}$}
    \STATE{$\pi_k \gets 
    \frac{1}{N} \sum_{i=1}^{N} h_{k}^{(i)}$}
\ENDFOR
\STATE{}
\STATE{$\ell \left(\boldsymbol{\theta} ; \mathcal{D} \right) 
\gets \sum_{i=1}^{N} \ln{ 
\sum_{k=1}^{K} \pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)
}$}
\UNTIL{ne konvergiraju parametri $\boldsymbol{\theta}$ ili log-izglednost $\ell \left(\boldsymbol{\theta} ; \mathcal{D} \right)$}
\RETURN{$\boldsymbol{\theta}$}

\end{algorithmic}
\end{croatianalgorithm}
Iz ‘‘natreniranih’’ parametara modela se lako računaju odgovornosti iz formule \ref{odgovornost}, što je rezultat mekog grupiranja:
\[
P \left(y = k \middle| \mathbf{x}^{(i)}\right) =
h_{k}^{(i)} =
\frac
{\pi_k \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)}
{\sum_{j=1}^{K} \pi_j \, p \left(\mathbf{x}^{(i)} \middle| \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j\right)}
\]
Na slici \ref{fig:gmmem} je prikazan tijek rada algoritma nad dvodimenzijskim primjerima sa zadanim brojem komponenata $K = 2$. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/gmmem.jpg}
    \caption{Tijek rada algoritma maksimizacije očekivanja nad modelom Gaussovih mješavina}
    \label{fig:gmmem}
\end{figure}
Nijanse boja točaka označuju u kojoj mjeri točka pripada pojedinim grupama. Nakon svake iteracije se lokacije (zamišljene točke sa crnim obrubom) i kovarijacijske matrice (vizualizirane elipsama, sve točke na elipsi imaju istu Mahalanobisovu udaljenost od lokacije) mijenjaju dok se ne postigne stacionarno stanje: konvergencija log-izglednosti ili parametara modela. 

\subsection{Svojstva i složenost algoritma}
Algoritam maksimizacije očekivanja nastoji maksimizirati log-izglednost opisanu formulom \ref{loglikelihood}, ali u tome neće uvijek uspjeti. Pronaći će lokalni maksimum log-izglednosti, ali to ne mora biti globalni maksimum. Rezultat uvelike ovisi o kvalitetnoj inicijalizaciji parametara $\boldsymbol{\theta}$. Slučajna inicijalizacija je moguća, ali najčešće se parametri inicijaliziraju tako da se najprije nad skupom primjera provede algoritam K-sredina koji će kao rezultat dati $K$ čvrstih grupa. Lokacije $\boldsymbol{\mu}_k$ svake komponente se inicijaliziraju centroidima dobivenih grupa, a koeficijenti mješavine $\pi_k$ se računaju kao udjeli broja primjera u grupi $k$ u odnosu na ukupan broj primjera $N$. Kovarijacijske matrice $\boldsymbol{\Sigma}_k$ se mogu inicijalizirati procjenama kovarijanci iz uzorka: primjera iz grupe $k$.

Što se tiče vremenske složenosti, algoritam je učinkovit jer je linearan sa brojem primjera i brojem komponenata. Računanje Gaussove gustoće, uz prethodno izračunatu determinantu i inverz kovarijacijske matrice\footnote{Takozvana matrica preciznosti ili samo preciznost.}, je $\mathcal{O} \left(n^2\right)$. Prema tome, E-korak je vremenske složenosti $\mathcal{O} \left(N K n^2\right)$ i prostorne složenosti $\mathcal{O} \left(N K\right)$. Što se tiče M-koraka, za svaku komponentu, računanje nove lokacije je $\mathcal{O} \left(N n\right)$, računanje nove kovarijacijske matrice je $\mathcal{O} \left(N n^2\right)$, a računanje novog koeficijenta mješavine je $\mathcal{O} \left(N\right)$. U opisu algoritma nije eksplicitno navedeno, ali potrebno je i računati inverz i determinantu nove kovarijacijske matrice jer su potrebni za kasnije računanje gustoće, što je u općenitom slučaju $\mathcal{O} \left(n^3\right)$. Dakle, M-korak je složenosti $\mathcal{O} \left(K \left(N n^2 + n^3\right)\right)$. Računanje log-izglednosti je vremenske složenosti $\mathcal{O} \left(N K n^2\right)$. Najzahtjevniji od svih tih koraka je M-korak, stoga je uz $T$ iteracija vremenska složenost algoritma $\mathcal{O} \left(T K \left(N n^2 + n^3\right)\right)$.

Složenost se može poboljšati ako se uvedu ograničenja na oblik kovarijacijske matrice, što pojednostavljuje model Gaussove mješavine i operacije poput matričnog množenja, invertiranja i računanje determinante postaju jednostavnije. Tako primjerice kovarijacijske matrice mogu biti
\begin{enumerate}
    \item \textbf{Izotropne}: svaka komponenta ima kovarijacijsku matricu oblika $\boldsymbol{\Sigma}_k = \sigma_{k}^2 \, \mathbf{I}$, gdje je $\mathbf{I}$ jedinična matrica, a $\sigma_{k}^2$ dijeljena (između značajki) varijanca svake komponente. Značajke svake komponente su tada nekorelirane, imaju jednaku varijancu i grupe su sferičnog oblika.
    \item \textbf{Dijeljene}: svaka komponenta ima istu kovarijacijsku matricu.
    \item \textbf{Dijagonalne}: značajke komponente su nekorelirane.
\end{enumerate}
Algoritam je učinkovit i na velikim skupovima primjera zbog toga što je složenost u svakom slučaju linearan sa brojem primjera.

Zbog prirode generativnog modela podataka, kao i kod algoritma K-sredina, postoji implicitna pretpostavka o konveksnom obliku grupa. EM algoritam najčešće neće moći kvalitetno razdvojiti dvije grupe koje su relativno blizu, a barem jedna od njih ima nekonveksan oblik. Za razliku od algoritma K-sredina, EM algoritam će ispravno razdvojiti konveksne nejednako velike ili ukošene grupe, jer se svi takvi oblici mogu ispravno modelirati različitim oblicima kovarijacijske matrice. Kako svaka komponenta Gaussove mješavine predstavlja razdiobu normalnog slučajnog vektora, grupiranje ovim algoritmom je moguće samo ako su sve značajke realni brojevi, dakle model i algoritam nisu primjenjivi nad podacima koji nisu realni brojevi.

\subsection{Odabir broja komponenata $K$}
Odabir optimalnog broja komponenata je nužan uvjet kvalitetnog grupiranja, isto kao što je to bio slučaj kod algoritma K-sredina. Zbog toga što je model Gaussove mješavine statistički model, prikladno je koristiti \textbf{Akaikeov informacijski kriterij} \engl{Akaike information criterion, AIC} za određivanje optimalnog broja komponenata. Akaikeov informacijski kriterij se računa izrazom
\[AIC = 2m - 2 \ln{L}\]
gdje $m$ predstavlja broj parametara modela, a $L$ maksimizirana izglednost. Dobra procjena optimalnog broja komponenata $K$ kod modela Gaussovih mješavina je ona vrijednost $K$ koja minimizira Akaikeov informacijski kriterij. Akaikeov informacijski kriterij pada povećanjem izglednosti, a raste povećanjem broja parametara.\\
U slučaju Gaussovih mješavina, parametri modela su lokacije, kovarijacijske matrice i koeficijenti mješavine. Svaka lokacija se sastoji od $n$ parametara s obzirom da se radi o $n$-dimenzijskom vektoru, dakle za $K$ lokacija radi se o $Kn$ mnogo parametara. Koeficijenata mješavine ima $K$ jer toliko ima komponenata, ali uvijek se zadnji može izračunati iz ostalih $K-1$ jer vrijedi jednakost \ref{eq:summixture}, prema tome radi se o $K - 1$ mnogo parametara. Što se tiče kovarijacijskih matrica, broj parametara ovisi o tome postoje li ograničenja na oblik matrice i ako postoji, o kojem ograničenju se radi. U općenitom slučaju bez ograničenja, svaka kovarijacijska matrica sadrži $n$ dijagonalnih elemenata i $\binom{n}{2}$ nedijagonalnih elemenata, što za svaku kovarijacijsku matricu daje ukupno $\frac{n \left(n+1\right)}{2}$ parametara, a za njih $K$ mnogo to je ukupno $\frac{K n \left(n+1\right)}{2}$ parametara. Ovaj broj je manji ako se uvedu ograničenja na kovarijacijsku matricu. Primjerice, ako su kovarijacijske matrice dijagonalne, tada se broje samo dijagonalni elementi, što daje broj parametara $Kn$. Iz navedenih razmatranja za model Gaussove mješavine vrijedi, u općenitom slučaju
\[m = \frac{K n \left(n+1\right)}{2} + K n + K - 1\]
U svakom slučaju radi se o direktnoj ovisnosti o broju komponenata. Izglednost $L$ također u konačnici ovisi o broju komponenata uz fiksan skup podataka. U tu svrhu ima smisla označiti broj parametara sa $m \left(K\right)$, a izglednost sa $L \left(K\right)$. Optimalan broj komponenata neka je $K^{\ast}$ i koristeći Akaikeov informacijski kriterij on se procjenjuje kao
\[K^{\ast} = \argmin_{K \in \mathbb{N}} \left(2 \, m \left(K\right) - 2 \ln{L \left(K\right)}\right)\]

\subsubsection{Određivanje optimalnog modela}
Osim variranja broja komponenata, mogu se varirati i ograničenja na kovarijacijske matrice modela Gaussove mješavine. Na slici \ref{fig:gmmaic} je prikazan primjer skupa dvodimenzijskih podataka i želi se pronaći najbolji model Gaussove mješavine s obzirom na broj komponenata i ograničenja kovarijacijske matrice.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/gmmaic.png}
    \caption{Primjer odabira optimalnog modela Gaussovih mješavinae}
    \label{fig:gmmaic}
\end{figure}
Za svaki broj komponenata pokrenut je EM algoritam sa navedenim ograničenjima i zabilježena vrijednost Akaikeovog informacijskog kriterija na gornjem dijagramu. Iz dijagrama je vidljivo da za sve brojeve komponenata AIC ispada najmanji za modele bez ograničenja. Uz to, iznos AIC ispada najmanji za 2 komponente, stoga je takav model najbolji izbor. Zaista, promatranjem podataka vidljivo je da se radi o mješavini dvaju komponenata i općenitim kovarijacijskim matricama bez ograničenja. 

\section{Hijerarhijsko aglomerativno grupiranje}
U poglavlju \ref{vrstegrupiranja} je navedeno na koji način hijerarhijsko grupiranje gradi grupe, te su spomenuta dva pristupa hijerarhijskog grupiranja: aglomerativno i divizivno. Ovo poglavlje se se bavi standardnim algoritmom \textbf{hijerarhijskog aglomerativnog grupiranja} \engl{hierarchical agglomerative clustering, HAC}. Također je u poglavlju \ref{vrstegrupiranja} dan primjer jednog hijerarhijskog aglomerativnog grupiranja na slici \ref{fig:hier_clustering}. U tom primjeru je prikazano kako su se grupe međusobno spajale. Odluka koje dvije grupe spojiti nije slučajna, stoga prije formalnog opisa algoritma potrebno je prvo definirati pojmove vezane za spajanje grupa.

\subsection{Spajanje grupa}
Odluka koje dvije grupe spojiti se osniva na \textbf{kriteriju spajanja} \engl{linkage criterion}. Kriterij spajanja mjeri koliko su dvije grupe slične, odnosno različite. On se osniva na nekoj od mjera sličnosti između dviju točaka, bila to udaljenost, sličnost ili različitost, i one su opisane u poglavlju \ref{vrstegrupiranja}. Najčešće se u svrhu definiranja kriterija spajanja koriste mjere udaljenosti, no mogu se koristiti i mjere sličnosti ili različitosti.

Kriterij spajanja u ovom radu neće biti detaljno opisani kao mjera udaljenosti, samo na površnoj razini. Neka su su $\mathcal{C}_i$, $\mathcal{C}_j \subset \mathcal{D}$ neprazni i disjunktni podskupovi skupa ulaznih točaka $\mathcal{D}$. Kriterij povezanosti označimo sa $D \left(\mathcal{C}_i, \mathcal{C}_j\right)$ (bitno je ovu oznaku razlikovati od oznake udaljenosti između dvije točke, npr.\ $d \left(\mathbf{x}, \mathbf{y}\right)$) i on mjeri koliko su grupe $\mathcal{C}_i$ i $\mathcal{C}_j$ udaljene. Postoji mnogo kriterija spajanja, u ovom poglavlju se navode one koje su implementirane u programskoj knjižnici Scikit-learn.

\textbf{Jednostruka povezanost} \engl{single-linkage clustering} udaljenost između grupa definira kao minimalnu udaljenost između točaka u tim grupama, gdje je jedna točka iz jedne, a druga točka iz druge grupe:
\[D_{\text{min}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \min_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Potpuna povezanost} \engl{complete-linkage clustering} udaljenost između grupa definira slično kao i jednostruka povezanost, ali ovaj put se gleda maksimalna udaljenost:
\[D_{\text{max}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \max_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Prosječna povezanost} \engl{average-linkage clustering} udaljenost između grupa definira kao prosječnu udaljenost svih parova grupa, opet gdje je jedna točka iz jedna druga točka iz druge grupe:
\[D_{\text{avg}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \frac{1}{\vert \mathcal{C}_i \vert \cdot \vert \mathcal{C}_j \vert} 
            \sum_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Wardova metoda}, također poznata kao Wardova metoda minimalne varijance, prati zbroj kvadratnih pogrešaka (između točaka i centroida) u grupama prije i nakon spajanja. U osnovi Wardova metoda pretpostavlja da točke dolaze iz realnog koordinatnog prostora, no postoje generalizacije na bilo kakve podatke koje koriste mjeru sličnosti. Prema tome, kao mjera pogreške koristi se Euklidova udaljenost između točke i centroida, odnosno koristi se Euklidova norma. Neka je $\operatorname{ESS} \left(\mathcal{C}\right)$ zbroj kvadratnih pogreški \engl{error sum of squares} neke neprazne grupe $\mathcal{C}$ čiji je centroid $\boldsymbol{\mu}$:
\[\operatorname{ESS} \left(\mathcal{C}\right) = \sum_{\mathbf{x} \in \mathcal{C}} 
\Vert \mathbf{x} - \boldsymbol{\mu} \Vert^2\]
Wardova metoda udaljenost između grupa definira kao razliku sume kvadratnih pogrešaka nakon i prije spajanja grupa:
\[D_{\text{Ward}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
= \operatorname{ESS} \left(\mathcal{C}_i \cup \mathcal{C}_j\right) - \left(\operatorname{ESS} \left(\mathcal{C}_i\right) + \operatorname{ESS} \left(\mathcal{C}_j\right)\right)
\]
Gornja formula se može raspisivanjem svesti na jednostavniju formulu:
\[D_{\text{Ward}} \left( \mathcal{C}_i, \mathcal{C}_j \right)
= \frac{\vert \mathcal{C}_i \vert \cdot \vert \mathcal{C}_j \vert}
{\vert \mathcal{C}_i \vert + \vert \mathcal{C}_j \vert}
\left\Vert \boldsymbol{\mu}_i - \boldsymbol{\mu}_j \right\Vert^2
\]
gdje su $\boldsymbol{\mu}_i$ i $\boldsymbol{\mu}_j$ redom centroidi grupa $\mathcal{C}_i$ i $\mathcal{C}_j$.

\subsection{Opis algoritma}
Hijerarhijskog aglomerativno grupiranje najprije stvara $N$ grupa, odnosno na početku tretira svaku ulaznu točku iz $\mathcal{D}$ kao zasebnu grupu. Prilikom svake iteracije donosi odluku koje dvije grupe spojiti na temelju kriterija spajanja: spojiti one dvije grupe koje su najbliže prema kriteriju spajanja koji je unaprijed odabran kao parametar algoritma. Nakon spajanja grupa, broj ukupnih grupa se smanjuje za 1 i postupak se ponavlja.\\
Naravno, algoritam može ponavljati postupak dok ne ostane samo jedna grupa, ali to najčešće nije od koristi. Umjesto toga, postupak se može zaustaviti\footnote{Tada se na neki način radi o particijskom grupiranje, jer kada se zaustavi postupak, onda se ‘‘presiječe’’ horizontalno dendrogram i prihvate grupe kakve jesu na toj razini, bez hijerarhijske strukture.} kada se ispuni neki kriterij zaustavljanja, a to može biti jedno od sljedećih opcija:
\begin{itemize}
    \item dostignut je željeni broj grupa;
    \item najmanja udaljenost između dvije grupe je premašila neku granicu.
\end{itemize}
Odluka kada će postupak završiti je također parametar algoritma. Ukoliko se želi zaustaviti postupak nakon dostignutih $K$ grupa, broj $K$ je potrebno predati kao parametar, a ako se ne želi premašiti najmanja udaljenost $\epsilon$ tijekom spajanja, tada je potrebno specificirati broj $\epsilon$.\\
\begin{croatianalgorithm}[H]
\caption{Hijerarhijsko aglomerativno grupiranje}
\label{algo:hac}
\begin{algorithmic}
\STATE{\textbf{Parametri}: kriterij spajanja $D \left(\mathcal{C}_i, \mathcal{C}_j\right)$, uvjet zaustavljanja: broj grupa $K$ ili najveća dopuštena minimalna udaljenost između grupa $\epsilon$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} grupiranje: skup međusobno disjunktnih grupa
$\Gamma = \left\{ \mathcal{C}_i\right\}_{i=1}^{M}$ koji sačinjavaju skup točaka
$ \mathcal{D} = \bigcup_{i=1}^{M} \mathcal{C}_i $}, gdje broj grupa $M$ je ili nepoznat ili $M = K$ ovisno o uvjetu zaustavljanja

\STATE

\STATE{$\Gamma \gets \emptyset$}
\FORALL{$i \in \left\{1, \dots, N\right\}$}
    \STATE{$\mathcal{C}_i \gets \left\{ \mathbf{x}^{(i)} \right\}$}
    \STATE{$\Gamma \gets \Gamma \cup \left\{ \mathcal{C}_i \right\}$}
\ENDFOR
\WHILE{}
\STATE{\textbf{prekini} ako je zadan $K$ i vrijedi $\vert \Gamma \vert \leq K$}
\STATE{$\left(\mathcal{C}_i, \mathcal{C}_j\right) \gets 
        \argmin_{\left(\mathcal{C}_a, \mathcal{C}_b\right) \in \Gamma \times \Gamma}
        D \left(\mathcal{C}_a, \mathcal{C}_b\right)
$}
\STATE{\textbf{prekini} ako je zadan $\epsilon$ i vrijedi $D \left(\mathcal{C}_i, \mathcal{C}_j\right) > \epsilon$}
\STATE{$\mathcal{C}_i \gets \mathcal{C}_i \cup \mathcal{C}_j$}
\STATE{$\Gamma \gets \Gamma \setminus \left\{\mathcal{C}_j\right\}$}
\ENDWHILE
\RETURN{$\Gamma$}
\end{algorithmic}
\end{croatianalgorithm}

\subsection{Primjer grupiranja}
Neka su ulazni podaci točke sa dvjema realnim značajkama prikazani na slici \ref{fig:hierdata}. Radi lakšeg snalaženja, sve točke su označene brojevima.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{img/hierdata.png}
    \caption{Ulazni podaci}
    \label{fig:hierdata}
\end{figure}
Neka kriterij spajanja bude prosječna povezanost i neka algoritam spoji sve podgrupe, odnosno postavimo broj grupa $K = 1$. Nad tim podacima pokrenut je algoritam hijerarhijskog aglomerativnog grupiranja. Rezultat grupiranja prikazan je dendrogramom na slici \ref{fig:dendrogram}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.85]{img/dendro.png}
    \caption{Dendrogram hijerarhijskog aglomerativnog grupiranja}
    \label{fig:dendrogram}
\end{figure}
Na donjoj liniji se nalaze oznake svake točke, a na lijevoj liniji se nalazi mjera koja iskazuje koja je bila prosječna udaljenost (jer koristimo prosječnu povezanost kao kriterij spajanja) između grupa u trenutku spajanja. Ukoliko se želi particijski grupirati primjere, to se odlučuje prema vlastitoj volji. Primjerice, ukoliko se ne žele spajati grupe čija prosječna udaljenost prelazi $\epsilon = 3.5$, tada se dendrogram ‘‘presiječe’’ horizontalno na toj razini i kao rezultat dobiju 3 grupe jer bi se dendrogram presjekao na 3 mjesta. Na sličan način se mogu i po volji grupirati točke u proizvoljan broj grupa $K$: dendrogram se presiječe na razini na kojoj se nalazi $K$ grupa. Na slici \ref{fig:hiereps} je prikazan rezultat grupiranja ako se specificira $\epsilon = 3.5$.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{img/hiereps.png}
    \caption{Hijerarhijsko aglomerativno grupiranje uz $\epsilon = 3.5$}
    \label{fig:hiereps}
\end{figure}

\subsection{Svojstva i složenost algoritma}
Opisani algoritam daje dobre rezultate, no najveći nedostatak je vremenska i prostorna složenost. Neka se algoritam zaustavlja nakon što je preostalo $K$ grupa. To znači da algoritam u glavnoj petlji radi $N-K$ koraka. Korak u kojem se odlučuje koje dvije grupe spojiti mora proći kroz sve kombinacije grupa, što je vremenske složenosti $\mathcal{O} \left(N^2\right)$. Prema tome, vremenska složenost algoritma je $\mathcal{O} \left(N^2 \left(N-K\right)\right)$, a kako je u praksi $N$ puno veći od $K$, onda je vremenska složenost $\mathcal{O} \left(N^3\right)$. Iz tog razloga algoritam nije dobar izbor za velike skupove podataka. Informacija o udaljenosti između točaka se jako često koristi tijekom izvršenja algoritma, stoga implementacije u pravilu najprije konstruiraju tzv.\ \textbf{matricu udaljenosti} \engl{distance matrix}. Matrica udaljenosti je simetrična matrica dimenzija $N \times N$ i sadrži izračunatu udaljenost između svih parova točaka. Računanje udaljenosti je tada vremenski brzo, ali problem je u prostornoj složenosti: $\mathcal{O} \left(N^2\right)$ (jer je u najmanju ruku potrebno pohraniti $\binom{N}{2}$ brojeva), i to za velike skupove podataka već predstavlja veliki problem. Umjesto matrice udaljenosti, analogno postoji matrica sličnosti i matrica udaljenosti.

Standardni ‘‘naivni’’ algoritam je vremenske složenosti $\mathcal{O} \left(N^3\right)$, no postoje bolje implementacije složenosti $\mathcal{O} \left(N^2 \log N\right)$. Specijalno, ako se koriste jednostruka ili potpuna povezanost, postoje algoritmi složenosti $\mathcal{O} \left(N^2\right)$ poznati kao SLINK i CLINK.

\section{DBSCAN}
DBSCAN \engl{Density-based spatial clustering of applications with noise} je algoritam čvrstog particijskog grupiranja koji se zasniva na \textbf{modelu gustoće} \engl{density model} točaka u prostoru. Kod takvog modela grupe su sačinjene od ‘‘gusto’’ skupljenih točaka, odnosno točaka koje oko sebe imaju puno bliskih susjednih točaka.

\subsection{Model gustoće točaka u prostoru}
Pojam ‘‘gusto’’ skupljenih točaka se definira u odnosu na svaku točku $\mathbf{x}$ u prostoru kao dovoljan broj ostalih točaka (uključujući i $\mathbf{x}$) čija je udaljenost do $\mathbf{x}$ manja od neke fiksne vrijednosti. Neka je parametar $\varepsilon \geq 0$ polumjer okoline u kojoj se traže susjedne točke, odnosno točke udaljene za $\varepsilon$. Neka $\mathcal{R} \left(\mathcal{D}, d, \mathbf{x}, \varepsilon\right)$ označava okolinu točke $\mathbf{x}$ polumjera $\varepsilon$: skup točaka iz $\mathcal{D}$ čija je udaljenost od $\mathbf{x}$ manja ili jednaka $\varepsilon$, s obzirom na mjeru udaljenosti $d$:
\[\mathcal{R} \left(\mathcal{D}, d, \mathbf{x}, \varepsilon\right) = \left\{
    \mathbf{y}
    \mid
    \mathbf{y} \in \mathcal{D},
    d \left(\mathbf{x}, \mathbf{y}\right) \leq \varepsilon
\right\}\]
Bitno je napomenuti da točka $\mathbf{x}$ također pripada toj okolini.

Kako bi se upotpunio model gustoće točaka, potrebno je specificirati i koliko najmanje točaka unutar okoline polumjera $\varepsilon$ (oko neke točke) se mora nalaziti da bi se područje smatralo ‘‘gustim’’, odnosno koliko najmanje točaka mora biti da bi se stvorila jedna grupa. Neka je taj parametar $m$.\\
Dakle uz standardne podatke poput ulaznog skupa točaka i mjere udaljenosti, potrebno je specificirati i dodatne parametre $\varepsilon$ i $m$. S obzirom na uvedene pojmove, model gustoće točaka sve točke iz ulaznog skupa primjera $\mathcal{D}$ dijeli na:
\begin{enumerate}
    \item \textbf{Jezgrene točke} \engl{core points}: točke koje u svojoj okolini polumjera $\varepsilon$ imaju barem $m$ točaka (uključujući i sebe). Točka $\mathbf{x}$ je jezgrena točka ako vrijedi $\left\vert\mathcal{R} \left(\mathcal{D}, d, \mathbf{x}, \varepsilon\right)\right\vert \geq m$.
    
    \item \textbf{Granične točke} \engl{border points}: točke koje nemaju dovoljno ($m$) točaka u svojoj okolini polumjera $\varepsilon$, ali su dio okoline barem jedne jezgrene točke. Točka $\mathbf{x}$ je granična točka ako vrijedi:\\
    $\left\vert\mathcal{R} \left(\mathcal{D}, d, \mathbf{x}, \varepsilon\right)\right\vert < m, \, 
    \exists \mathbf{y} \in \mathcal{D} \, : \,
    \mathbf{x} \in \mathcal{R} \left(\mathcal{D}, d, \mathbf{y}, \varepsilon\right), \,
    \mathbf{y} \, \text{je jezgrena točka}$
    
    \item \textbf{Šum} \engl{noise points}: točke koje nemaju dovoljno točaka u svojoj okolini polumjera $\varepsilon$ i nisu dio okoline nijedne jezgrene točke.
\end{enumerate}
Kod takvog modela grupom se smatraju jezgrene točke i okoline oko njih. Ako se u okolini jezgrene točke nalazi neka druga jezgrena točka, oni zajedno sa svojim okolinama čine istu grupu. Na taj način se točke povezuju u istu grupu, odnosno čine gusto skupljene točke (gusto s obzirom na parametre $\varepsilon$ i $m$). Točka šuma u svojoj okolini može imati samo ostale točke šuma i granične točke. Granične točke će se nalaziti na rubovima takvih grupa, a šum predstavljaju stršeće vrijednosti i takve točke nisu svrstane ni u koju grupu. Ovakav model omogućava koncept stršećih vrijednosti, za razliku od prethodnih modela kod kojih su oni mogli predstavljati problem kod formiranja grupa jer se nalaze daleko od svih ostalih točaka.

Na slici \ref{fig:densitycluster} vizualiziran je primjer grupe kod takvog modela.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{img/densitycluster.png}
    \caption{Primjer grupe kod modela gustoća točaka}
    \label{fig:densitycluster}
\end{figure} Okoline točaka polumjera $\varepsilon$ vizualizirane su kružnicama oko točaka. U ovom primjeru $m = 4$. Crvene točke su jezgrene, žute su granične, a plava točka N je šum. Sve crvene točke oko sebe imaju 4 ili više točaka u svojoj okolini, što ih čini jezgrenima. Granične točke B i C imaju samo dvije točke u svojoj okolini, no nalaze se u barem jednoj okolini jezgrene točke, dok to nije slučaj sa točkom N. Dakle postoji jedna grupa koju čine sve crvene i žute točke, te postoji jedna točka šuma.

\subsection{Opis algoritma}
Cilj algoritma DBSCAN jest pronaći jezgrene točke i ‘‘proširivati’’ ih sve dok se ne nađu granične točke. Algoritam započinje tako da uzme proizvoljnu točku iz ulaznog skupa i provjerava koliko točaka ima u svojoj okolini. Ako nema dovoljno, onda se na trenutak točka svrstava kao šum, ako ima, onda je pronađena jezgrena točka i započinje se nova grupa. Točke iz okoline su također u toj grupi, pa ako je neka od tih točaka opet jezgrena, onda je i njihova okolina dio grupe i na taj način se gradi grupa do graničnih točaka. Tada se uzima nova neobrađena točka i postupak se ponavlja.

Rezultat grupiranja u kontekstu algoritma DBSCAN su oznake svake točke: točke sa istim oznakama su u istoj grupi. Jedina iznimka su točke šuma: za njih mora postojati posebna oznaka ali to ne znači da čine jednu grupu, nego se oni smatraju nesvrstanima. Stoga će skup podataka nakon završetka algoritma biti skup uređenih parova
\[\mathcal{D} = \left\{
    \left( \mathbf{x}^{(i)}, y^{(i)}  \right)
\right\}_{i=1}^{N}\]
gdje $y^{(i)}$ predstavlja oznaku točke $\mathbf{x}^{(i)}$. Prije izvršena algoritma, s obzirom da se radi o nenazdiranom strojnom učenju, oznake nisu poznate, odnosno nisu definirane. DBSCAN će tijekom izvršavanja dodjeljivati oznake svakoj točki koju obradi, najjednostavnije u obliku cijelih brojeva. Oznaka šuma bit će posebna oznaka. Neka je $\operatorname{L} \left(\mathbf{x}\right)$ oznaka grupe za točku $\mathbf{x}$, dakle za svaki $\mathbf{x}^{(i)} \in \mathcal{D}$ vrijedi
\[\operatorname{L} \left(\mathbf{x}^{(i)}\right) = y^{(i)}\]
U nastavku je opisan algoritam DBSCAN koji dodjeljuje oznake svakoj ulaznoj točki iz $\mathcal{D}$.

\begin{croatianalgorithm}[H]
\caption{DBSCAN}
\label{algo:dbscan}
\begin{algorithmic}
\STATE{\textbf{Parametri}: mjera udaljenosti $d$, minimalan broj točaka grupe $m$, polumjer okoline $\varepsilon$
}
\STATE{\textbf{Ulaz:} neoznačeni skup točaka $\mathcal{D} = \left\{\mathbf{x}^{(i)}
\right\}_{i=1}^{N}$}
\STATE{\textbf{Izlaz:} označeni skup točaka $\mathcal{D} = \left\{ \left(\mathbf{x}^{(i)}, y^{(i)}\right)
\right\}_{i=1}^{N}$ sa \textbf{definiranim} oznakama $y^{(i)}$
}

\STATE

\STATE{$k \gets 0$}
\FORALL{$\mathbf{x}^{(i)} \in \mathcal{D}$ \quad (1)}
    \STATE{\textbf{vrati se na} početak petlje (1) ako je oznaka $\operatorname{L} \left(\mathbf{x}^{(i)}\right)$ definirana}
    \STATE{$\mathcal{S} \gets \mathcal{R} \left(\mathcal{D}, d, \mathbf{x}^{(i)}, \varepsilon\right)$}
    \IF{$\left\vert \mathcal{S} \right\vert < m$}
        \STATE{$\operatorname{L} \left(\mathbf{x}^{(i)}\right) \gets \text{šum}$}
        \STATE{\textbf{vrati se na} početak petlje (1)}
    \ENDIF
    \STATE
    \STATE{$k \gets k + 1$}
    \STATE{$\operatorname{L} \left(\mathbf{x}^{(i)}\right) \gets k$}
    \STATE{$\mathcal{S} \gets \mathcal{S} \setminus
    \left\{\mathbf{x}^{(i)}\right\}$}
    \STATE{\textbf{stvori} red elemenata $\mathcal{Q} \gets \mathcal{S}$}
    \WHILE{dok red $\mathcal{Q}$ nije prazan \quad (2)}
        \STATE{\textbf{ukloni} prvi element $\mathbf{q}$ iz reda $\mathcal{Q}$}
        \STATE{\textbf{ako} 
        $\operatorname{L}\left(\mathbf{q}\right) = \text{šum}$
        \textbf{onda} $\operatorname{L} \left(\mathbf{q}\right) \gets k$} \COMMENT{Granična točka}
        \STATE{\textbf{vrati se na} početak petlje (2) ako je oznaka $\operatorname{L} \left(\mathbf{q}\right)$ definirana}
        \COMMENT {Granična točka druge grupe}
        \STATE{$\operatorname{L} \left(\mathbf{q}\right) \gets k$}
        \COMMENT{Jezgrena točka}
        \STATE{$\widehat{\mathcal{S}} \gets \mathcal{R} \left(\mathcal{D}, d, \mathbf{q}, \varepsilon\right)$}
        \IF{$\left\vert
        \widehat{\mathcal{S}}
        \right\vert \geq m$}
            \STATE{\textbf{stavi} sve elemente iz $\widehat{\mathcal{S}}$ na kraj reda $\mathcal{Q}$}
        \ENDIF
    \ENDWHILE
\ENDFOR

\end{algorithmic}
\end{croatianalgorithm}
Na slici \ref{fig:dbscanexample} je prikazan primjer grupiranja dvodimenzijskih podataka algoritmom DBSCAN.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{img/dbscanexample.png}
    \caption{Primjer grupiranja algoritmom DBSCAN}
    \label{fig:dbscanexample}
\end{figure}
Točke koje se nalaze unutar područja svijetlije nijanse su granične točke, a točke unutar područja tamnije nijanse su jezgrene točke. Sive točke su šum.

\subsection{Svojstva i složenost algoritma}
Najveća prednost modela gustoće točaka i algoritma DBSCAN jest mogućnost razdvajanja grupa nekonveksnih oblika, što nije slučaj kod algoritma K-sredina i EM algoritma. To je omogućeno samim modelom gustoće točaka jer grupe se grade prema tome koliko gusto su raspoređene u prostoru, a ne koliko su udaljene od centroida.

Nije potrebno unaprijed odrediti broj grupa koje algoritam treba proizvesti, što nije lako odrediti općenito. S druge strane, potrebno je specificirati dobre parametre $m$ i $\varepsilon$, što može biti izazovno ukoliko se ne poznaje priroda i domena podataka. Ukoliko se odaberu premali $m$ i preveliki $\varepsilon$, algoritam će spajati rijetka područja što može rezultirati spajanjem više prirodnih grupa u jednu zajedno sa šumom. S druge strane, ako se odaberu preveliki $m$ i premali $\varepsilon$, algoritam će veliki dio točaka označiti kao šum. Odabir ispravnih parametara $m$ i $\varepsilon$ postaje još izazovnije ukoliko grupe variraju u gustoći.

Vremenska složenost algoritma DBSCAN ovisi na koji način je izvedeno traženje okoline točke $\mathbf{x}$ polumjera $\varepsilon$: $\mathcal{R} \left(\mathcal{D}, d, \mathbf{x}, \varepsilon\right)$. Naivan pristup je linearna pretraga svakog elementa iz $\mathcal{D}$ i računanje udaljenosti do $\mathbf{x}$. Takav pristup je vremenske složenosti $\mathcal{O} \left(N n\right)$. Najčešće u implementacijama se za spremanje ulaznog skupa podataka $\mathcal{D}$ koriste strukture podataka osmišljene za spremanje prostornih podataka\footnote{Primjerice K-D stablo, R-stablo. Ako se ulazni podaci spreme u neku bazu podataka, ta baza onda može stvoriti jednu od takvih struktura kao indeks.}. U tom slučaju se traženje $\mathcal{R} \left(\mathcal{D}, d, \mathbf{x}, \varepsilon\right)$ ubrzava i vremenske je složenosti $\mathcal{O} \left(\log{N}\right)$. Traženje okoline se obavlja za svaku točku, dakle vremenska složenost algoritma DBSCAN je $\mathcal{O} \left(N^2\right)$ kod naivnog pristupa, a $\mathcal{O} \left(N \log{N}\right)$ ako se koristi brza pretraga okoline, što je prihvatljivo za velike skupove podataka. Što se tiče prostorne složenosti, ono je $\mathcal{O} \left(N\right)$ jer je potrebna struktura podataka koja omogućava spremanje ‘‘neobrađenih’’ susjednih točaka i njihovo uklanjanje nakon obrade. Ako se koristi prethodno pripremljena matrica udaljenosti, tada se radi o prostornoj složenosti $\mathcal{O} \left(N^2\right)$, ali se zauzvrat ubrzava računanje udaljenosti.

\chapter{Postupci vrednovanja algoritama grupiranja}
\label{evaluation}
Prethodno poglavlje se bavilo problemom grupiranja: na koji način podijeliti ulazni skup točaka u grupe sa međusobno sličnim točkama. Nakon što se to napravi na neki način, postavlja se pitanje na koji način ocijeniti rezultat grupiranja. Taj zadatak može biti jednako težak kao i sam problem grupiranja ukoliko nije dostupna neka vanjska informacija o tome kako bi podaci trebali stvarno biti grupirani. Naravno, kada bi na raspolaganju bile takve informacije, ne bi bilo potrebe za grupiranjem. Prema tome, vrednovanja grupiranja se dijele na \textbf{unutarnje} \engl{internal evaluation} i \textbf{vanjsko} \engl{external evaluation} vrednovanje. Kod vanjskog vrednovanja, osim ulaznih podataka dostupne su i njihove oznake, dok to nije slučaj kod unutarnjeg vrednovanja.

\section{Unutarnje vrednovanje}
Unutarnje vrednovanje nastoji nekom grupiranju dodijeliti ocjenu samo na temelju ulaznih podataka i oznaka koje je generiralo grupiranje. Kriteriji vrednovanja najčećše daju veće ocjene grupiranjima koja stvaraju grupe u kojima su točke međusobno slične, a različite u odnosu na točke iz ostalih grupa. Nedostatak takvih vrednovanja jest taj da će ono dati bolje rezultate algoritmima koji upravo nastoje povećati ocjenu tog specifičnog vrednovanja, a da takav algoritam nije u stvarnosti obavio grupiranje na optimalan način.

\subsection{Davies-Bouldin indeks}
Davies-Bouldin indeks promatra sve parove grupa i bilježi koliko su grupe udaljene, kao i koliko su grupe same po sebi raspršene. Neka je dano grupiranje $\Gamma = \left\{\mathcal{C}_k\right\}_{k=1}^{K}$. Neka je $S_i$ prosječna kvadratna udaljenost točaka iz grupe $\mathcal{C}_i$ od centroida $\boldsymbol{\mu}_i$ te grupe:
\[S_i = \frac{1}{\left\vert \mathcal{C}_i \right\vert}
\sum_{\mathbf{x} \in \mathcal{C}_i} \Vert \mathbf{x} - \boldsymbol{\mu}_i  \Vert^2
\]
$S_i$ je mjera raspršenja unutar grupe $\mathcal{C}_i$. Naravno, kako bi se mogao prema ovakvoj definiciji računati $S_i$, mora se moći izračunati centroid, a to je moguće jedino ako podaci dolaze iz nekog vektorskog prostora. Ranije smo spomenuli da se najčešće radi o realnom koordinatnom prostoru pa se koristi Euklidova udaljenost i Euklidova norma. Neka je $M_{ij}$ udaljenost centroida grupa $\mathcal{C}_i$ i $\mathcal{C}_j$:
\[M_{ij} = \Vert \boldsymbol{\mu}_i - \boldsymbol{\mu}_j \Vert\]
$M_{ij}$ mjeri koliko dobro su razdvojene grupe u smislu koliko su im udaljeni centroidi. Neka je $R_{ij}$ definiran kao
\[R_{ij} = \frac{S_i + S_j}{M_{ij}}\]
Ono mjeri koliko dobro su grupirane međusobno grupe $\mathcal{C}_i$ i $\mathcal{C}_j$: veća razdvojenost i manje raspršenje unutar grupa će dati manji iznos $R_{ij}$, što znači bolje grupiranje između te dvije grupe. $R_{ij}$ se može shvatiti kao neka mjera sličnosti između dvije grupe. Označimo sa $R_i$ kao najveći iznos $R_{ij}$ za grupu $\mathcal{C}_i$ i sve ostale grupe:
\[R_i = \max_{\substack{1 \leq j \leq K \\ j \neq i}} R_{ij}\]
Davies-Bouldin indeks $DBI$ se tada definira kao
\[DBI = \frac{1}{K} \sum_{k=1}^{K} R_k\]

Nedostatak vrednovanja Davies-Bouldin indeksom je činjenica da se veći iznosi postižu ako su grupe konveksnog oblika, odnosno iznosi su veći kod grupiranja sa centroidnim modelima (algoritam K-sredina, EM algoritam i model Gaussovih mješavina) nego kod modela kojeg koristi primjerice DBSCAN. Uz to, Davies-Bouldin indeks se može računati samo nad grupiranjima kod kojih je moguće računati centroide i može se koristiti Euklidova udaljenost.
\subsection{Vrijednost siluete}
\label{subsec:silhouette}
Vrijednost siluete \engl{silhouette score} mjeri koliko je svaka točka iz $\mathcal{D}$ slična svojoj grupi u odnosu na ostale grupe, dakle svakoj točki se pridružuje vrijednost siluete. Metoda koja se zasniva na ovoj mjeri se naziva \textbf{metoda siluete}. Za svaku točku $\mathbf{x}^{(i)}$ iz ulaznog skupa točaka $\mathcal{D} = \left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N}$ neka $\mathcal{C} \left(i\right)$ označava grupu kojoj pripada ta točka. Također je na raspolaganju grupiranje koje je generirao promatrani algoritam, odnosno skup od $K$ grupa $\Gamma = \left\{\mathcal{C}_k\right\}_{k=1}^{K}$.
Prije nego što se definira vrijednost siluete $s (i)$,  neka $a (i)$ označava prosječnu udaljenost točke $\mathbf{x}^{(i)}$ do ostalih točaka u svojoj grupi:
\[a (i) = \frac{1}{\left\vert \mathcal{C} \left(i\right) \right\vert - 1} \sum_{\substack{\mathbf{y} \in \mathcal{C} \left(i\right) \\ \mathbf{y} \neq \mathbf{x}^{(i)}}} d \left(\mathbf{x}^{(i)}, \mathbf{y}\right)
\]
Neka $b (i)$ označava prosječnu udaljenost točke $\mathbf{x}^{(i)}$ do točaka najbliže grupe (najbližu u smislu prosječne udaljenosti):
\[b (i) = \min_{\substack{\mathcal{C} \in \Gamma \\ \mathcal{C} \neq \mathcal{C} \left(i\right)}} \frac{1}{\vert \mathcal{C} \vert} \sum_{\mathbf{y} \in \mathcal{C}}
d \left(\mathbf{x}^{(i)}, \mathbf{y}\right)\]
Vrijednost siluete se tada definira kao
\[s (i) = \frac{b (i) - a (i)}{\operatorname{max} \left\{a (i), b (i)\right\}}\]
a u slučaju da je točka $\mathbf{x}^{(i)}$ sama u svojoj grupi, odnosno $\left\vert \mathcal{C} \left(i\right) \right\vert = 1$, tada je $s (i) = 0$ jer vrijednost $a (i)$ nema smisla. Uzimajući u obzir međusobne odnose $a (i)$ i $b (i)$, definicija vrijednosti siluete se može izraziti na sljedeći način:
\[
s (i) = \begin{cases}
 0 & \text{ako} \; \left\vert \mathcal{C} \left(i\right) \right\vert = 1 \\
 1 - \frac{a (i)}{b (i)} & \text{ako} \; a (i) < b (i) \\  
 0 & \text{ako} \; a (i) = b (i) \\  
 \frac{b (i)}{a (i)} - 1 & \text{ako} \; a (i) > b(i)  
 \end{cases}
\]
Jasno je vidljivo iz ovakve definicije vrijednosti siluete da ona može samo poprimiti vrijednosti između -1 i 1, odnosno $-1 \leq s (i) \leq 1$. Kako $a (i)$ mjeri koliko je točka $\mathbf{x}^{(i)}$ blizu ostalim točkama iz svoje grupe, manja vrijedost $a(i)$ znači veću sličnost $\mathbf{x}^{(i)}$ sa ostalim točkama iz svoje grupe. S druge strane, $b(i)$ mjeri koliko je $\mathbf{x}^{(i)}$ blizu točkama iz najbliže susjedne grupe, stoga veća vrijednost $b (i)$ znači veću ‘‘razdvojenost’’, odnosno veću različitost od ostalih grupa. Prema tome, kada je $a(i)$ puno manji od $b(i)$, vrijednost siluete $s (i)$ će biti blizu 1, što označava dobro grupiranje konkretno za promatranu točku. Analogno, kada je $b (i)$ puno veći od $a (i)$, to znači da je točka bliža nekoj drugoj grupi nego vlastitoj. Tada će $s(i)$ biti blizu -1 i to označava loše grupiranje. Vrijednost siluete blizu 0 znači da se $a(i)$ i $b(i)$ malo razlikuju, što znači da je točka $\mathbf{x}^{(i)}$ na granici dvije grupe.

Osim što se može računati vrijednost siluete za svaku točku $\mathbf{x}^{(i)}$ iz ulaznog skupa točaka $\mathcal{D}$, može se računati vrijednost siluete za cijelo grupiranje kao aritmetička sredina silueta svih točaka:
\[s = \sum_{i=1}^{N} \frac{s (i)}{N}\]

Kao i kod Davies-Bouldin indeksa, vrijednosti siluete su veće kod točaka grupa konveksnog oblika, što ga ne čini dobrim izborom unutarnog vrednovanja ako se grupiranje radi modelom koji ne pretpostavlja konveksan oblik grupa (primjerice model gustoće točaka kod algoritma DBSCAN).

\subsubsection{Metoda siluete}
Metoda siluete, odnosno analiza siluete, se zasniva na računanju vrijednosti siluete svake točke iz ulaznog skupa. Metoda siluete se može iskoristiti kako bi se odredio optimalan broj grupa kod algoritma K-sredina. U nastavku je primjer analize siluete nad istim ulaznim podacima kao na slici \ref{fig:elbowdata} gdje je bila demonstrirana metoda koljena.

Neka je dano grupiranje koje je proizvelo K-sredina. Nakon grupiranja, vrijednost siluete se računa za svaku točku iz ulaznog skupa i grafički se prikazuje na dijagramu uz zabilježenu informaciju u kojoj grupi se nalazi svaka točka. Također se prati koja je prosječna vrijednost siluete za sve točke, te se grafički pokušava zaključiti koliko je kvalitetno grupiranje s obzirom na dobivene vrijednosti siluete u odnosu na prosječnu vrijednost.

\section{Vanjsko vrednovanje}
Vanjsko vrednovanje, uz ulazne podatke i grupe koje je promatrani algoritam generirao, na raspolaganju ima i oznake svake točke koje otkrivaju referentno grupiranje. Te oznake nisu poznate postupcima nenadziranog strojnog učenja, pa tako i algoritmima grupiranja. Do njih je ponekad moguće doći primjerice ljudskom ocjenom ulaznog skupa primjera, ili su mogli biti poznati ranije, no odluka je bila ne specificirati oznake metodama nenadziranog strojnog učenja (pa tako i grupiranja).


\subsection{Randov indeks}
Randov indeks mjeri preciznost grupiranja tako da promatra sve parove ulaznih primjera i uspoređuje dva grupiranja istovremeno. Jedno grupiranje je ono koje je generirao algoritam grupiranja, a drugo grupiranje (referentno grupiranje) je prema prethodno poznatim oznakama: dva primjera su u istoj grupi ako imaju istu oznaku. Za svaki mogući par primjera se promatra jesu li završili u istoj grupi, i jesu li oni u istoj grupi kod referentnog grupiranja. Parovi ulaznih primjera se tada dijele na:
\begin{enumerate}
    \item \textbf{Istinito pozitivne} \engl{true positive}: primjeri se nalaze u istoj grupi u oba grupiranja. Neka je $TP$ broj takvih parova.
    \item \textbf{Istinito negativne} \engl{true negative}: primjeri se nalaze u različitim grupama u oba grupiranja. Neka je $TN$ broj takvih parova.
    \item \textbf{Lažno pozitivne} \engl{false positive}: primjeri se nalaze u istoj grupi u dobivenom grupiranju, a u referentnom grupiranju se nalaze u različitim grupama. Neka je $FP$ broj takvih parova.
    \item \textbf{Lažno negativne} \engl{false negative}: primjeri se nalaze u različitim grupama u dobivenom grupiranju, a u referentnom grupiranju se nalaze u istoj grupi. Neka je $FN$ broj takvih parova.
\end{enumerate}

Parovi primjera su dobro grupirani ako situacija odgovara referentnom grupiranju, dakle promatraju se primjeri koji su istinito pozitivni i istinito negativni.
Randov indeks $R$ se tada definira kao:
\[R = \frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{\binom{N}{2}}\]
Jasno je da su u nazivniku pokriveni svi mogući parovi ulaznih primjera, dakle broj takvih parova mora biti $\binom{N}{2}$. Moguće vrijednosti su $R \in \left[0, 1\right]$
Grupiranje koje je identično referentnom grupiranju ima $R = 1$.

Primjerice, neka su ulazni primjeri skup od 5 prirodnih brojeva od 1 do 5. Neka su ulazni podaci grupirani na sljedeći način:
\[\left\{ \left\{1, 2, 4\right\}, \left\{3, 5\right\} \right\}\]
Neka je referentno grupiranje
\[\left\{\left\{1,2\right\}, \left\{3,4\right\}, \left\{5\right\}\right\}\]
Za ovaj primjer je $TP = 1$ jer samo par 12 se nalazi u istim grupama u oba grupiranja, a $TN = 5$ i radi se o parovima 13, 15, 23, 25, 45. Prema tome $R = \frac{1 + 5}{\binom{5}{2}} = \frac{6}{10} = 0.6$.

Nedostatak vrednovanja Randovim indeksom jest činjenica da jednako nagrađuje istinito pozitivne i istinito negativne parove primjera. U slučaju da se brojevi grupa razlikuju između dobivenog i referentnog grupiranja, to rezultira velikim brojem istinito negativnih primjerima. Postoji mogućnost da Randov indeks slučajnog grupiranja bude broj puno veći od 0, kao i da Randov indeks relativno lošeg grupiranja bude broj blizu 1. Taj problem rješava \textbf{prilagođeni Randov indeks} \engl{adjusted Rand index} koji uzima u obzir očekivani Randov indeks za slučajna grupiranja. Prilagođeni Randov indeks slučajnim grupiranjima pridjeljuje vrijednosti bliže nuli, a lošim grupiranjima može pridijeliti negativnu vrijednost. Prilagođeni Randov indeks je češće korištena mjera vanjskog vrednovanja u odnosu na neprilagođeni. Način na koji se računa prilagođeni Randov indeks i matematičke formulacije iza njega nećemo navoditi.

\subsection{Uzajamna informacija}
\textbf{Uzajamna informacija} \engl{mutual information} je mjera iz teorije informacije koja mjeri kako se mijenja nesigurnost predviđanja jedne slučajne varijable ukoliko znamo realizacije druge slučajne varijable. U kontekstu grupiranja radi se o oznakama grupa, (multi)skupovima koji u sebi sadrže oznake za svaki ulazni podatak.

Neka je $X$ diskretna slučajna varijabla koja može poprimiti vrijednosti $x_1, \dots, x_n$ sa vjerojatnostima $P \left(x_i\right) = P \left(X = x_i\right)$. \textbf{Entropija} slučajne varijable $X$ se definira kao
\[\operatorname{H} \left(X\right) = \sum_{i=1}^{n} P \left(x_i\right) \log{P \left(x_i\right)}\]
gdje izbor baze logaritma može varirati između primjena, no najčešće se uzima baza 2. Kod grupiranja, skup oznaka grupa se mogu smatrati kao realizacijama neke slučajne varijable. Neka skup $U$ sadrži ukupno $N$ oznaka ($\vert U \vert = N$) od kojih je $n$ različitih oznaka $u_1, \dots, u_n$, te neka je $U_i$ podskup (konceptualno radi se o grupi sa tom oznakom) od $U$ koji sadrži samo oznake $u_i$. Entropija takvog grupiranja $\operatorname{H} \left(U\right)$ je tada
\[\operatorname{H} \left(U\right) = 
- \sum_{i=1}^{n} P \left(u_i\right) \log{P \left(u_i\right)}
= - \sum_{i=1}^{n} \frac{\left\vert U_i \right\vert}{N}
\log{\frac{\left\vert U_i \right\vert}{N}}\]
Primjerice, entropija skupa $U = \left\{0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2\right\}$ (ovdje je $N = 11$ i $n = 3$ jer su prisutne 3 različite vrijednosti) je
\[\operatorname{H} \left(U\right) = - \left(\frac{3}{11} \log{\frac{3}{11}}
+ \frac{2}{11} \log{\frac{2}{11}}
+ \frac{6}{11} \log{\frac{6}{11}}\right) = 1.435
\]
te bi to odgovaralo entropiji grupiranja
$\left\{\left\{a, b, c\right\}, \left\{d, e\right\}, \left\{f, g, h, i, j, k\right\}\right\}$ gdje mala slova predstavljaju svaki ulazni podatak.

Uzajamna informacija za dva grupiranja (dva skupa oznaka) $U$ i $V$, gdje $U$ ima $n$ različitih oznaka, a $V$ ima $m$ različitih oznaka, se definira kao
\[\operatorname{MI} \left(U, V\right) = \sum_{i=1}^{n} \sum_{j=1}^{m} 
P \left(u_i, v_j\right) \log{\frac{P \left(u_i, v_j\right)}{P \left(u_i\right) P \left(v_j\right)}}
= \sum_{i=1}^{n} \sum_{j=1}^{m} 
\frac{\left\vert U_i \cap V_j \right\vert}{N} \log{\frac{N \left\vert U_i \cap V_j \right\vert}{\left\vert U_i \right\vert  \left\vert V_j \right\vert}}
\]
Što su grupiranja sličnija, to će uzajamna informacija biti veća.

Osim uzajamne informacije, koristi se i \textbf{normalizirana uzajamna informacija} koja se definira kao
\[\operatorname{NMI} \left(U, V\right) = \frac{\operatorname{MI} \left(U, V\right)}
{\frac{\operatorname{H} (U) + \operatorname{H} (V)}{2}}\]
i moguće vrijednosti su između 0 i 1. Obje mjere imaju isti problem kao i neprilagođeni Randov indeks: slučajna grupiranja imaju iznose koje su znatno veće od 0. Na isti način se uvodi i najčešće koristi \textbf{prilagođena uzajamna informacija} \engl{adjusted mutual information}.

\chapter{Programsko ostvarenje i rezultati}
\label{chap:results}

\section{Programska knjižnica Scikit-learn}
Scikit-learn je besplatna programska knjižnica otvorenog koda za programski jezik Python namijenjena za strojno učenje. Za potrebe ovog završnog rada posebno je zanimljiv modul za grupiranje \texttt{sklearn.cluster} gdje je implementirana većina\footnote{Osim EM algoritma Gaussovih mješavina koja je dio \texttt{sklearn.mixture} modula.} algoritama grupiranja. Mjere vrednovanja su implementirane u \texttt{sklearn.metrics}.

Svaki algoritam grupiranja je modeliran razredom, a u konstruktoru takvog razreda specificiraju se parametri i postavke algoritma. Primjerice, algoritam K-sredina je modeliran razredom \texttt{sklearn.cluster.KMeans} i u konstruktoru se može specificirati: broj grupa, maksimalan broj iteracija, strategiju inicijalizacije početnih centroida, kolika promjena kriterijske funkcije između iteracija se smatra konvergencijom, koliko puta ponoviti algoritam, žele li se prethodno izračunati udaljenosti između svih parova točaka, itd. Na sličan načim možemo podešavati i ostale algoritme grupiranja, a kod onih čiji model to dopušta može se specificirati i proizvoljna mjera udaljenosti.

Pokretanje algoritma nad stvorenim objektom postiže se pozivom metode \texttt{fit} ili neke srodne metode (primjerice metoda \texttt{fit\_predict}). Kao obavezan argument se prima matrica\footnote{Za modeliranje i manipulaciju matrica koristi se programska knjižnica NumPy} dimenzija $N \times n$, gdje svaki redak te matrice predstavlja jedan ulazni $n$-dimenzionalni podatak. Metoda \texttt{fit} ne obavlja nužno grupiranje, već ‘‘namješta’’ model iz ulaznih podataka. U kontekstu strojnog učenja, radi se o treniranju modela. U slučaju algoritma K-sredina, računaju se završni centroidi. Kod EM algoritma Gaussovih mješavina, računaju se parametri $\boldsymbol{\theta}$. Kod hijerarhijskog aglomerativnog grupiranja, gradi se ‘‘presiječeno’’ (dakle ne potpuno) stablo hijerarhije, a algoritam DBSCAN traži jezgrene točke i usput obavlja grupiranje. Metoda \texttt{fit\_predict} uz treniranje obavlja i grupiranje ulaznog skupa točaka. Pozivatelju se vraća polje duljine $N$ koje sadrži cjelobrojne oznake čvrstih grupa počevši od 0 za svaki ulazni podatak (oznaka na indeksu $i$ se oznaka grupe za ulazni podatak u $i$-tom retku). Algoritam DBSCAN šum označuje sa -1. Ako postoji za konkretan razred, može se koristiti i metoda \texttt{predict} koja dodjeljuje oznake grupa podataka koji nisu nužno ulazni podaci nad kojima je treniran model. Kako se radi o nenadziranom strojnom učenju, mogu se i novi neviđeni podaci klasificirati na temelju starih (korištenih za treniranje modela). Od opisanih algoritma u poglavlju \ref{clusteringalgos}, tu mogućnost nude algoritam K-sredina i EM algoritam Gaussovih mješavina. Kod algoritma K-sredina, svaki podatak predan kao argument metode \texttt{predict} se svrstava u onu grupu čiji centroid je najbliži, a kod Gaussovih mješavina se svrstava u onu komponentu sa najvećom odgovornošću. Posebno za model Gaussovih mješavina, postoji i metoda \texttt{predict\_proba} koja vraća odgovornosti (meko grupiranje) za svaki ulazni podatak i svaku komponentu u obliku matrice $N \times K$. Naglasak ovog rada je usporedba metoda grupiranja postojećih podataka, a ne klasifikacija novih neviđenih primjera, stoga je korištena samo metoda \texttt{fit\_predict}.

\section{Skupovi podataka}
Za potrebe usporedbe algoritama grupiranja korišteno je 6 različitih skupova podataka. Radi se o umjetno stvorenim podacima koji služe za demonstraciju i usporedbu različitih algoritama grupiranja. Kako bi svi opisani algoritmi i modeli grupiranja bili primjenjivi, značajke svakog podatka su realni brojevi, a njihova dimenzija je 2 kako bi se lakše vizualno prikazali. Svaki skup ima 1500 ulaznih točaka kako trajanje izvršenja algoritama ne bi bilo predugo, a s druge strane sasvim dovoljno za demonstraciju. Prema tome
\[N = 1500, \qquad n = 2, \qquad \mathcal{V} \subseteq \mathbb{R}^2\]

\chapter{Zaključak}
\label{chap:conclusion}
Zaključak.

\bibliography{literatura}
\bibliographystyle{fer}

\begin{sazetak}
Sažetak na hrvatskom jeziku.

\kljucnerijeci{Ključne riječi, odvojene zarezima.}
\end{sazetak}
%‘‘slični’’
% TODO: Navedite naslov na engleskom jeziku.
\engtitle{Title}
\begin{abstract}
Abstract.

\keywords{Keywords.}
\end{abstract}

\end{document}

