\documentclass[times, utf8, zavrsni]{fer}
\usepackage{booktabs}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{relsize}

\newenvironment{croatianalgorithm}[1][]
  {\begin{algorithm}[#1]
     \selectlanguage{croatian}%
     \floatname{algorithm}{Algoritam}%
     \renewcommand{\algorithmicrepeat}{\textbf{ponavljaj}}%
     \renewcommand{\algorithmicuntil}{\textbf{dok}}%
     \renewcommand{\algorithmicforall}{\textbf{za svaki}}%
     \renewcommand{\algorithmicendfor}{\textbf{kraj za}}
     \renewcommand{\algorithmicdo}{\,}
     \renewcommand{\algorithmicwhile}{\textbf{ponavljaj}}
     \renewcommand{\algorithmicendwhile}{\textbf{kraj ponavljaj}}
     \renewcommand{\algorithmicreturn}{\textbf{vrati}}
     \renewcommand{\algorithmicif}{\textbf{ako}}
     \renewcommand{\algorithmicendif}{\textbf{kraj ako}}
     % Set other language requirements
  }
  {\end{algorithm}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}

% TODO: Navedite broj rada.
\thesisnumber{000}

% TODO: Navedite naslov rada.
\title{Usporedba metoda grupiranja primjenom programskog jezika Python}

% TODO: Navedite vaše ime i prezime.
\author{Tomislav Bjelčić}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{}

\tableofcontents

\chapter{Uvod}

\textbf{Grupiranje} \engl{clustering} je postupak kojim se neki skup podataka razvrstava u skupine, odnosno \textbf{grupe} \engl{clusters}, u kojima su podaci međusobno slični. Grupiranje je jedno od metoda \textbf{nenadziranog strojnog učenja} \engl{unsupervised learning}, dakle ulazni podaci nisu \textbf{označeni} \engl{unlabeled}, odnosno nemaju neku ciljnu vrijednost koja bi naznačila kojoj grupi pripada neki podatak. Algoritmi grupiranja iz takvih podataka onda moraju sami prepoznati grupe podataka, odrediti za svaki ulazni podatak kojoj grupi bi pripadao i ukoliko je to moguće, odrediti \textbf{stršeće vrijednosti} \engl{outliers}.

Uzmimo neki jednostavan primjer. Neka na raspolaganju imamo podatke o visini i masi odraslih pasa iz nekog 
skloništa za životinje. U ovom jednostavnom primjeru ulazni podaci imaju dvije značajke, odnosno dimenzije: visina i masa. Općenito, ulazni podaci mogu imati proizvoljno mnogo značajka. Prikažimo ulazne podatke kao 
točke na grafu gdje os apscisa predstavlja masu, a os ordinata predstavlja visinu pojedinog psa.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.52]{img/uvod.png}
    \caption{Podaci o psima u skloništu za životinje}
\end{figure}

Promatranjem točaka na grafu možemo uočiti da se podaci prirodno grupiraju u tri grupe, koje odgovaraju različitim pasminama. Sjetimo se, ulazni podaci nisu označeni, dakle jedino na raspolaganju imamo masu i visinu. Htjeli bismo da odabran algoritam grupiranja prepozna da se radi o tri grupe i slične točke pridruži istoj grupi. Općenito, poželjno je da algoritam grupiranja podatke grupira na onaj način koji odgovara prirodnom grupiranju. U našem primjeru je prirodno grupiranje vizualno očito i ono odgovara pasminama, no to ne mora biti slučaj. Neće uvijek ni vizualnom metodom biti jednoznačno jasno kakvo je njihovo prirodno grupiranje. Ovdje bismo mjeru sličnosti mogli definirati koristeći udaljenost točaka (što su točke bliže, više su slične) s obzirom da se radi o brojevnim podacima, no općenito podaci mogu biti bilo kakve prirode, što će pojedini algoritmi grupiranja uzimati u obzir.

\chapter{Općenito o algoritmima grupiranja}
Definirajmo što su ulazni podaci. U kontekstu algoritama grupiranja\footnote{Isto tako i u kontekstu nenadziranog strojnog učenja.} ulazni podaci (primjeri) su skup od $N$ neoznačenih, višedimenzionalnih \textbf{točaka} (vektora) \[\mathcal{D} = \left\{ \mathbf{x}^{(i)} \right\}_{i=1}^{N} \]
gdje svaka točka $\mathbf{x} = \left(x_1, x_2, \dots, x_{n-1}, x_n\right)$ ima $n$ \textbf{značajki} \engl{features}, odnosno dimenzija. Prostor podataka iz koje dolaze pojedine značajke, odnosno točke, mogu biti razne. Primjerice, može biti riječ o realnim brojevima (uvodni primjer), znakovima (primjerice grupiranje neoznačenih novinskih članaka), kategorijskim podacima poput vrijednosti istina/laž, itd. Neki algoritmi grupiranja imaju osnovne pretpostavke o tome iz kojeg prostora podataka dolaze značajke, što znači da ne funkcioniraju za one podatke koji nemaju ispunjene takve pretpostavke.

\section{Udaljenost, sličnost i različitost točaka}
\label{pointsimilarity}
U uvodnom poglavlju smo grupiranje opisali kao postupak kojim se skup podataka razvrstava u grupe na taj način gdje su točke u istoj grupi međusobno manje-više ‘‘slični’’. Potrebno je definirati kako se određuje, odnosno kako se mjeri takva ‘‘sličnost’’ između dvije točke. Definirajmo najprije \textbf{mjeru udaljenosti} \engl{distance measure}. Neka je $\mathcal{V}$ prostor iz kojeg dolaze ulazni podaci čiji je $\mathcal{D}$ podskup, te neka je $\mathbb{R}$ skup realnih brojeva. Udaljenost je funkcija
\[d: \mathcal{V} \times \mathcal{V} \to \mathbb{R}\]
koja za svaki \(\mathbf{x}, \mathbf{y}, \mathbf{z} \in \mathcal{V}\) zadovoljava tzv.\ svojstva \textbf{metrike}:
\begin{enumerate}
    \item \(d \left(\mathbf{x}, \mathbf{y}\right) = 0 \quad
                \text{akko} \quad \mathbf{x} = \mathbf{y}\)
    \item \(d \left(\mathbf{x}, \mathbf{y}\right) = d \left(\mathbf{y}, \mathbf{x}\right)\) \qquad (simetričnost)
    \item \(d \left(\mathbf{x}, \mathbf{z}\right) \leq 
            d \left(\mathbf{x}, \mathbf{y}\right)
            + d \left(\mathbf{y}, \mathbf{z}\right)\) \qquad (nejednakost trokuta)
\end{enumerate}
Iz navedenih aksioma metrike može se pokazati da vrijedi \(d \left(\mathbf{x}, \mathbf{y}\right) \geq 0\)
za svaki \(\mathbf{x}, \mathbf{y} \in \mathcal{V}\).
Konceptualno, dvije točke koje se više ‘‘razlikuju’’ u svom prostoru imaju veću udaljenost.
Ponekad su podaci takvi da su sve značajke realni brojevi, to jest \(\mathcal{V} \subseteq \mathbb{R}^n\), tada je \textbf{Euklidska udaljenost} prikladna\footnote{Euklidska udaljenost nije jedina mjera udaljenosti za takav prostor, ali je najkorištenija.} mjera udaljenosti koja zadovoljava svojstva metrike:
\begin{equation}
    d \left(\mathbf{x}, \mathbf{y}\right) = 
    \sqrt{\sum_{i=1}^{n} \left(x_i - y_i\right)^2}
    \label{euclidean_distance}
\end{equation}
Općenito, ako je \(\mathcal{V}\) normirani vektorski prostor sa definiranom normom \(\Vert \cdot \Vert : \mathcal{V} \to \mathbb{R}\) tada se udaljenost može definirati pomoću te norme:
\begin{equation}
    d \left(\mathbf{x}, \mathbf{y}\right) = \Vert \mathbf{y} - \mathbf{x} \Vert
    \label{norm_distance}
\end{equation}
U slučaju realnih značajki i ako se koristi Euklidova norma, dobijemo upravo \ref{euclidean_distance}.\\
Vrlo često u praksi imamo značajke koje nisu realni brojevi niti dolaze iz nekog drugog normiranog prostora. Tada se koriste neke druge mjere udaljenosti specijalizirane za specifične prostore \(\mathcal{V}\). Primjerice, postoji
\begin{itemize}
    \item \textbf{Hammingova udaljenost}: binarni nizovi duljine $n$
    \item \textbf{Jaccardova udaljenost}: skupovi i multiskupovi
    \item \textbf{??? udaljenost} \engl{Edit distance}: znakovni nizovi
    %TODO: PREVESTI EDIT DISTANCE
\end{itemize}
\textbf{Sličnost} \engl{similarity measure} je vrsta mjere koja, poput mjere udaljenosti, brojčano iskazuje koliko se razlikuju dvije točke. Za razliku od mjere udaljenosti, mjera sličnosti ne mora zadovoljavati sva svojstva metrike poput nejednakosti trokuta. Jednako vrijedi i za \textbf{mjeru različitosti} \engl{dissimilariy measure}. Kako su dvije točke ‘‘sličnije’’ tako mjera sličnosti raste, a mjera različitosti pada. Detaljnu definiciju mjera sličnosti i različitosti nećemo navoditi iz razloga što se u algoritmima grupiranja (barem onih koje ćemo opisati) koriste uglavnom mjere udaljenosti u svrhu kvantificiranja sličnosti ili različitosti točaka.

\section{Vrste grupiranja}
\label{vrstegrupiranja}
Ne postoji jedinstvena definicija grupe koja vrijedi za sve algoritme grupiranja. Svaki algoritam ima svoj model grupiranja u kojem je definirano što je to grupa, a sam algoritam pokušava točke grupirati na način koji to najviše odgovara za taj model. Različite modele grupiranja ćemo detaljnije razmotriti u sljedećem poglavlju, a sada navedimo dvije generalne podijele.\\
S obzirom na koji način se oblikuju grupe, grupiranje može biti:
\begin{itemize}
    \item \textbf{Hijerarhijsko grupiranje}
    \item \textbf{Particijsko grupiranje}
\end{itemize}
Kod hijerarhijskog grupiranja svaka grupa, počevši od grupe koja predstavlja cijeli skup točaka $\mathcal{D}$, ima podgrupe koje se tako rekurzivno dijele sve dok svaka točka nije svoja grupa. Na taj način se gradi hijerarhija grupa (od tud i naziv hijerarhijskog grupiranja) koji se može prikazati \textbf{dendrogramom}. Kod hijerarhijskog grupiranja postoje dva pristupa:
\begin{itemize}
    \item \textbf{Aglomerativno grupiranje}: početno je svaka točka u svojoj grupi pa se spajaju u veće grupe;
    \item \textbf{Divizivno grupiranje}: početna grupa je $\mathcal{D}$ pa se ona dijeli u manje podgrupe.
\end{itemize}
Na slici \ref{fig:hier_clustering} je prikazan tijek nekog hijerarhijskog aglomerativnog grupiranja nekog ulaznog skupa točaka.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.95]{img/hier.png}
    \caption{Tijek nekog hijerarhijskog aglomerativnog grupiranja}
    \label{fig:hier_clustering}
\end{figure}
Na lijevoj strani crnom bojom su označene i numerirane točke ulaznog skupa podataka, a crvenim brojevima je označen redosljed kojim je postupak grupiranja spajao grupe u veće grupe. Na desnoj strani je postupak i rezultat grafički prikazan u obliku dendrograma. Na donjoj liniji su oznake točaka, a brojeve na lijevoj liniji zasad zanemarimo. Ako dendrogram shvatimo kao stablo čiji su čvorovi (pod)grupe, a listovi točke iz $\mathcal{D}$, tada aglomerativno grupiranje gradi dendrogram od listova prema korijenu, a divizivno grupiranje od korijena prema listovima.\\
Za razliku od hijerarhijskog grupiranja, particijsko grupiranje nema hijerarhiju grupa i podgrupa. Rezultat grupiranja je neki fiksiran broj grupa bez neke unutarnje strukture osim prikladnih točaka u njima. Najpopularniji i najučinkovitiji algoritmi su upravo particijska grupiranja.

Prirodne grupe točaka ponekad nije moguće jednoznačno odrediti. Grupiranja tada mogu, bilo grupiranje particijsko ili hijerarhijsko, neke točke svrstati ili u isključivo jednu grupu ili u više grupa istovremeno. Prema tome razlikujemo:
\begin{itemize}
    \item \textbf{Čvrsto grupiranje} \engl{hard clustering}: jedna točka može pripadati isključivo jednoj grupi;
    \item \textbf{Meko grupiranje} \engl{soft clustering}: jedna točka može pripadati više grupa sa nekom mjerom pripadnošću svakoj od tih grupa. Primjerice, možemo vjerojatnošću da neka točka pripada nekoj grupi modelirati sigurnost ili nesigurnost njegovoj stvarnoj pripadnosti.
\end{itemize}
Na slici \ref{fig:hard_vs_soft_clustering} vizualno je prikazana razlika između čvrstog i mekog grupiranja nekog skupa točaka. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/hard_vs_soft_clustering.png}
    \caption{Primjer čvrstog i mekog grupiranja}
    \label{fig:hard_vs_soft_clustering}
\end{figure} Uočimo kako je primjerice crvenu točku meko grupiranje svrstalo u dvije grupe istovremeno, a čvrsto grupiranje u isključivo jednu.

\chapter{Algoritmi grupiranja}
U daljnjim potpoglavljima opisat ćemo podskup algoritama grupiranja implementiranih u Pythonovoj programskoj knjižnici \textbf{Scikit-learn}, unutar modula za grupiranje \texttt{sklearn.cluster}. Svako potpoglavlje sadrži najprije opis modela grupiranja kojeg taj algoritam koristi, a zatim opis samog algoritma. Krenimo sa najpopularnijim algoritmom grupiranja.
\section{Algoritam K-sredina} \label{kmeans}
Algoritam \textbf{K-sredina} \engl{K-means clustering} je jednostavan i učinkovit algoritam particijskog čvrstog grupiranja koji ulazni skup točaka $\mathcal{D}$ particionira u $K$ grupa. $K$ je broj grupa koje je potrebno proizvesti i on se zadaje unaprijed kao parametar algoritma. Nešto kasnije ćemo reći nešto i o metodama kako odrediti broj grupa $K$.

Algoritam K-sredina predstavlja svaku grupu sa jednom točkom koja označava središte te grupe: \textbf{centroidom}. Neka je $\mathcal{C} \subseteq \mathcal{D}$ grupa koja sadrži $M$ točaka ($M = \vert \mathcal{C} \vert$, $\vert \mathcal{C} \vert$ predstavlja kardinalitet skupa $\mathcal{C}$), odnosno 
$\mathcal{C} = \left\{ \mathbf{x}^{(i)}  \right\}_{i=1}^{M}$.
Centroid te grupe $\boldsymbol{\mu}$ se definira kao:
\[\boldsymbol{\mu} = \frac{1}{\vert \mathcal{C} \vert} \sum_{\mathbf{x} \in \mathcal{C}} \mathbf{x} = 
\frac{1}{M} \sum_{i=1}^{M} \mathbf{x}^{(i)}\]
Uočimo kako se centroid računa na isti način kako bismo računali aritmetičku sredinu, odakle dolazi naziv K-sredina. Također uočimo da se radi o zbrajanju točaka te u konačnici dijeljenju sa nekim brojem. Dakle da bi uopće bilo moguće računati centroide, nad točkama, odnosno ulaznim podacima, koje potječu iz prostora $\mathcal{V}$, moraju biti definirane operacije zbrajanja i množenja sa skalarom (u našem slučaju realnim brojem $\frac{1}{M}$). Tako dolazimo do prve pretpostavke modela: prostor podataka $\mathcal{V}$ mora biti vektorski prostor\footnote{Vektorski prostor je skup objekata, odnosno vektora, nad kojim su definirane operacije međusobnog zbrajanja i množenja sa skalarom, a te operacije zadovoljavaju 8 aksioma vektorskog prostora.}. Najčešće će u pitanju biti podskup realnog koordinatnog prostora, odnosno $\mathcal{V} \subseteq \mathbb{R}^n$. 
Postoji i varijanta algoritma koja radi nad podacima bilo kakve prirode i koristi mjeru sličnosti (koja je općenitija, odnosno manje ‘‘zahtjevna’’ od mjere udaljenosti): \textbf{algoritam K-medoida}, no nećemo se time baviti.\\
Kao što pojam sugerira, centroid neke grupe konceptualno predstavlja centar, odnosno središte te grupe. Jasno je da centroid grupe se uopće ne mora nalaziti u grupi.

Cilj algoritma K-sredina jest minimizirati \textbf{kriterijsku funkciju}. Neka je ulazni skup točaka $\mathcal{D}$ čvrsto grupiran u $K$ grupa:
\[\mathcal{D} = \bigcup_{k=1}^{K} \mathcal{C}_k, \qquad \forall i \neq j :  \mathcal{C}_i \cap \mathcal{C}_j = \emptyset\]
Kako se radi o čvrstom grupiranju, grupe su međusobno disjunktne, odnosno ne može se dogoditi da neka točka iz $\mathcal{D}$ završi u više grupa. Neka je $\boldsymbol{\mu}_k$ centroid grupe $\mathcal{C}_k$. Kriterijska funkcija za dano grupiranje kod algoritma K-sredina se definira izrazom:
\[J = \sum_{k=1}^{K} \sum_{\mathbf{x} \in \mathcal{C}_k} 
d \left(\mathbf{x}, \boldsymbol{\mu}_k\right)^2\]
Ako podaci dolaze iz podskupa realnog koordinatnog prostora, onda možemo koristiti Euklidsku udaljenost i Euklidsku normu, odnosno
$d \left(\mathbf{x}, \boldsymbol{\mu}_k\right) = \Vert \mathbf{x} - \boldsymbol{\mu}_k \Vert$. Tada kriterijska funkcija glasi:
\[J = \sum_{k=1}^{K} \sum_{\mathbf{x} \in \mathcal{C}_k} 
\Vert \mathbf{x} - \boldsymbol{\mu}_k \Vert^2\]
Kriterijska funkcija zbraja kvadratna odstupanja točaka od centroida grupe u kojima se nalaze. Što su točke bliže svojim centroidima to će iznos kriterijske funkcije biti manji, i to predstavlja bolje grupiranje kod algoritma K-sredina. Algoritam K-sredina nastoji minimizirati iznos kriterijske funkcije, odnosno particionirati (grupirati) skup $\mathcal{D}$ na način koji će dati minimalan iznos kriterijske funkcije. Analitičkim postupcima se ne može pronaći minimum kriterijske funkcije, stoga se optimizacija provodi iterativno, a iterativni postupak optimizacije nudi algoritam K-sredina.

\subsection{Opis algoritma}
Najprije se inicijalizira $K$ centroida, te se u svakoj iteraciji sve točke pridružuju najbližem centroidu. Zatim se centroidi svake grupe ponovno računaju na temelju točaka iz te grupe (pridruženi starom centroidu koji im je bio najbliži). Postupak se ponavlja do konvergencije, to jest dok dvije uzastopne iteracije nisu ništa promijenile u smislu da sve točke zadržavaju svoje grupe i ponovno računanje svih centroida ih ne mijenja. Formalno opišimo algoritam:
\begin{croatianalgorithm}[H]
\caption{Algoritam K-sredina}
\label{algo:k-means}
\begin{algorithmic}
\STATE{\textbf{Parametri}: broj grupa $K$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} čvrste grupe $\mathcal{C}_k, \mathcal{D} = \bigcup_{k=1}^{K} \mathcal{C}_k$}
\STATE
\STATE{\textbf{inicijaliziraj} centroide $\boldsymbol{\mu}_k, \; k \in \left\{1, \dots, K\right\}$}
\REPEAT
\STATE{$\mathcal{C}_k \gets \emptyset, \; k \in \left\{1, \dots, K\right\}$}
\FORALL{$\mathbf{x}^{(i)} \in \mathcal{D}$}
\STATE{$k \gets \argmin_{j \in \left\{1, \dots, K\right\}} \Vert \mathbf{x}^{(i)} - \boldsymbol{\mu}_j \Vert$}
\STATE{$\mathcal{C}_k \gets \mathcal{C}_k \cup \left\{\mathbf{x}^{(i)}\right\}$ }
\ENDFOR
\FORALL{$k \in \left\{1, \dots, K\right\}$}
\STATE{$\boldsymbol{\mu}_k \gets \frac{1}{\vert \mathcal{C}_k \vert} \sum_{\mathbf{x} \in \mathcal{C}_k} \mathbf{x}$}
\ENDFOR
\UNTIL{$\boldsymbol{\mu}_k$ ne konvergiraju}
\end{algorithmic}
\end{croatianalgorithm}

Na slici \ref{fig:kmeans_demo} je na jednostavnom primjeru prikazan princip rada algoritma sa zadanim brojem grupa $K = 3$. Križići predstavljaju centroide, a točke iste boje su pridružene istoj grupi.
\begin{figure}[H]
    \centering
    \includegraphics{img/kmeans_demo.png}
    \caption{Demonstracijski primjer grupiranja algoritmom K-sredina}
    \label{fig:kmeans_demo}
\end{figure}
Potrebno je napomenuti da je algoritam kao inicijalne centroide odabrao već postojeće točke, no to općenito ne mora biti slučaj. Metode odabira inicijalnih centroida ćemo raspraviti u poglavlju \ref{kminitial}.

\subsection{Svojstva i složenost algoritma}
\label{kmsvojstva}
Kao što smo spomenuli na početku poglavlja \ref{kmeans}, cilj algoritma K-sredina jest minimizirati kriterijsku funkciju. Postavlja se pitanje, hoće li algoritam u tome uvijek i uspjeti? Odgovor je: neće. Algoritam će pronaći lokalni optimum, ali ne garantira da će to biti i globalni optimum. Velik utjecaj na konačni rezultat ima upravo prvi korak algoritma: inicijalizacija početnih $K$ centroida. Kako bi se osigurao što bolji rezultat, odnosno što manji iznos kriterijske funkcije, jako je važno na pametan način odabrati početne centroide. Jednako tako je važno odabrati ispravan broj grupa: parametar $K$.

Složenost algoritma K-sredina je $\mathcal{O} \left(TnKN\right)$, gdje je $T$ broj iteracija algoritma do konvergencije, $n$ broj značajki (dimenzija) točaka, a $N$ broj točaka. Korak algoritma u kojem pridružujemo točke najbližem centroidu je složenosti $\mathcal{O} \left(nKN\right)$ jer je potrebno za svaku od $N$ točaka ispitati koji od $K$ trenutnih centroida je najbliži, dakle mora ispitati udaljenost od svakog centroida, a računanje udaljenosti je složenosti $\mathcal{O} \left(n\right)$. Korak algoritma u kojem računamo centroide je složenosti $\mathcal{O} \left(nN\right)$ jer iako iteriramo po grupama, efektivno iteriramo po svim ulaznim točkama te radimo operacije zbrajanja i dijeljenja sa nekim brojem (koje je također složenosti $\mathcal{O} \left(n\right)$).\\
Takva složenost je prihvatljiva i algoritam uz dobar odabir početnih centroida i dobar odabir parametra $K$ proizvodi dobre rezultate. Zbog povoljne složenosti, algoritam je dobar i za jako velike podatke (veliki $N$), a broj grupa i značajka ionako su najčešće\footnote{Kod velikog broja značajki tradicionalni algoritmi grupiranja, ali i postupci obrade podataka općenito, nisu učinkoviti. Tada govorimo o problemu prokletstva visoke dimenzionalnosti \engl{curse of dimensionality}} puno manji od $N$.

\subsection{Odabir početnih $K$ centroida}
\label{kminitial}
Način na koji odabiremo početne centroide kod algoritma K-sredina nije jasno naznačen u algoritmu, a u poglavlju \ref{kmsvojstva} smo spomenuli da je to jako važno kako bi algoritam imao priliku postići globalni optimum.

Postoji niz strategija za odabir početnih $K$ centroida. Prvo što pada na pamet jest kao početne centroide postaviti $K$ slučajno izabranih točaka iz $\mathcal{D}$. Slučajan odabir nije baš najbolja ideja, jer rezultati neće biti dobri ako neki od početnih $K$ centroida se nalazi unutar istih prirodnih grupa.\\
Početni centroidi se mogu izabrati na način da samo prvi centroid bude slučajno odabrana točka iz $\mathcal{D}$, a daljnje centroide izabiremo isto iz $\mathcal{D}$ (ali među onima koji nisu još odabrani kao centroidi) na način da su što udaljenije od već odabranih centroida. Kao uzastopni centroid odabrat ćemo točku koja maksimizira udaljenost do najbližeg, već odabranog centroida. Opišimo to sljedećim algoritmom: 
\begin{croatianalgorithm}[H]
\begin{algorithmic}
\STATE{\textbf{Parametri}: broj grupa $K$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} skup inicijalnih $K$ centroida: 
$\mathcal{I} = \left\{ \boldsymbol{\mu}_k \right\}_{k=1}^{K}$}
\STATE
\STATE{$\mathcal{I} \gets \emptyset$}
\STATE{$\widehat{\mathcal{D}} \gets \mathcal{D}$}
\STATE{\textbf{slučajno odaberi} točku $\mathbf{x} \in \widehat{\mathcal{D}}$}
\STATE{$\mathcal{I} \gets \mathcal{I} \cup \left\{ \mathbf{x} \right\}$}
\STATE{$\widehat{\mathcal{D}} \gets \widehat{\mathcal{D}} \setminus
        \left\{ \mathbf{x} \right\}$}
\WHILE{dokle god $\left\vert \widehat{\mathcal{D}} \right\vert < K$}
\STATE{$\boldsymbol{\mu} \gets \argmax_{\textbf{x} \in \widehat{\mathcal{D}}} 
        \left\Vert \mathbf{x} - \argmin_{\boldsymbol{\mu}' \in \mathcal{I}}
                \Vert \mathbf{x} - \boldsymbol{\mu}' \Vert
        \right\Vert$}
\STATE{$\mathcal{I} \gets \mathcal{I} \cup \left\{ \boldsymbol{\mu} \right\}$}
\STATE{$\widehat{\mathcal{D}} \gets \widehat{\mathcal{D}} \setminus
        \left\{ \boldsymbol{\mu} \right\}$}
\ENDWHILE
\RETURN{$\mathcal{I}$}
\end{algorithmic}
\end{croatianalgorithm}
Ova strategija odabira početnih centroida je u pravilu dobra, no postoji jedna mana: odabirat će se stršeće vrijednosti, a kako su one relativno udaljeni od prirodnih grupa, to bi moglo uzrokovati neoptimalno grupiranje. Postoji alternativa ovakvoj strategiji koja do neke mjere rješava problem odabira stršećih vrijednosti.

Umjesto odabira točke koja maksimizira udaljenost do najbližeg centroida, možemo svakoj točki pridružiti vjerojatnost odabira koja je proporcionalna kvadratu udaljenosti do najbližeg centroida. Takva strategija odabira je poznata kao \textbf{K-means++}.
Pretpostavimo da u nekom trenutku smo odabrali $k$ centroida i one se nalaze u skupu $\mathcal{I}$, tada vjerojatnost odabira neke točke $\mathbf{x} \in \widehat{\mathcal{D}}$ kao sljedeći centroid glasi:
\[P \left( \boldsymbol{\mu}_{k+1} = \mathbf{x} \middle| \widehat{\mathcal{D}}, \mathcal{I}\right)
= \frac{\min_{\boldsymbol{\mu}' \in \mathcal{I}} \Vert \mathbf{x} - \boldsymbol{\mu}' \Vert^2}{\sum_{\mathbf{x}' \in \widehat{\mathcal{D}}} 
\min_{\boldsymbol{\mu}' \in \mathcal{I}} \Vert \mathbf{x}' - \boldsymbol{\mu}' \Vert^2}\]
Nakon što se izračunaju vjerojatnosti za svaku od neodabranih točaka iz $\widehat{\mathcal{D}}$, onda se slučajno točka slučajno odabire prema navedenoj razdiobi vjerojatnosti. Stršeće vrijednosti će i dalje imati najveću vjerojatnost odabira, ali stršećih vrijednosti nema mnogo, pa je ipak vjerojatniji odabir neki od prirodno prosječnih primjera.\\
K-means++ u praksi daje bolje rezultate od varijante K-sredina u kojem se početni centroidi odabiru slučajno. Iznosi kriterijskih funkcija ispadaju manji i algoritam prije konvergira, odnosno potreban je manji broj iteracija do stacionarnog stanja.

\section{Model Gaussove mješavine}
GMM i EM.

\section{Hijerarhijsko aglomerativno grupiranje}
U poglavlju \ref{vrstegrupiranja} smo naveli na koji način hijerarhijsko grupiranje gradi grupe, te smo spomenuli dva pristupa hijerarhijskog grupiranja: aglomerativno i divizivno. U ovom poglavlju ćemo se baviti standardnim algoritmom hijerarhijskog aglomerativnog grupiranja. Također smo u poglavlju \ref{vrstegrupiranja} dali primjer jednog hijerarhijskog aglomerativnog grupiranja na slici \ref{fig:hier_clustering} i vidjeli spajanje grupa. Odluka koje dvije grupe spojiti nije slučajna, stoga prije formalnog opisa algoritma potrebno je prvo definirati pojmove vezane za spajanje grupa.

\subsection{Spajanje grupa}
Odluka koje dvije grupe spojiti se temelji na \textbf{kriteriju spajanja} \engl{linkage criterion}. Kriterij spajanja mjeri koliko su dvije grupe slične, odnosno različite. On se temelji na nekoj od mjera sličnosti između dviju točaka, bila to udaljenost, sličnost ili različitost, i njih smo opisali u poglavlju \ref{vrstegrupiranja}. Najčešće se u svrhu definiranja kriterija spajanja koriste mjere udaljenosti, no mogu se koristiti i mjere sličnosti ili različitosti.

Kriterij spajanja nećemo detaljno opisati kao mjeru udaljenosti, samo na površnoj razini. Neka su su $\mathcal{C}_i$, $\mathcal{C}_j \subset \mathcal{D}$ neprazni i disjunktni podskupovi skupa ulaznih točaka $\mathcal{D}$. Kriterij povezanosti ćemo tada označiti sa $D \left(\mathcal{C}_i, \mathcal{C}_j\right)$ (bitno je ovu oznaku razlikovati od oznake udaljenosti između dvije točke, npr.\ $d \left(\mathbf{x}, \mathbf{y}\right)$) i on mjeri koliko su grupe $\mathcal{C}_i$ i $\mathcal{C}_j$ udaljene. Postoji mnogo kriterija spajanja, mi ćemo navesti one koje su implementirane u programskoj knjižnici Scikit-learn.

\textbf{Jednostruka povezanost} \engl{single-linkage clustering} udaljenost između grupa definira kao minimalnu udaljenost između točaka u tim grupama, gdje je jedna točka iz jedne, a druga točka iz druge grupe:
\[D_{\text{min}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \min_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Potpuna povezanost} \engl{complete-linkage clustering} udaljenost između grupa definira slično kao i jednostruka povezanost, ali ovaj put se gleda maksimalna udaljenost:
\[D_{\text{max}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \max_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Prosječna povezanost} \engl{average-linkage clustering} udaljenost između grupa definira kao prosječnu udaljenost svih parova grupa, opet gdje je jedna točka iz jedna druga točka iz druge grupe:
\[D_{\text{avg}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
        = \frac{1}{\vert \mathcal{C}_i \vert \cdot \vert \mathcal{C}_j \vert} 
            \sum_{\left(\mathbf{x}, \mathbf{y}\right) \in \mathcal{C}_i \times \mathcal{C}_j} d\left(\mathbf{x}, \mathbf{y}\right)
\]
\textbf{Wardova metoda}, također poznata kao Wardova metoda minimalne varijance, prati zbroj kvadratnih pogrešaka (između točaka i centroida) u grupama prije i nakon spajanja. U osnovi Wardova metoda pretpostavlja da točke dolaze iz realnog koordinatnog prostora, no postoje generalizacije na bilo kakve podatke koje koriste mjeru sličnosti. Prema tome se kao mjera pogreške koristi Euklidska udaljenost između točke i centroida, odnosno koristi se Euklidsku norma. Definirajmo zbroj kvadratnih pogreški \engl{error sum of squares} neke neprazne grupe $\mathcal{C}$ čiji je centroid $\boldsymbol{\mu}$:
\[\operatorname{ESS} \left(\mathcal{C}\right) = \sum_{\mathbf{x} \in \mathcal{C}} 
\Vert \mathbf{x} - \boldsymbol{\mu} \Vert^2\]
Wardova metoda udaljenost između grupa definira kao razliku sume kvadratnih pogrešaka nakon i prije spajanja grupa:
\[D_{\text{Ward}} \left( \mathcal{C}_i, \mathcal{C}_j \right) 
= \operatorname{ESS} \left(\mathcal{C}_i \cup \mathcal{C}_j\right) - \left(\operatorname{ESS} \left(\mathcal{C}_i\right) + \operatorname{ESS} \left(\mathcal{C}_j\right)\right)
\]
Gornja formula se može raspisivanjem svesti na jednostavniju formulu:
\[D_{\text{Ward}} \left( \mathcal{C}_i, \mathcal{C}_j \right)
= \frac{\vert \mathcal{C}_i \vert \cdot \vert \mathcal{C}_j \vert}
{\vert \mathcal{C}_i \vert + \vert \mathcal{C}_j \vert}
\left\Vert \boldsymbol{\mu}_i - \boldsymbol{\mu}_j \right\Vert^2
\]
gdje su $\boldsymbol{\mu}_i$ i $\boldsymbol{\mu}_j$ redom centroidi grupa $\mathcal{C}_i$ i $\mathcal{C}_j$.

\subsection{Opis algoritma}
Hijerarhijskog aglomerativno grupiranje najprije stvara $N$ grupa, odnosno na početku tretira svaku ulaznu točku iz $\mathcal{D}$ kao zasebnu grupu. Prilikom svake iteracije donosi odluku koje dvije grupe spojiti na temelju kriterija spajanja: spojiti one dvije grupe koje su najbliže prema kriteriju spajanja koji je unaprijed odabran kao parametar algoritma. Nakon spajanja grupa, broj ukupnih grupa se smanjuje za 1 i postupak se ponavlja.\\
Naravno, algoritam može ponavljati postupak dok ne ostane samo jedna grupa, ali to nam nije od koristi. Umjesto toga, postupak možemo zaustaviti\footnote{Tada u neku ruku imamo particijsko grupiranje, jer kada zaustavimo postupak, onda smo ‘‘presjekli’’ horizontalno dendrogram i prihvatili grupe kakve jesu na toj razini, bez hijerarhijske strukture.} kada se ispuni neki kriterij zaustavljanja, a to može biti jedno od sljedećih opcija:
\begin{itemize}
    \item dostignut je željeni broj grupa;
    \item najmanja udaljenost između dvije grupe je premašila neku granicu.
\end{itemize}
Odluka kada će postupak završiti je također parametar algoritma. Ukoliko želimo zaustaviti postupak nakon dostignutih $K$ grupa, broj $K$ moramo predati kao parametar, a ako želimo da se ne premaši najmanja udaljenost $\epsilon$ tijekom spajanja, tada moramo specificirati broj $\epsilon$.\\
Formalno opišimo algoritam:
\begin{croatianalgorithm}[H]
\caption{Hijerarhijsko aglomerativno grupiranje}
\label{algo:hac}
\begin{algorithmic}
\STATE{\textbf{Parametri}: kriterij spajanja $D \left(\mathcal{C}_i, \mathcal{C}_j\right)$, uvjet zaustavljanja: broj grupa $K$ ili najveća dopuštena minimalna udaljenost između grupa $\epsilon$}
\STATE{\textbf{Ulaz:} skup točaka $\mathcal{D} = \left\{ \mathbf{x}^{(i)}
\right\}_{i=1}^{N}$ }
\STATE{\textbf{Izlaz:} grupiranje: skup međusobno disjunktnih grupa
$\Gamma = \left\{ \mathcal{C}_i\right\}_{i=1}^{M}$ koji sačinjavaju skup točaka
$ \mathcal{D} = \bigcup_{i=1}^{M} \mathcal{C}_i $}, gdje broj grupa $M$ je ili nepoznat ili $M = K$ ovisno o uvjetu zaustavljanja

\STATE

\STATE{$\Gamma \gets \emptyset$}
\FORALL{$i \in \left\{1, \dots, N\right\}$}
    \STATE{$\mathcal{C}_i \gets \left\{ \mathbf{x}^{(i)} \right\}$}
    \STATE{$\Gamma \gets \Gamma \cup \left\{ \mathcal{C}_i \right\}$}
\ENDFOR
\WHILE{}
\STATE{\textbf{prekini} ako je zadan $K$ i vrijedi $\vert \Gamma \vert \leq K$}
\STATE{$\left(\mathcal{C}_i, \mathcal{C}_j\right) \gets 
        \argmin_{\left(\mathcal{C}_a, \mathcal{C}_b\right) \in \Gamma \times \Gamma}
        D \left(\mathcal{C}_a, \mathcal{C}_b\right)
$}
\STATE{\textbf{prekini} ako je zadan $\epsilon$ i vrijedi $D \left(\mathcal{C}_i, \mathcal{C}_j\right) > \epsilon$}
\STATE{$\mathcal{C}_i \gets \mathcal{C}_i \cup \mathcal{C}_j$}
\STATE{$\Gamma \gets \Gamma \setminus \left\{\mathcal{C}_j\right\}$}
\ENDWHILE
\RETURN{$\Gamma$}
\end{algorithmic}
\end{croatianalgorithm}

\subsection{Primjer grupiranja}
Pogledajmo jednostavan primjer hijerarhijskog aglomerativnog grupiranja. Neka su ulazni podaci točke sa dvjema realnim značajkama prikazani na slici \ref{fig:hierdata}. Radi lakšeg snalaženja, sve točke su označene brojevima.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{img/hierdata.png}
    \caption{Ulazni podaci}
    \label{fig:hierdata}
\end{figure}
Provedimo hijerarhijsko aglomerativno gurpiranje tih točaka. Neka kriterij spajanja bude prosječna povezanost i neka algoritam spoji sve podgrupe, odnosno postavimo broj grupa $K = 1$. Vizualni rezultat grupiranja prikažimo dendrogramom na slici \ref{fig:dendrogram}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.85]{img/dendro.png}
    \caption{Dendrogram hijerarhijskog aglomerativnog grupiranja}
    \label{fig:dendrogram}
\end{figure}
Na donjoj liniji se nalaze oznake svake točke, a na lijevoj liniji se nalazi mjera koja iskazuje koja je bila prosječna udaljenost (jer koristimo prosječnu povezanost kao kriterij spajanja) između grupa u trenutku spajanja. Ukoliko bismo htjeli particijski grupirati primjere, to možemo odlučiti prema vlastitoj volji. Primjerice, ukoliko ne želimo spojiti grupe čija prosječna udaljenost prelazi $\epsilon = 3.5$, tada bismo dendrogram ‘‘presjekli’’ horizontalno na toj razini i kao rezultat bismo dobili 3 grupe jer bismo presjekli dendrogram na 3 mjesta. Na sličan način možemo i po volji grupirati točke u proizvoljan broj grupa. Na slici \ref{fig:hiereps} je prikazan rezultat grupiranja ako se specificira $\epsilon = 3.5$.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{img/hiereps.png}
    \caption{Hijerarhijsko aglomerativno grupiranje uz $\epsilon = 3.5$}
    \label{fig:hiereps}
\end{figure}

\subsection{Svojstva i složenost algoritma}
Opisani algoritam daje dobre rezultate, no najveći nedostatak je vremenska i prostorna složenost. Pretpostavimo da se algoritam zaustavlja nakon što je preostalo $K$ grupa. To znači da algoritam u glavnoj petlji radi $N-K$ koraka. Korak u kojem se odlučuje koje dvije grupe spojiti mora proći kroz sve kombinacije grupa, što je vremenske složenosti $\mathcal{O} \left(N^2\right)$. Prema tome, vremenska složenost algoritma je $\mathcal{O} \left(N^2 \left(N-K\right)\right)$, a kako je u praksi $N$ puno veći od $K$, onda je vremenska složenost $\mathcal{O} \left(N^3\right)$. Iz tog razloga algoritam nije dobar izbor za velike skupove podataka. Također se informacija o udaljenosti između točaka jako često koristi tijekom izvršenja algoritma, stoga implementacije u pravilu najprije konstruiraju tzv.\ \textbf{matricu udaljenosti} \engl{distance matrix}. Matrica udaljenosti je simetrična matrica dimenzija $N \times N$ i sadrži izračunatu udaljenost između svih parova točaka. Računanje udaljenosti je tada vremenski brzo, ali problem je u prostornoj složenosti: $\mathcal{O} \left(N^2\right)$ (jer u najmanju ruku moramo spremiti $\binom{N}{2}$ brojeva), i to za velike skupove podataka već predstavlja veliki problem. Umjesto matrice udaljenosti, analogno postoji matrica sličnosti i matrica udaljenosti.

Standardni ‘‘naivni’’ algoritam je vremenske složenosti $\mathcal{O} \left(N^3\right)$, no postoje bolje implementacije složenosti $\mathcal{O} \left(N^2 \log N\right)$. Specijalno, ako se koriste jednostruka ili potpuna povezanost, postoje algoritmi složenosti $\mathcal{O} \left(N^2\right)$ poznati kao SLINK i CLINK.

\section{DBSCAN}
DBSCAN.

\chapter{Postupci vrednovanja algoritama grupiranja}
Dosad smo se bavili problemom grupiranja: na koji način podijeliti ulazni skup točaka u grupe sa međusobno sličnim točkama. Nakon što to napravimo na neki način, postavlja se pitanje na koji način ocijeniti rezultat grupiranja. Taj zadatak može biti jednako težak kao i sam problem grupiranja ukoliko nemamo neku vanjsku informaciju o tome kako bi podaci trebali stvarno biti grupirani. Naravno, kada bismo na raspolaganju imali takve informacije, ne bi bilo potrebe za grupiranjem. Prema tome, vrednovanje grupiranja dijelimo na \textbf{unutarnje} \engl{internal evaluation} i \textbf{vanjsko} \engl{external evaluation} vrednovanje. Kod vanjskog vrednovanja, osim ulaznih podataka dostupne su nam i njihove oznake, dok to nije slučaj kod unutarnjeg vrednovanja.

\section{Unutarnje vrednovanje}
Unutarnje vrednovanje nastoji nekom grupiranju dodijeliti ocjenu samo na temelju ulaznih podataka i oznaka koje je generiralo grupiranje. Kriteriji vrednovanja najčećše daju veće ocjene grupiranjima koja stvaraju grupe u kojima su točke međusobno slične, a različite u odnosu na točke iz ostalih grupa. Nedostatak takvih vrednovanja jest taj da će ono dati bolje rezultate algoritmima koji upravo nastoje povećati ocjenu tog specifičnog vrednovanja, a da takav algoritam nije u stvarnosti obavio grupiranje na optimalan način.

\subsection{Metoda siluete}
Metoda siluete je grafička metoda kojom se mjeri koliko je svaka točka iz $\mathcal{D}$ slična svojoj grupi u odnosu na ostale grupe. Svakoj točki se pridružuje \textbf{vrijednost siluete}. Za svaku točku $\mathbf{x}^{(i)}$ iz ulaznog skupa točaka $\mathcal{D} = \left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N}$ uvedimo oznaku $\mathcal{C} \left(i\right)$ koja označava grupu kojoj pripada ta točka. Također na raspolaganju imamo grupiranje koje je generiralo promatrani algoritam, odnosno skup od $K$ grupa $\Gamma = \left\{\mathcal{C}_k\right\}_{k=1}^{K}$.
Prije nego što definiramo vrijednost siluete $s (i)$, definirajmo $a (i)$ kao prosječnu udaljenost točke $\mathbf{x}^{(i)}$ do ostalih točaka u svojoj grupi):
\[a (i) = \frac{1}{\left\vert \mathcal{C} \left(i\right) \right\vert - 1} \sum_{\substack{\mathbf{y} \in \mathcal{C} \left(i\right) \\ \mathbf{y} \neq \mathbf{x}^{(i)}}} d \left(\mathbf{x}^{(i)}, \mathbf{y}\right)
\]
Zatim definirajmo $b (i)$ kao prosječnu udaljenost točke $\mathbf{x}^{(i)}$ do točaka najbliže grupe:
\[b (i) = \min_{\substack{\mathcal{C} \in \Gamma \\ \mathcal{C} \neq \mathcal{C} \left(i\right)}} \frac{1}{\vert \mathcal{C} \vert} \sum_{\mathbf{y} \in \mathcal{C}}
d \left(\mathbf{x}^{(i)}, \mathbf{y}\right)\]
Vrijednost siluete se tada definira kao
\[s (i) = \frac{b (i) - a (i)}{\operatorname{max} \left\{a (i), b (i)\right\}}\]
a u slučaju da je točka $\mathbf{x}^{(i)}$ sama u svojoj grupi, odnosno $\left\vert \mathcal{C} \left(i\right) \right\vert = 1$, tada je $s (i) = 0$ jer vrijednost $a (i)$ nema smisla. Uzimajući u obzir međusobne odnose $a (i)$ i $b (i)$, definiciju vrijednosti siluete možemo izraziti na sljedeći način:
\[
s (i) = \begin{cases}
 0 & \text{ako} \; \left\vert \mathcal{C} \left(i\right) \right\vert = 1 \\
 1 - \frac{a (i)}{b (i)} & \text{ako} \; a (i) < b (i) \\  
 0 & \text{ako} \; a (i) = b (i) \\  
 \frac{b (i)}{a (i)} - 1 & \text{ako} \; a (i) > b(i)  
 \end{cases}
\]
Jasno je vidljivo iz ovakve definicije vrijednosti siluete da ona može samo poprimiti vrijednosti između -1 i 1, odnosno $-1 \leq s (i) \geq 1$. Kako $a (i)$ mjeri koliko je točka $\mathbf{x}^{(i)}$ blizu ostalim točkama iz svoje grupe, manja vrijedost $a(i)$ znači veću sličnost $\mathbf{x}^{(i)}$ sa ostalim točkama iz svoje grupe. S druge strane, $b(i)$ mjeri koliko je $\mathbf{x}^{(i)}$ blizu točkama iz najbliže susjedne grupe, stoga veća vrijednost $b (i)$ znači veću ‘‘razdvojenost’’, odnosno veću različitost od ostalih grupa. Prema tome, kada je $a(i)$ puno manji od $b(i)$, vrijednost siluete $s (i)$ će biti blizu 1, što označava dobro grupiranje konkretno za promatranu točku. Analogno, kada je $b (i)$ puno veći od $a (i)$, to znači da je točka bliža nekoj drugoj grupi nego vlastitoj. Tada će $s(i)$ biti blizu -1 i to označava loše grupiranje. Vrijednost siluete blizu 0 znači da se $a(i)$ i $b(i)$ malo razlikuju, što znači da je točka $\mathbf{x}^{(i)}$ na granici dvije grupe.
%TODO: NASTAVITI METODU SILUETE%

\section{Vanjsko vrednovanje}
Vanjsko vrednovanje, uz ulazne podatke i grupe koje je promatrani algoritam generirao, na raspolaganju ima i oznake svake točke koje otkrivaju referentno grupiranje. Te oznake nisu poznate postupcima nenadziranog strojnog učenja, pa tako i algoritmima grupiranja. Do njih je ponekad moguće doći primjerice ljudskom ocjenom ulaznog skupa primjera, ili smo ih mogli imati od prije, no odlučili smo ih ne specificirati metodama nenadziranog strojnog učenja (pa tako i grupiranja).


\subsection{Randov indeks}
Randov indeks mjeri preciznost grupiranja tako da promatra sve parove ulaznih primjera i uspoređuje dva grupiranja istovremeno. Jedno grupiranje je ono koje je generirao algoritam grupiranja, a drugo grupiranje (referentno grupiranje) je prema prethodno poznatim oznakama: dva primjera su u istoj grupi ako imaju istu oznaku. Za svaki mogući par promatramo jesu li završili u istoj grupi, i jesu li oni u istoj grupi kod referentnog grupiranja. Parove ulaznih primjera tada dijelimo na:
\begin{enumerate}
    \item \textbf{istinito pozitivne} \engl{true positive}: primjeri se nalaze u istoj grupi u oba grupiranja, broj takvih parova označimo sa $TP$;
    \item \textbf{istinito negativne} \engl{true negative}: primjeri se nalaze u različitim grupama u oba grupiranja, broj takvih parova označimo sa $TN$;
    \item \textbf{lažno pozitivne} \engl{false positive}: primjeri se nalaze u istoj grupi u dobivenom grupiranju, a u referentnom grupiranju se nalaze u različitim grupama, broj takvih parova označimo sa $FP$;
    \item \textbf{lažno negativne} \engl{false negative}: primjeri se nalaze u različitim grupama u dobivenom grupiranju, a u referentnom grupiranju se nalaze u istoj grupi, broj takvih parova označimo sa $FN$;
\end{enumerate}

Parovi primjera su dobro grupirani ako situacija odgovara referentnom grupiranju, dakle zanimaju nas primjeri koji su istinito pozitivni i istinito negativni.
Randov indeks $R$ se tada definira kao:
\[R = \frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{\binom{N}{2}}\]
Jasno je da su u nazivniku pokriveni svi mogući parovi ulaznih primjera, broj takvih parova znamo da mora biti $\binom{N}{2}$.

Kao primjer uzmimo da su podaci skup od 6 prirodnih brojeva od 1 do 6. Pretpostavimo da smo podatke grupirali na sljedeći način:
\[\left\{ \left\{1, 2, 4\right\}, \left\{3, 5\right\} \right\}\]
U programskim implementacijama obično se grupiranje specificira nizom oznakama. Za ovaj primjer bi onda u pitanju bio niz $\left\{0, 0, 1, 0, 1\right\}$ (kod oznaka je samo bitno da primjeri iste grupe imaju istu oznaku, kakva je oznaka je nebitno). Pretpostavimo da je referentno grupiranje:
\[\left\{\left\{1,2\right\}, \left\{3,4\right\}, \left\{5\right\}\right\}\]
odnosno u terminima oznaka bi onda specificirali nešto poput $\left\{0, 0, 1, 1, 2\right\}$.
Za ovaj primjer je $TP = 1$ jer samo par 12 se nalazi u istim grupama u oba grupiranja, a $TN = 5$ i radi se o parovima 13, 15, 23, 25, 45. Prema tome $R = \frac{1 + 5}{\binom{5}{2}} = \frac{6}{10} = 0.6$.

\chapter{Programsko ostvarenje i rezultati}
Programsko ostvarenje i rezultati.

\chapter{Zaključak}
Zaključak.

\bibliography{literatura}
\bibliographystyle{fer}

\begin{sazetak}
Sažetak na hrvatskom jeziku.

\kljucnerijeci{Ključne riječi, odvojene zarezima.}
\end{sazetak}
%‘‘slični’’
% TODO: Navedite naslov na engleskom jeziku.
\engtitle{Title}
\begin{abstract}
Abstract.

\keywords{Keywords.}
\end{abstract}

\end{document}
